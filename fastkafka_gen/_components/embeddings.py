# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/Embeddings.ipynb.

# %% auto 0
__all__ = ['logger', 'app', 'DAFAULT_DB_PATH', 'DAFAULT_START_URL', 'generate']

# %% ../../nbs/Embeddings.ipynb 1
from typing import *
from urllib.request import Request, urlopen
from urllib.parse import urlparse, urljoin
from urllib.error import HTTPError
from pathlib import Path
import shutil

from bs4 import BeautifulSoup
from langchain.document_loaders import WebBaseLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema.document import Document
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
import typer

from .logger import get_logger
from .package_data import get_root_data_path

# %% ../../nbs/Embeddings.ipynb 3
logger = get_logger(__name__)

# %% ../../nbs/Embeddings.ipynb 5
def _get_all_links_from_website(start_url: str, visited: Optional[set] = None) -> Set[str]:
    """Get a set of all links (URLs) found on the given website, starting from the given start URL.
    
    Args:
        start_url: The starting URL of the website.
        visited: Optional. A set of URLs that have already been visited. Defaults to an empty set.

    Returns:
        A set of all links found on the website.
    """
    if visited is None:
        visited = set()
    try:
        req = Request(start_url)
        # nosemgrep: python.lang.security.audit.dynamic-urllib-use-detected.dynamic-urllib-use-detected
        html_page = urlopen(req) # nosec B310
        soup = BeautifulSoup(html_page, "lxml")

        base_url = urlparse(start_url).scheme + '://' + urlparse(start_url).hostname #type: ignore

        links = set()
        for link in soup.find_all('a', href=True):
            url = urljoin(base_url, link['href']).split("#")[0].strip("/")
            if urlparse(url).hostname == urlparse(start_url).hostname:
                links.add(url)

        visited.add(start_url)
        for link in links:
            if link not in visited:
                visited |= _get_all_links_from_website(link, visited)
                
    except HTTPError as e:
        logger.warning(f'Unable to parse: {e.url}')
    
    return visited

# %% ../../nbs/Embeddings.ipynb 7
def _extract_latest_doc_urls(start_url: str, urls: List[str]) -> List[str]:
    """Extract latest documentation URLs from a list of URLs.

    Args:
        start_url: The URL of the documentation homepage.
        urls: A list of documentation URLs to be filtered.

    Returns:
        A new list containing only the latest version of the documentation URLs.
    """
    ret_val = []
    for url in urls:
        parts = url.split(f"{start_url}/docs/")
        if len(parts) == 1:
            ret_val.append(url)
        else:
            identifier = parts[1].split("/")[0]
            if identifier != "next" and not identifier.replace(".", "").isdigit():
                ret_val.append(url)
    ret_val = [url for url in ret_val if "/guides/" in url or url == "https://fastkafka.airt.ai/docs"]
    return ret_val

# %% ../../nbs/Embeddings.ipynb 10
def _create_documents(urls: List[str]) -> List[Document]:
    """Scrape the URLs and create a document object.
    
    Args:
        urls: A list of URLs to scrape
        
    Returns:
        A list of document object
    """
    loader = WebBaseLoader(urls)
    data = loader.load()
    return data

# %% ../../nbs/Embeddings.ipynb 12
def _split_document_into_chunks(
    documents: List[Document],
    chunk_size: int = 1500,
    chunk_overlap: int = 150,
    separators: List[str] = ["\n\n", "\n", "(?<=\. )", " ", ""],
) -> List[Document]:
    """Split the list of documents into chunks

    Args:
        documents: List of documents to be split into chunks.
        chunk_size: The maximum size of each chunk in characters. Defaults to 1500.
        chunk_overlap: The overlap between consecutive chunks in characters. Defaults to 150.
        separators: List of separator patterns used for chunking. Defaults to ["\n\n", "\n", "(?<=\. )", " ", ""].

    Returns:
        A list of documents where each document represents a chunk.
    """
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=separators
    )
    chunks = text_splitter.split_documents(documents)
    return chunks

# %% ../../nbs/Embeddings.ipynb 14
def _save_embeddings_db(doc_chunks: List[Document], db_path: str) -> None:
    """Save the embeddings in a FAISS db
    
    Args:
        doc_chunks: A list of documents where each document represents a chunk.
        db_path: Path to save the FAISS db.
    """
    db = FAISS.from_documents(doc_chunks, OpenAIEmbeddings()) # type: ignore
    db.save_local(db_path)

# %% ../../nbs/Embeddings.ipynb 16
def _delete_directory(directory_path: Path) -> None:
    """Delete a directory and its contents if it exists.

    Args:
        directory_path: The path to the directory to be deleted.
    """
    if directory_path.exists():
        try:
            shutil.rmtree(directory_path)
        except Exception as e:
            print(f"Error deleting directory: {e}")

# %% ../../nbs/Embeddings.ipynb 18
app = typer.Typer(
    short_help="Scrape FastKafka documentation, create embeddings from extracted content, and save them in a vector database.",
)

# %% ../../nbs/Embeddings.ipynb 19
DAFAULT_DB_PATH = get_root_data_path() / "docs"
DAFAULT_START_URL = "https://fastkafka.airt.ai"

@app.command(
    "generate",
    help="Scrape FastKafka documentation, create embeddings from extracted content, and save them in a vector database.",
)
def generate(
    start_url: str = typer.Option(
        DAFAULT_START_URL,
        "--start_url",
        "-u",
        help="The start_url of the website to scrape."
    ),
    db_path: str = typer.Option(
        DAFAULT_DB_PATH, 
        "--db_path",
        "-p",
        help="The path to save the vector database."
    ),
) -> None:
    try:
        _delete_directory(Path(db_path))
        
        typer.echo(f"Scrapping {start_url} (The whole process usually takes around 45 to 90 seconds...)")
        all_doc_links = list(_get_all_links_from_website(start_url))
        filtered_doc_links = _extract_latest_doc_urls(start_url, all_doc_links)
        
        typer.echo(f"Number of identified URLs for scraping: {len(filtered_doc_links)}\n")
        typer.echo(f"Scraping the below URLs:\n")
        typer.echo("\n".join(filtered_doc_links))
        
        docs = _create_documents(filtered_doc_links)
        doc_chunks = _split_document_into_chunks(docs)
        _save_embeddings_db(doc_chunks, db_path)
        
        typer.echo(f"\nWebsite embeddings have been successfully saved to: {db_path}")
    except Exception as e:
        fg = typer.colors.RED
        typer.secho(f"Unexpected internal error: {e}", err=True, fg=fg)
        raise typer.Exit(code=1)
