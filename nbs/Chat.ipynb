{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87d24ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp _code_generator.chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa65c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from typing import *\n",
    "import random\n",
    "import logging\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import openai\n",
    "from fastcore.foundation import patch\n",
    "from langchain.schema.document import Document\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from faststream_gen._code_generator.constants import (\n",
    "    DEFAULT_PARAMS,\n",
    "    MAX_RETRIES,\n",
    "    STEP_LOG_DIR_NAMES,\n",
    "    MAX_NUM_FIXES_MSG,\n",
    "    INCOMPLETE_DESCRIPTION,\n",
    "    DESCRIPTION_EXAMPLE,\n",
    "    LOGS_DIR_NAME,\n",
    ")\n",
    "from faststream_gen._components.logger import get_logger, set_level\n",
    "from faststream_gen._code_generator.prompts import SYSTEM_PROMPT\n",
    "from faststream_gen._code_generator.helper import add_tokens_usage\n",
    "from faststream_gen._components.package_data import get_root_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85048c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "import pytest\n",
    "\n",
    "from faststream_gen._components.logger import suppress_timestamps\n",
    "from faststream_gen._code_generator.constants import OpenAIModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4add3503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "logger = get_logger(__name__, level=logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a18be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: ok\n"
     ]
    }
   ],
   "source": [
    "suppress_timestamps()\n",
    "logger = get_logger(__name__, level=20)\n",
    "logger.info(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd26ff13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "# Reference: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_handle_rate_limits.ipynb\n",
    "\n",
    "\n",
    "def _retry_with_exponential_backoff(\n",
    "    initial_delay: float = 1,\n",
    "    exponential_base: float = 2,\n",
    "    jitter: bool = True,\n",
    "    max_retries: int = 10,\n",
    "    max_wait: float = 60,\n",
    "    errors: tuple = (\n",
    "        openai.error.RateLimitError,\n",
    "        openai.error.ServiceUnavailableError,\n",
    "        openai.error.APIError,\n",
    "    ),\n",
    ") -> Callable:\n",
    "    \"\"\"Retry a function with exponential backoff.\"\"\"\n",
    "\n",
    "    def decorator(\n",
    "        func: Callable[[str], Tuple[str, str]]\n",
    "    ) -> Callable[[str], Tuple[str, str]]:\n",
    "        def wrapper(*args, **kwargs):  # type: ignore\n",
    "            num_retries = 0\n",
    "            delay = initial_delay\n",
    "\n",
    "            while True:\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "\n",
    "                except errors as e:\n",
    "                    num_retries += 1\n",
    "                    if num_retries > max_retries:\n",
    "                        raise Exception(\n",
    "                            f\"Maximum number of retries ({max_retries}) exceeded.\"\n",
    "                        )\n",
    "                    delay = min(\n",
    "                        delay\n",
    "                        * exponential_base\n",
    "                        * (1 + jitter * random.random()),  # nosec\n",
    "                        max_wait,\n",
    "                    )\n",
    "                    logger.info(\n",
    "                        f\"Note: OpenAI's API rate limit reached. Command will automatically retry in {int(delay)} seconds. For more information visit: https://help.openai.com/en/articles/5955598-is-api-usage-subject-to-any-rate-limits\",\n",
    "                    )\n",
    "                    time.sleep(delay)\n",
    "\n",
    "                except Exception as e:\n",
    "                    raise e\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3912d382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "@_retry_with_exponential_backoff()\n",
    "def mock_func():\n",
    "    return \"Success\"\n",
    "\n",
    "actual = mock_func()\n",
    "expected = \"Success\"\n",
    "\n",
    "print(actual)\n",
    "assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e0da95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: Note: OpenAI's API rate limit reached. Command will automatically retry in 3 seconds. For more information visit: https://help.openai.com/en/articles/5955598-is-api-usage-subject-to-any-rate-limits\n",
      "Maximum number of retries (1) exceeded.\n"
     ]
    }
   ],
   "source": [
    "# Test max retries exceeded\n",
    "@_retry_with_exponential_backoff(max_retries=1)\n",
    "def mock_func_error():\n",
    "    raise openai.error.RateLimitError\n",
    "\n",
    "\n",
    "with pytest.raises(Exception) as e:\n",
    "    mock_func_error()\n",
    "\n",
    "print(e.value)\n",
    "assert str(e.value) == \"Maximum number of retries (1) exceeded.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f834ebb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def _get_relevant_document(query: str) -> str:\n",
    "    \"\"\"Load the vector database and retrieve the most relevant document based on the given query.\n",
    "\n",
    "    Args:\n",
    "        query: The query for relevance-based document retrieval.\n",
    "\n",
    "    Returns:\n",
    "        The content of the most relevant document as a string.\n",
    "    \"\"\"\n",
    "    db_path = get_root_data_path() / \"docs\"\n",
    "    db = FAISS.load_local(db_path, OpenAIEmbeddings()) # type: ignore\n",
    "    results = db.max_marginal_relevance_search(query, k=1, fetch_k=3)\n",
    "    results_str = \"\\n\".join([result.page_content for result in results])\n",
    "    return results_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509e0e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] faiss.loader: Loading faiss with AVX2 support.\n",
      "[INFO] faiss.loader: Successfully loaded faiss with AVX2 support.\n",
      "hide:\n",
      "  - navigation\n",
      "  - footer\n",
      "\n",
      "Release Notes\n",
      "\n",
      "FastStream is a new package based on the ideas and experiences gained from FastKafka and Propan. By joining our forces, we picked up the best from both \n"
     ]
    }
   ],
   "source": [
    "query = \"What is FastStream?\"\n",
    "actual = _get_relevant_document(query)\n",
    "print(actual[:200])\n",
    "assert len(actual) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9dc8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class CustomAIChat:\n",
    "    \"\"\"Custom class for interacting with OpenAI\n",
    "\n",
    "    Attributes:\n",
    "        model: The OpenAI model to use. If not passed, defaults to gpt-3.5-turbo-16k.\n",
    "        system_prompt: Initial system prompt to the AI model. If not passed, defaults to SYSTEM_PROMPT.\n",
    "        initial_user_prompt: Initial user prompt to the AI model.\n",
    "        params: Parameters to use while initiating the OpenAI chat model. DEFAULT_PARAMS used if not provided.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str,\n",
    "        user_prompt: Optional[str] = None,\n",
    "        params: Dict[str, float] = DEFAULT_PARAMS,\n",
    "        semantic_search_query: Optional[str] = None,\n",
    "    ):\n",
    "        \"\"\"Instantiates a new CustomAIChat object.\n",
    "\n",
    "        Args:\n",
    "            model: The OpenAI model to use. If not passed, defaults to gpt-3.5-turbo-16k.\n",
    "            user_prompt: The user prompt to the AI model.\n",
    "            params: Parameters to use while initiating the OpenAI chat model. DEFAULT_PARAMS used if not provided.\n",
    "            semantic_search_query: A query string to fetch relevant documents from the database\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.messages = [\n",
    "            {\"role\": role, \"content\": content}\n",
    "            for role, content in [\n",
    "                (\"system\", SYSTEM_PROMPT),\n",
    "                (\"user\", self._get_doc(semantic_search_query)),\n",
    "                (\"user\", user_prompt),\n",
    "            ]\n",
    "            if content is not None\n",
    "        ]\n",
    "        self.params = params\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_doc(semantic_search_query: Optional[str] = None) -> str:\n",
    "        if semantic_search_query is None:\n",
    "            return \"\"\n",
    "        return _get_relevant_document(semantic_search_query)\n",
    "    \n",
    "    @_retry_with_exponential_backoff()\n",
    "    def __call__(self, user_prompt: str) -> Tuple[str, Dict[str, int]]:\n",
    "        \"\"\"Call OpenAI API chat completion endpoint and generate a response.\n",
    "\n",
    "        Args:\n",
    "            user_prompt: A string containing user's input prompt.\n",
    "\n",
    "        Returns:\n",
    "            A tuple with AI's response message content and the total number of tokens used while generating the response.\n",
    "        \"\"\"\n",
    "        self.messages.append(\n",
    "            {\"role\": \"user\", \"content\": f\"{user_prompt}\\n==== YOUR RESPONSE ====\\n\"}\n",
    "        )\n",
    "        prompt_str = \"\\n\\n\".join([f\"===Role:{m['role']}===\\n\\nMessage:\\n{m['content']}\" for m in self.messages])\n",
    "        logger.info(f\"\\n\\nPrompt to the model: \\n\\n{prompt_str}\")\n",
    "        \n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=self.model,\n",
    "            messages=self.messages,\n",
    "            temperature=self.params[\"temperature\"],\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            response[\"choices\"][0][\"message\"][\"content\"],\n",
    "            response[\"usage\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d9caa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: \n",
      "\n",
      "Prompt to the model: \n",
      "\n",
      "===Role:system===\n",
      "\n",
      "Message:\n",
      "\n",
      "You are an expert Python developer, tasked to generate executable Python code as a part of your work with the FastStream framework. \n",
      "\n",
      "You are to abide by the following guidelines:\n",
      "\n",
      "1. You must never enclose the generated Python code with ``` python. It is mandatory that the output is a valid and executable Python code. Please ensure this rule is never broken.\n",
      "\n",
      "2. Some prompts might require you to generate code that contains async functions. For example:\n",
      "\n",
      "async def app_setup(context: ContextRepo):\n",
      "    raise NotImplementedError()\n",
      "\n",
      "In such cases, it is necessary to add the \"import asyncio\" statement at the top of the code. \n",
      "\n",
      "You will encounter sections marked as:\n",
      "\n",
      "==== APP DESCRIPTION: ====\n",
      "\n",
      "These sections contain the description of the FastStream app you need to implement. Treat everything below this line, until the end of the prompt, as the description to follow for the app implementation.\n",
      "\n",
      "\n",
      "===Role:user===\n",
      "\n",
      "Message:\n",
      "\n",
      "\n",
      "===Role:user===\n",
      "\n",
      "Message:\n",
      "\n",
      "You should respond with 0, 1 or 2 and nothing else. Below are your rules:\n",
      "\n",
      "==== RULES: ====\n",
      "\n",
      "If the ==== APP DESCRIPTION: ==== section is not related to FastKafka or contains violence, self-harm, harassment/threatening or hate/threatening information then you should respond with 0.\n",
      "\n",
      "If the ==== APP DESCRIPTION: ==== section is related to FastKafka but focuses on what is it and its general information then you should respond with 1. \n",
      "\n",
      "If the ==== APP DESCRIPTION: ==== section is related to FastKafka but focuses how to use it and instructions to create a new app then you should respond with 2. \n",
      "\n",
      "\n",
      "===Role:user===\n",
      "\n",
      "Message:\n",
      "Name the tallest mountain in the world\n",
      "==== YOUR RESPONSE ====\n",
      "\n",
      "0\n",
      "{\n",
      "  \"prompt_tokens\": 348,\n",
      "  \"completion_tokens\": 1,\n",
      "  \"total_tokens\": 349\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "TEST_INITIAL_USER_PROMPT = \"\"\"\n",
    "You should respond with 0, 1 or 2 and nothing else. Below are your rules:\n",
    "\n",
    "==== RULES: ====\n",
    "\n",
    "If the ==== APP DESCRIPTION: ==== section is not related to FastKafka or contains violence, self-harm, harassment/threatening or hate/threatening information then you should respond with 0.\n",
    "\n",
    "If the ==== APP DESCRIPTION: ==== section is related to FastKafka but focuses on what is it and its general information then you should respond with 1. \n",
    "\n",
    "If the ==== APP DESCRIPTION: ==== section is related to FastKafka but focuses how to use it and instructions to create a new app then you should respond with 2. \n",
    "\"\"\"\n",
    "\n",
    "ai = CustomAIChat(user_prompt = TEST_INITIAL_USER_PROMPT, model=OpenAIModel.gpt3.value)\n",
    "response, usage = ai(\"Name the tallest mountain in the world\")\n",
    "\n",
    "print(response)\n",
    "print(usage)\n",
    "\n",
    "assert response == \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d47b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class ValidateAndFixResponse:\n",
    "    \"\"\"Generates and validates response from OpenAI\n",
    "\n",
    "    Attributes:\n",
    "        generate: A callable object for generating responses.\n",
    "        validate: A callable object for validating responses.\n",
    "        max_retries: An optional integer specifying the maximum number of attempts to generate and validate a response.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        generate: Callable[..., Any],\n",
    "        validate: Callable[..., Any],\n",
    "        max_retries: Optional[int] = MAX_RETRIES,\n",
    "    ):\n",
    "        self.generate = generate\n",
    "        self.validate = validate\n",
    "        self.max_retries = max_retries\n",
    "\n",
    "    def fix(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        total_usage: List[Dict[str, int]],\n",
    "        step_name: Optional[str] = None,\n",
    "        log_dir_path: Optional[str] = None,\n",
    "        **kwargs: Dict[str, Any],\n",
    "    ) -> Tuple[str, List[Dict[str, int]]]:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d9626d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def _save_log_results(\n",
    "    step_name: str,\n",
    "    log_dir_path: str,\n",
    "    messages: List[Dict[str, str]],\n",
    "    response: str,\n",
    "    error_str: str,\n",
    "    retry_cnt: int,\n",
    "    **kwargs: Dict[str, int],\n",
    ") -> None:\n",
    "    if log_dir_path is not None and \"attempt\" in kwargs:\n",
    "        step_dir = Path(log_dir_path) / step_name\n",
    "        step_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        attempt_dir = step_dir / f'attempt_{kwargs[\"attempt\"] + 1}'  # type: ignore\n",
    "        attempt_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        try_dir = attempt_dir / f\"try_{retry_cnt+1}\"\n",
    "        try_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        formatted_msg = \"\\n\".join(\n",
    "            [f\"===={m['role']}====\\n\\n{m['content']}\\n\\n\" for m in messages]\n",
    "        )\n",
    "\n",
    "        with open((try_dir / \"input.txt\"), \"w\", encoding=\"utf-8\") as f_input, open(\n",
    "            (try_dir / \"output.txt\"), \"w\", encoding=\"utf-8\"\n",
    "        ) as f_output, open(\n",
    "            (try_dir / \"errors.txt\"), \"w\", encoding=\"utf-8\"\n",
    "        ) as f_errors:\n",
    "            f_input.write(formatted_msg)\n",
    "            f_output.write(response)\n",
    "            f_errors.write(error_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f8d129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('/tmp/tmpb96cmfze/app/attempt_3/try_1/errors.txt'), PosixPath('/tmp/tmpb96cmfze/app/attempt_3/try_1/output.txt'), PosixPath('/tmp/tmpb96cmfze/app/attempt_3/try_1/input.txt')]\n",
      "[PosixPath('/tmp/tmpb96cmfze/test/attempt_3/try_1/errors.txt'), PosixPath('/tmp/tmpb96cmfze/test/attempt_3/try_1/output.txt'), PosixPath('/tmp/tmpb96cmfze/test/attempt_3/try_1/input.txt')]\n"
     ]
    }
   ],
   "source": [
    "with TemporaryDirectory() as d:\n",
    "    messages = [{\"role\": \"role\", \"content\": \"content\"}]\n",
    "    kwargs = {\"attempt\": 2}\n",
    "    for step_name in [\"app\", \"test\"]:\n",
    "        _save_log_results(step_name, d, messages, \"response\", \"error_str\", 0, **kwargs)\n",
    "\n",
    "        step_dir = Path(d) / step_name\n",
    "        assert step_dir.exists()\n",
    "\n",
    "        attempt_dir = step_dir / \"attempt_3\"\n",
    "        assert attempt_dir.exists()\n",
    "\n",
    "        try_dir = attempt_dir / \"try_1\"\n",
    "        assert try_dir.exists()\n",
    "\n",
    "        print(list(Path(try_dir).glob('**/*')))\n",
    "        assert (Path(d) / step_dir / \"attempt_3\" / f\"try_1\" / \"input.txt\").exists()\n",
    "        assert (Path(d) / step_dir / \"attempt_3\" / f\"try_1\" / \"output.txt\").exists()\n",
    "        assert (Path(d) / step_dir / \"attempt_3\" / f\"try_1\" / \"errors.txt\").exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f6abb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def _construct_prompt_with_error_msg(\n",
    "    response: str,\n",
    "    errors: str,\n",
    ") -> str:\n",
    "    \"\"\"Construct prompt message along with the error message.\n",
    "\n",
    "    Args:\n",
    "        prompt: The original prompt string.\n",
    "        response: The invalid response string from OpenAI.\n",
    "        errors: The errors which needs to be fixed in the invalid response.\n",
    "\n",
    "    Returns:\n",
    "        A string combining the original prompt, invalid response, and the error message.\n",
    "    \"\"\"\n",
    "    prompt_with_errors = (\n",
    "        f\"\\n\\n==== YOUR RESPONSE (WITH ISSUES) ====\\n\\n{response}\"\n",
    "        + f\"\\n\\nRead the contents of ==== YOUR RESPONSE (WITH ISSUES) ==== section and fix the below mentioned issues:\\n\\n{errors}\"\n",
    "    )\n",
    "    return prompt_with_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631cff6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==== YOUR RESPONSE (WITH ISSUES) ====\n",
      "\n",
      "some response\n",
      "\n",
      "Read the contents of ==== YOUR RESPONSE (WITH ISSUES) ==== section and fix the below mentioned issues:\n",
      "\n",
      "error 1\n",
      "error 2\n",
      "error 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = \"some response\"\n",
    "errors = \"\"\"error 1\n",
    "error 2\n",
    "error 3\n",
    "\"\"\"\n",
    "\n",
    "expected = \"\"\"\n",
    "\n",
    "==== YOUR RESPONSE (WITH ISSUES) ====\n",
    "\n",
    "some response\n",
    "\n",
    "Read the contents of ==== YOUR RESPONSE (WITH ISSUES) ==== section and fix the below mentioned issues:\n",
    "\n",
    "error 1\n",
    "error 2\n",
    "error 3\n",
    "\"\"\"\n",
    "actual = _construct_prompt_with_error_msg(response, errors)\n",
    "print(actual)\n",
    "\n",
    "assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c631ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch  # type: ignore\n",
    "def fix(\n",
    "    self: ValidateAndFixResponse,\n",
    "    prompt: str,\n",
    "    total_usage: List[Dict[str, int]],\n",
    "    step_name: str,\n",
    "    output_directory: str,\n",
    "    **kwargs: Dict[str, Any],\n",
    ") -> List[Dict[str, int]]:\n",
    "    \"\"\"Fix the response from OpenAI until no errors remain or maximum number of attempts is reached.\n",
    "\n",
    "    Args:\n",
    "        prompt: The initial prompt string.\n",
    "        kwargs: Additional keyword arguments to be passed to the validation function.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated response that has passed the validation.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the maximum number of attempts is exceeded and the response has not successfully passed the validation.\n",
    "    \"\"\"\n",
    "    total_tokens_usage: Dict[str, int] = defaultdict(int)\n",
    "    log_dir_path = Path(output_directory) / LOGS_DIR_NAME\n",
    "    for i in range(self.max_retries):  # type: ignore\n",
    "        response, usage = self.generate(prompt)\n",
    "        total_tokens_usage = add_tokens_usage([total_tokens_usage, usage])\n",
    "        \n",
    "        errors = self.validate(response, output_directory, **kwargs)\n",
    "        error_str = \"\\n\".join(errors)\n",
    "        _save_log_results(\n",
    "            step_name,\n",
    "            str(log_dir_path),\n",
    "            self.generate.messages,  # type: ignore\n",
    "            response,\n",
    "            error_str,\n",
    "            i,\n",
    "            **kwargs,\n",
    "        )\n",
    "        if len(errors) == 0:\n",
    "            total_usage.append(total_tokens_usage)\n",
    "            return total_usage\n",
    "\n",
    "        self.generate.messages[-1][\"content\"] = self.generate.messages[-1][ # type: ignore\n",
    "            \"content\"\n",
    "        ].rsplit(\"==== YOUR RESPONSE ====\", 1)[0]\n",
    "        prompt = _construct_prompt_with_error_msg(response, error_str)\n",
    "        logger.info(f\"Validation failed, trying again...Errors:\\n{error_str}\")\n",
    "\n",
    "    total_usage.append(total_tokens_usage)\n",
    "    \n",
    "    # we send False to notify the generated code contains bugs\n",
    "    raise ValueError(total_usage, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef480ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[defaultdict(<class 'int'>, {'prompt_tokens': 129, 'completion_tokens': 1, 'total_tokens': 130})]\n",
      "====system====\n",
      "\n",
      "\n",
      "You are an expert Python developer, tasked to generate executable Python code as a part of your work with the FastStream framework. \n",
      "\n",
      "You are to abide by the following guidelines:\n",
      "\n",
      "1. You must never enclose the generated Python code with ``` python. It is mandatory that the output is a valid and executable Python code. Please ensure this rule is never broken.\n",
      "\n",
      "2. Some prompts might require you to generate code that contains async functions. For example:\n",
      "\n",
      "async def app_setup(context: ContextRepo):\n",
      "    raise NotImplementedError()\n",
      "\n",
      "In such cases, it is necessary to add the \"import asyncio\" statement at the top of the code. \n",
      "\n",
      "You will encounter sections marked as:\n",
      "\n",
      "==== APP DESCRIPTION: ====\n",
      "\n",
      "These sections contain the description of the FastStream app you need to implement. Treat everything below this line, until the end of the prompt, as the description to follow for the app implementation.\n",
      "\n",
      "\n",
      "\n",
      "====user====\n",
      "\n",
      "some valid prompt\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fixture_initial_prompt = \"some valid prompt\"\n",
    "expected = \"some valid prompt\"\n",
    "max_retries = 3\n",
    "\n",
    "\n",
    "class FixtureGenerate:\n",
    "    def __init__(self, user_prompt):\n",
    "        self.messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
    "\n",
    "    def __call__(self, prompt):\n",
    "        self.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        usage = {\"prompt_tokens\": 129, \"completion_tokens\": 1, \"total_tokens\": 130}\n",
    "        return fixture_initial_prompt, usage\n",
    "    \n",
    "def fixture_validate(response, output_directory, attempt):\n",
    "        return []\n",
    "\n",
    "with TemporaryDirectory() as d:\n",
    "    kwargs = {\"attempt\": 0}\n",
    "    fixture_generate = FixtureGenerate(fixture_initial_prompt)\n",
    "    v = ValidateAndFixResponse(fixture_generate, fixture_validate, max_retries)\n",
    "    actual = v.fix(fixture_initial_prompt, [], STEP_LOG_DIR_NAMES[\"app\"], d, **kwargs)\n",
    "    print(actual)\n",
    "    \n",
    "    assert (Path(d) / LOGS_DIR_NAME / STEP_LOG_DIR_NAMES[\"app\"] / \"attempt_1\" / f\"try_1\").exists()\n",
    "    assert (Path(d) / LOGS_DIR_NAME / STEP_LOG_DIR_NAMES[\"app\"] / \"attempt_1\" / f\"try_1\" / \"input.txt\").exists()\n",
    "    assert (Path(d) / LOGS_DIR_NAME / STEP_LOG_DIR_NAMES[\"app\"] / \"attempt_1\" / f\"try_1\" / \"output.txt\").exists()\n",
    "    assert (Path(d) / LOGS_DIR_NAME / STEP_LOG_DIR_NAMES[\"app\"] / \"attempt_1\" / f\"try_1\" / \"errors.txt\").exists()\n",
    "\n",
    "    with open((Path(d) / LOGS_DIR_NAME / STEP_LOG_DIR_NAMES[\"app\"] / \"attempt_1\" / f\"try_1\" / \"input.txt\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11329664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: Validation failed, trying again...Errors:\n",
      "error 1\n",
      "error 2\n",
      "[INFO] __main__: Validation failed, trying again...Errors:\n",
      "error 1\n",
      "error 2\n",
      "[INFO] __main__: Validation failed, trying again...Errors:\n",
      "error 1\n",
      "error 2\n",
      "e.value=ValueError([defaultdict(<class 'int'>, {'prompt_tokens': 387, 'completion_tokens': 3, 'total_tokens': 390})], False)\n",
      "====system====\n",
      "\n",
      "\n",
      "You are an expert Python developer, tasked to generate executable Python code as a part of your work with the FastStream framework. \n",
      "\n",
      "You are to abide by the following guidelines:\n",
      "\n",
      "1. You must never enclose the generated Python code with ``` python. It is mandatory that the output is a valid and executable Python code. Please ensure this rule is never broken.\n",
      "\n",
      "2. Some prompts might require you to generate code that contains async functions. For example:\n",
      "\n",
      "async def app_setup(context: ContextRepo):\n",
      "    raise NotImplementedError()\n",
      "\n",
      "In such cases, it is necessary to add the \"import asyncio\" statement at the top of the code. \n",
      "\n",
      "You will encounter sections marked as:\n",
      "\n",
      "==== APP DESCRIPTION: ====\n",
      "\n",
      "These sections contain the description of the FastStream app you need to implement. Treat everything below this line, until the end of the prompt, as the description to follow for the app implementation.\n",
      "\n",
      "\n",
      "\n",
      "====user====\n",
      "\n",
      "some invalid prompt\n",
      "\n",
      "\n",
      "====user====\n",
      "\n",
      "\n",
      "\n",
      "==== YOUR RESPONSE (WITH ISSUES) ====\n",
      "\n",
      "some invalid prompt\n",
      "\n",
      "Read the contents of ==== YOUR RESPONSE (WITH ISSUES) ==== section and fix the below mentioned issues:\n",
      "\n",
      "error 1\n",
      "error 2\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fixture_initial_prompt = \"some invalid prompt\"\n",
    "max_retries = 3\n",
    "\n",
    "\n",
    "class FixtureGenerate:\n",
    "    def __init__(self, user_prompt):\n",
    "        self.messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
    "\n",
    "    def __call__(self, prompt):\n",
    "        self.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        usage = {\"prompt_tokens\": 129, \"completion_tokens\": 1, \"total_tokens\": 130}\n",
    "        return fixture_initial_prompt, usage\n",
    "\n",
    "\n",
    "fixture_generate = FixtureGenerate(fixture_initial_prompt)\n",
    "\n",
    "with TemporaryDirectory() as d:\n",
    "    def fixture_validate(response, output_path, attempt):\n",
    "        return [\"error 1\", \"error 2\"]\n",
    "\n",
    "    with pytest.raises(ValueError) as e:\n",
    "        kwargs = {\"attempt\": 0}\n",
    "        v = ValidateAndFixResponse(fixture_generate, fixture_validate, max_retries)\n",
    "        actual = v.fix(fixture_initial_prompt, [], STEP_LOG_DIR_NAMES[\"app\"], d, **kwargs)\n",
    "    \n",
    "    print(f\"{e.value=}\")\n",
    "    assert not e.value.args[1]\n",
    "\n",
    "    for i in range(3):\n",
    "        assert (Path(d) / LOGS_DIR_NAME / STEP_LOG_DIR_NAMES[\"app\"] / \"attempt_1\" / f\"try_{i+1}\").exists()\n",
    "        assert (Path(d) / LOGS_DIR_NAME / STEP_LOG_DIR_NAMES[\"app\"] / \"attempt_1\" / f\"try_{i+1}\" / \"input.txt\").exists()\n",
    "        assert (Path(d) / LOGS_DIR_NAME / STEP_LOG_DIR_NAMES[\"app\"] / \"attempt_1\" / f\"try_{i+1}\" / \"output.txt\").exists()\n",
    "        assert (Path(d) / LOGS_DIR_NAME / STEP_LOG_DIR_NAMES[\"app\"] / \"attempt_1\" / f\"try_{i+1}\" / \"errors.txt\").exists()\n",
    "        \n",
    "    with open((Path(d) / LOGS_DIR_NAME / STEP_LOG_DIR_NAMES[\"app\"] / \"attempt_1\" / f\"try_2\" / \"input.txt\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab08ec4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: Validation failed, trying again...Errors:\n",
      "error 1\n",
      "error 2\n",
      "[INFO] __main__: Validation failed, trying again...Errors:\n",
      "error 1\n",
      "error 2\n",
      "[INFO] __main__: Validation failed, trying again...Errors:\n",
      "error 1\n",
      "error 2\n",
      "([defaultdict(<class 'int'>, {'prompt_tokens': 387, 'completion_tokens': 3, 'total_tokens': 390})], False)\n",
      "====system====\n",
      "\n",
      "\n",
      "You are an expert Python developer, tasked to generate executable Python code as a part of your work with the FastStream framework. \n",
      "\n",
      "You are to abide by the following guidelines:\n",
      "\n",
      "1. You must never enclose the generated Python code with ``` python. It is mandatory that the output is a valid and executable Python code. Please ensure this rule is never broken.\n",
      "\n",
      "2. Some prompts might require you to generate code that contains async functions. For example:\n",
      "\n",
      "async def app_setup(context: ContextRepo):\n",
      "    raise NotImplementedError()\n",
      "\n",
      "In such cases, it is necessary to add the \"import asyncio\" statement at the top of the code. \n",
      "\n",
      "You will encounter sections marked as:\n",
      "\n",
      "==== APP DESCRIPTION: ====\n",
      "\n",
      "These sections contain the description of the FastStream app you need to implement. Treat everything below this line, until the end of the prompt, as the description to follow for the app implementation.\n",
      "\n",
      "\n",
      "\n",
      "====user====\n",
      "\n",
      "some invalid prompt\n",
      "\n",
      "\n",
      "====user====\n",
      "\n",
      "\n",
      "\n",
      "==== YOUR RESPONSE (WITH ISSUES) ====\n",
      "\n",
      "some invalid prompt\n",
      "\n",
      "Read the contents of ==== YOUR RESPONSE (WITH ISSUES) ==== section and fix the below mentioned issues:\n",
      "\n",
      "error 1\n",
      "error 2\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fixture_initial_prompt = \"some invalid prompt\"\n",
    "max_retries = 3\n",
    "\n",
    "\n",
    "class FixtureGenerate:\n",
    "    def __init__(self, user_prompt):\n",
    "        self.messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
    "\n",
    "    def __call__(self, prompt):\n",
    "        self.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        usage = {\"prompt_tokens\": 129, \"completion_tokens\": 1, \"total_tokens\": 130}\n",
    "        return fixture_initial_prompt, usage\n",
    "\n",
    "\n",
    "fixture_generate = FixtureGenerate(fixture_initial_prompt)\n",
    "\n",
    "with TemporaryDirectory() as d:\n",
    "    def fixture_validate(response, output_path, attempt):\n",
    "        return [\"error 1\", \"error 2\"]\n",
    "\n",
    "    with pytest.raises(ValueError) as e:\n",
    "        kwargs = {\"attempt\": 0}\n",
    "        v = ValidateAndFixResponse(fixture_generate, fixture_validate, max_retries)\n",
    "        actual = v.fix(fixture_initial_prompt, [], STEP_LOG_DIR_NAMES[\"skeleton\"], d, **kwargs)\n",
    "    \n",
    "    print(e.value)\n",
    "    assert not e.value.args[1]\n",
    "\n",
    "    for i in range(3):\n",
    "        assert (Path(d) / LOGS_DIR_NAME / STEP_LOG_DIR_NAMES[\"skeleton\"] / \"attempt_1\" / f\"try_{i+1}\").exists()\n",
    "        assert (Path(d) / LOGS_DIR_NAME / STEP_LOG_DIR_NAMES[\"skeleton\"] / \"attempt_1\" / f\"try_{i+1}\" / \"input.txt\").exists()\n",
    "        assert (Path(d) / LOGS_DIR_NAME / STEP_LOG_DIR_NAMES[\"skeleton\"] / \"attempt_1\" / f\"try_{i+1}\" / \"output.txt\").exists()\n",
    "        assert (Path(d) / LOGS_DIR_NAME / STEP_LOG_DIR_NAMES[\"skeleton\"] / \"attempt_1\" / f\"try_{i+1}\" / \"errors.txt\").exists()\n",
    "        \n",
    "    with open((Path(d) / LOGS_DIR_NAME / STEP_LOG_DIR_NAMES[\"skeleton\"] / \"attempt_1\" / f\"try_2\" / \"input.txt\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        print(f.read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
