{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6067a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259bd40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from typing import *\n",
    "\n",
    "import typer\n",
    "from pathlib import Path\n",
    "\n",
    "from faststream_gen._components.logger import get_logger\n",
    "from faststream_gen._code_generator.app_description_validator import (\n",
    "    validate_app_description,\n",
    ")\n",
    "from faststream_gen._code_generator.helper import (\n",
    "    set_logger_level,\n",
    "    add_tokens_usage,\n",
    "    get_relevant_prompt_examples,\n",
    "    strip_white_spaces,\n",
    "    write_file_contents,\n",
    "    ensure_openai_api_key_set,\n",
    ")\n",
    "from faststream_gen._code_generator.constants import MODEL_PRICING, EMPTY_DESCRIPTION_ERROR, OpenAIModel\n",
    "from faststream_gen._components.new_project_generator import create_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e74720c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typer.testing import CliRunner\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "import pytest\n",
    "\n",
    "from faststream_gen._components.logger import suppress_timestamps\n",
    "from faststream_gen._code_generator.helper import mock_openai_create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25b7ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe03136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: ok\n"
     ]
    }
   ],
   "source": [
    "suppress_timestamps()\n",
    "logger = get_logger(__name__, level=20)\n",
    "logger.info(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baab0afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = CliRunner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720e3ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "app = typer.Typer(\n",
    "    short_help=\"Commands for accelerating FastStream project creation using advanced AI technology\",\n",
    "     help=\"\"\"Commands for accelerating FastStream project creation using advanced AI technology.\n",
    "\n",
    "These commands use OpenAI's model to generate FastStream project. To access this feature, kindly sign up if you haven't already and create an API key with OpenAI. You can generate API keys in the OpenAI web interface. See https://platform.openai.com/account/api-keys for details.\n",
    "\n",
    "Once you have the key, please set it in the OPENAI_API_KEY environment variable before executing the code generation commands.\n",
    "\n",
    "Note: Accessing OpenAI API incurs charges. For further information on pricing and free credicts, check this link: https://openai.com/pricing\n",
    "    \"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2263d5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def _calculate_price(total_tokens_usage: Dict[str, int], model: str) -> float:\n",
    "    \"\"\"Calculates the total price based on the number of promt & completion tokens and the models price for input and output tokens (per 1k tokens).\n",
    "\n",
    "    Args:\n",
    "        total_tokens_usage: OpenAI \"usage\" dictionaries which defines prompt_tokens, completion_tokens and total_tokens\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        float: The price for used tokens\n",
    "    \"\"\"\n",
    "    model_price = MODEL_PRICING[model]\n",
    "    price = (total_tokens_usage[\"prompt_tokens\"] * model_price[\"input\"] + total_tokens_usage[\"completion_tokens\"] * model_price[\"output\"]) / 1000\n",
    "    return price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56799d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.007859999999999999\n"
     ]
    }
   ],
   "source": [
    "usage = {\"prompt_tokens\": 129, \"completion_tokens\": 1, \"total_tokens\": 130}\n",
    "\n",
    "usage_list = [usage, usage]\n",
    "total_tokens_usage = add_tokens_usage(usage_list)\n",
    "\n",
    "actual = _calculate_price(total_tokens_usage, \"gpt-4\")\n",
    "expected = 0.007859999999999999\n",
    "\n",
    "print(actual)\n",
    "assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cdf954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "\n",
    "def _get_description(input_path: str) -> str:\n",
    "    \"\"\"Reads description from te file and returns it as a string\n",
    "\n",
    "    Args:\n",
    "        input_path: Path to the file with the desription\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If the file does not exist.\n",
    "\n",
    "    Returns:\n",
    "        The description string which was read from the file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(input_path) as file:\n",
    "            # Read all lines to list\n",
    "            lines = file.readlines()\n",
    "            # Join the lines \n",
    "            description = '\\r'.join(lines)\n",
    "            logger.info(f\"Reading application description from '{str(Path(input_path).absolute())}'.\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error while reading from the file: '{str(Path(input_path).absolute())}'\\n{str(e)}\")\n",
    "    return description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664a0d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: Reading application description from '/tmp/tmp_8r2wav6/app_description.txt'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Create a FastKafka application using localhost broker for testing, staging.airt.ai for staging and prod.airt.ai for production. Use default port number.\\n\\r\\n\\rIt should consume from 'store_product' topic an JSON encoded object with the following three attributes: product_name, currency and price. The format of the currency will be three letter string, e.g. 'EUR'.\\n\\rFor each consumed message, check if the currency attribute is set to 'HRK'. If it is then change the currency to 'EUR' and divide the price by 7.5, if the currency is not set to 'HRK' don't change the original message. Finally, publish the consumed message to 'change_currency' topic.\\n\\r\\n\\rUse SASL_SSL with SCRAM-SHA-256 for authentication with username and password.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with TemporaryDirectory() as d:\n",
    "    input_path = f\"{d}/app_description.txt\"\n",
    "    description = \"\"\"Create a FastKafka application using localhost broker for testing, staging.airt.ai for staging and prod.airt.ai for production. Use default port number.\n",
    "\n",
    "It should consume from 'store_product' topic an JSON encoded object with the following three attributes: product_name, currency and price. The format of the currency will be three letter string, e.g. 'EUR'.\n",
    "For each consumed message, check if the currency attribute is set to 'HRK'. If it is then change the currency to 'EUR' and divide the price by 7.5, if the currency is not set to 'HRK' don't change the original message. Finally, publish the consumed message to 'change_currency' topic.\n",
    "\n",
    "Use SASL_SSL with SCRAM-SHA-256 for authentication with username and password.\"\"\"\n",
    "    \n",
    "    \n",
    "    write_file_contents(input_path, description)\n",
    "    actual = _get_description(input_path)\n",
    "    display(actual)\n",
    "    expected = \"Create a FastKafka application using localhost broker for testing, staging.airt.ai for staging and prod.airt.ai for production. Use default port number.\\n\\r\\n\\rIt should consume from 'store_product' topic an JSON encoded object with the following three attributes: product_name, currency and price. The format of the currency will be three letter string, e.g. 'EUR'.\\n\\rFor each consumed message, check if the currency attribute is set to 'HRK'. If it is then change the currency to 'EUR' and divide the price by 7.5, if the currency is not set to 'HRK' don't change the original message. Finally, publish the consumed message to 'change_currency' topic.\\n\\r\\n\\rUse SASL_SSL with SCRAM-SHA-256 for authentication with username and password.\"         \n",
    "    \n",
    "    assert actual == expected, actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6483a24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while reading from the file: '/work/fastkafka-gen/nbs/incorrect_path'\n",
      "[Errno 2] No such file or directory: 'incorrect_path'\n"
     ]
    }
   ],
   "source": [
    "with pytest.raises(ValueError) as e:\n",
    "        _get_description(\"incorrect_path\")\n",
    "        \n",
    "print(e.value)\n",
    "assert str(e.value) == f\"Error while reading from the file: '{str(Path('incorrect_path').absolute())}'\\n[Errno 2] No such file or directory: 'incorrect_path'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dd701a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def _validate_app_description(\n",
    "    description: Optional[str],\n",
    "    input_path: str,\n",
    "    model: OpenAIModel,\n",
    "    tokens_list: List[Dict[str, int]],\n",
    ") -> Tuple[str, List[Dict[str, int]]]:\n",
    "    if not description:\n",
    "        if not input_path:\n",
    "            raise ValueError(EMPTY_DESCRIPTION_ERROR)\n",
    "        description = _get_description(input_path)\n",
    "        cleaned_description = strip_white_spaces(description)\n",
    "        return validate_app_description(cleaned_description, model.value, tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22897a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pytest.raises(ValueError) as e:\n",
    "    _validate_app_description(None, None, OpenAIModel.gpt3, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fafc8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_response = \"3\"\n",
    "\n",
    "with mock_openai_create(test_response):\n",
    "    _validate_app_description(\"some valid description\", None, OpenAIModel.gpt3, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fe5ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: Reading application description from '/tmp/tmp43mtmta8/input.txt'.\n",
      "✨  Generating a new FastStream application!\n",
      "[INFO] faststream_gen._code_generator.app_description_validator: ==== App description validation ====\n",
      "⠹ Validating the application description... [INFO] faststream_gen._code_generator.chat: \n",
      "\n",
      "Prompt to the model: \n",
      "\n",
      "===Role:system===\n",
      "\n",
      "Message:\n",
      "\n",
      "You are an expert Python developer, tasked to generate executable Python code as a part of your work with the FastStream framework. \n",
      "\n",
      "You are to abide by the following guidelines:\n",
      "\n",
      "1. You must never enclose the generated Python code with ``` python. It is mandatory that the output is a valid and executable Python code. Please ensure this rule is never broken.\n",
      "\n",
      "2. Some prompts might require you to generate code that contains async functions. For example:\n",
      "\n",
      "async def app_setup(context: ContextRepo):\n",
      "    raise NotImplementedError()\n",
      "\n",
      "In such cases, it is necessary to add the \"import asyncio\" statement at the top of the code. \n",
      "\n",
      "You will encounter sections marked as:\n",
      "\n",
      "==== APP DESCRIPTION: ====\n",
      "\n",
      "These sections contain the description of the FastStream app you need to implement. Treat everything below this line, until the end of the prompt, as the description to follow for the app implementation.\n",
      "\n",
      "\n",
      "===Role:user===\n",
      "\n",
      "Message:\n",
      "hide:\n",
      "  - navigation\n",
      "  - footer\n",
      "\n",
      "Release Notes\n",
      "\n",
      "FastStream is a new package based on the ideas and experiences gained from FastKafka and Propan. By joining our forces, we picked up the best from both packages and created the unified way to write services capable of processing streamed data regradless of the underliying protocol. We'll continue to maintain both packages, but new development will be in this project. If you are starting a new service, this package is the recommended way to do it.\n",
      "\n",
      "Features\n",
      "\n",
      "FastStream simplifies the process of writing producers and consumers for message queues, handling all the\n",
      "parsing, networking and documentation generation automatically.\n",
      "\n",
      "Making streaming microservices has never been easier. Designed with junior developers in mind, FastStream simplifies your work while keeping the door open for more advanced use-cases. Here's a look at the core features that make FastStream a go-to framework for modern, data-centric microservices.\n",
      "\n",
      "Multiple Brokers: FastStream provides a unified API to work across multiple message brokers (Kafka, RabbitMQ support)\n",
      "\n",
      "Pydantic Validation: Leverage Pydantic's validation capabilities to serialize and validates incoming messages\n",
      "\n",
      "Automatic Docs: Stay ahead with automatic AsyncAPI documentation.\n",
      "\n",
      "Intuitive: full typed editor support makes your development experience smooth, catching errors before they reach runtime\n",
      "\n",
      "Powerful Dependency Injection System: Manage your service dependencies efficiently with FastStream's built-in DI system.\n",
      "\n",
      "Testable: supports in-memory tests, making your CI/CD pipeline faster and more reliable\n",
      "\n",
      "Extendable: use extensions for lifespans, custom serialization and middlewares\n",
      "\n",
      "Integrations: FastStream is fully compatible with any HTTP framework you want (FastAPI especially)\n",
      "\n",
      "Built for Automatic Code Generation: FastStream is optimized for automatic code generation using advanced models like GPT and Llama\n",
      "\n",
      "That's FastStream in a nutshell—easy, efficient, and powerful. Whether you're just starting with streaming microservices or looking to scale, FastStream has got you covered.\n",
      "\n",
      "===Role:user===\n",
      "\n",
      "Message:\n",
      "\n",
      "You should provide a response of 0, 1, 2, or 3, and nothing else, based on the following rules:\n",
      "\n",
      "==== RULES: ====\n",
      "\n",
      "If the ==== APP DESCRIPTION: ==== section is not related to FastStream or contains violence, self-harm, harassment/threatening, or hate/threatening information, respond with 0.\n",
      "\n",
      "If the ==== APP DESCRIPTION: ==== section is related to FastStream but primarily provides general information about FastStream and what it is, respond with 1.\n",
      "\n",
      "If it is NOT possible to infer the topic name or there is no explanation about the business logic in the ==== APP DESCRIPTION: ==== section, respond with 2. This is crucial.\n",
      "\n",
      "If the ==== APP DESCRIPTION: ==== section is related to FastStream, provides instructions on which topic the messages should be consumed/produced, and includes at least one defined topic, respond with 3.\n",
      "\n",
      "Here are few examples for your understanding:\n",
      "\n",
      "\n",
      "==== EXAMPLE APP DESCRIPTION ====\n",
      "Generate a new FastStream app, which has a producer function and a consumer function \n",
      "==== YOUR RESPONSE ====\n",
      "2\n",
      "\n",
      "==== EXAMPLE APP DESCRIPTION ====\n",
      "In App description 1, the user has not defined the message structure or the topic name to publish/subscribe. As a result, you should respond with 2. \n",
      "==== YOUR RESPONSE ====\n",
      "2\n",
      "\n",
      "==== EXAMPLE APP DESCRIPTION ====\n",
      "Create a FastStream application.\n",
      "==== YOUR RESPONSE ====\n",
      "2\n",
      "\n",
      "==== EXAMPLE APP DESCRIPTION ====\n",
      "create FastStream app where message has user_data attribute.\n",
      "==== YOUR RESPONSE ====\n",
      "2\n",
      "\n",
      "==== EXAMPLE APP DESCRIPTION ====\n",
      "FastStream app with for consuming messages from the hello topic\n",
      "==== YOUR RESPONSE ====\n",
      "3\n",
      "\n",
      "==== EXAMPLE APP DESCRIPTION ====\n",
      "Write a FastStream application with with one consumer function and two producer functions. The consumer function should receive the a message posted on \"new_joinee\" topic. The message should contain \"employee_name\", \"age\", \"location\" and \"experience\" attributes. After consuming the consumer function should send the details to the \"project_team\" and \"admin_team\" topics. Use only localhost broker==== Response 5 ====\n",
      "==== YOUR RESPONSE ====\n",
      "3\n",
      "\n",
      "==== EXAMPLE APP DESCRIPTION ====\n",
      "Develop a new FastStream application that consumes JSON-encoded objects from the \"receive_order\" topic. These objects include attributes like \"name\" and \"quantity.\" Upon consumption, enhance the message by adding a \"location\" attribute set to \"Zagreb.\" Subsequently, forward the modified message to the \"place_order\" topic. After this, send another message to the \"update_inventory\" topic. This message should include a \"quantity\" attribute that corresponds to the received quantity value. No authentication is required.==== Response 6 ====\n",
      "==== YOUR RESPONSE ====\n",
      "3\n",
      "\n",
      "==== EXAMPLE APP DESCRIPTION ====\n",
      "Who are you\n",
      "==== YOUR RESPONSE ====\n",
      "0\n",
      "\n",
      "==== EXAMPLE APP DESCRIPTION ====\n",
      "What is the latest vesion of FastStream\n",
      "==== YOUR RESPONSE ====\n",
      "1\n",
      "\n",
      "Please respond only with numbers 0, 1, 2 or 3 (WITH NO ADDITIONAL TEXT!)\n",
      "\n",
      "==== APP DESCRIPTION: ====\n",
      "\n",
      "\n",
      "\n",
      "===Role:user===\n",
      "\n",
      "Message:\n",
      "app description\n",
      "==== YOUR RESPONSE ====\n",
      "\n",
      " ✔ Application description validated.       \n"
     ]
    }
   ],
   "source": [
    "test_response = \"3\"\n",
    "\n",
    "with TemporaryDirectory() as d:\n",
    "    input_path = Path(d)/\"input.txt\"\n",
    "    with open(input_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"app description\")\n",
    "    \n",
    "    with mock_openai_create(test_response):\n",
    "        _validate_app_description(None, str(input_path), OpenAIModel.gpt3, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a85689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@app.command(\n",
    "    \"generate\",\n",
    "    help=\"Effortlessly create a new FastStream project based on the app description.\",\n",
    ")\n",
    "@set_logger_level\n",
    "def generate_fastkafka_app(\n",
    "    description: Optional[str] = typer.Argument(\n",
    "        None,\n",
    "        help=\"\"\"Summarize your FastStream application in a few sentences!\n",
    "\n",
    "\n",
    "\\nInclude details about messages, topics, servers, and a brief overview of the intended business logic.\n",
    "\n",
    "\n",
    "\\nThe simpler and more specific the app description is, the better the generated app will be. Please refer to the below example for inspiration:\n",
    "\n",
    "\n",
    "\\nCreate a FastStream application using localhost broker for testing and use the default port number. \n",
    "It should consume messages from the \"input_data\" topic, where each message is a JSON encoded object containing a single attribute: 'data'. \n",
    "For each consumed message, create a new message object and increment the value of the data attribute by 1. Finally, send the modified message to the 'output_data' topic.\n",
    "\\n\"\"\",\n",
    "    ),\n",
    "    input_path: str = typer.Option(\n",
    "        None,\n",
    "        \"--input_file\",\n",
    "        \"-i\",\n",
    "        help=\"\"\"\n",
    "        The path to the file with the app desription. This path should be relative to the current working directory.\n",
    "        \n",
    "        \\n\\nIf the app description is passed via both a --input_file and a command line argument, the description from the command line will be used to create the application.\n",
    "        \"\"\",\n",
    "    ),\n",
    "    output_path: str = typer.Option(\n",
    "        \".\",\n",
    "        \"--output_path\",\n",
    "        \"-o\",\n",
    "        help=\"The path to the output directory where the generated project files will be saved. This path should be relative to the current working directory.\",\n",
    "    ),\n",
    "    model: OpenAIModel = typer.Option(\n",
    "        OpenAIModel.gpt3.value,\n",
    "        \"--model\",\n",
    "        \"-m\",\n",
    "        help=f\"The OpenAI model that will be used to create the FastStream project. For better results, we recommend using '{OpenAIModel.gpt4.value}'.\",\n",
    "    ),\n",
    "    verbose: bool = typer.Option(\n",
    "        False,\n",
    "        \"--verbose\",\n",
    "        \"-v\",\n",
    "        help=\"Enable verbose logging by setting the logger level to INFO.\",\n",
    "    ),\n",
    "    save_log_files: bool = typer.Option(\n",
    "        False,\n",
    "        \"--dev\",\n",
    "        \"-d\",\n",
    "        help=\"Save the complete logs generated by faststream-gen inside the output_path directory.\",\n",
    "    ),\n",
    ") -> None:\n",
    "    \"\"\"Effortlessly create a new FastStream project based on the app description.\"\"\"\n",
    "    logger.info(\"Project generation started.\")\n",
    "    try:\n",
    "        tokens_list: List[Dict[str, int]] = []\n",
    "        ensure_openai_api_key_set()\n",
    "        \n",
    "        # Step 1: Validate description\n",
    "        validated_description, tokens_list = _validate_app_description(description, input_path, model, tokens_list)\n",
    "        \n",
    "        # Step 2: Project creation\n",
    "        create_project(output_path)\n",
    "        \n",
    "        print(\"Project Created\")\n",
    "        \n",
    "    except (ValueError, KeyError) as e:\n",
    "        fg = typer.colors.RED\n",
    "        typer.secho(e, err=True, fg=fg)\n",
    "        raise typer.Exit(code=1)\n",
    "    except Exception as e:\n",
    "        fg = typer.colors.RED\n",
    "        typer.secho(f\"Unexpected internal error: {e}\", err=True, fg=fg)\n",
    "        raise typer.Exit(code=1)\n",
    "    finally:\n",
    "        total_tokens_usage = add_tokens_usage(tokens_list)\n",
    "        price = _calculate_price(total_tokens_usage, model.value)\n",
    "\n",
    "        fg = typer.colors.CYAN\n",
    "        typer.secho(f\" Tokens used: {total_tokens_usage['total_tokens']}\", fg=fg)\n",
    "        logger.info(f\"Prompt Tokens: {total_tokens_usage['prompt_tokens']}\")\n",
    "        logger.info(f\"Completion Tokens: {total_tokens_usage['completion_tokens']}\")\n",
    "        typer.secho(f\" Total Cost (USD): ${round(price, 5)}\", fg=fg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b07e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = runner.invoke(app, [\"generate\", \"--help\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112bb630",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
