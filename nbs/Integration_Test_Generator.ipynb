{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4c4cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp _components.integration_test_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89c07c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "from typing import *\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import os\n",
    "import toml\n",
    "import re\n",
    "from tempfile import TemporaryDirectory\n",
    "from contextlib import contextmanager\n",
    "from collections import defaultdict\n",
    "import subprocess  # nosec: B404: Consider possible security implications associated with the subprocess module.\n",
    "\n",
    "from yaspin import yaspin\n",
    "\n",
    "from faststream_gen._components.logger import get_logger\n",
    "\n",
    "from faststream_gen._code_generator.prompts import REQUIREMENTS_GENERATION_PROMPT\n",
    "from faststream_gen._code_generator.constants import (\n",
    "    APPLICATION_FILE_PATH,\n",
    "    TEST_FILE_PATH,\n",
    "    STEP_LOG_DIR_NAMES,\n",
    "    TOML_FILE_NAME,\n",
    "    OpenAIModel,\n",
    ")\n",
    "\n",
    "from faststream_gen._code_generator.chat import CustomAIChat, ValidateAndFixResponse\n",
    "\n",
    "from faststream_gen._code_generator.helper import (\n",
    "    write_file_contents,\n",
    "    read_file_contents,\n",
    "    set_cwd,\n",
    "    mock_openai_create,\n",
    "    retry_on_error,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0403097d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pytest\n",
    "\n",
    "from faststream_gen._components.logger import suppress_timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cc9bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "create_venv_and_run_tests_bash_script = \"\"\"\n",
    "#!/bin/bash\n",
    "\n",
    "# get the project directory from command line argument\n",
    "project_dir=$1\n",
    "\n",
    "# get the venv directory from command line argument\n",
    "venv_dir=$2\n",
    "\n",
    "# create a new venv in the specified directory\n",
    "python3 -m venv $venv_dir/my_venv > /dev/null 2>&1\n",
    "\n",
    "# activate the venv\n",
    "source $venv_dir/my_venv/bin/activate\n",
    "\n",
    "# navigate to the project directory\n",
    "cd $project_dir\n",
    "\n",
    "# install the python project inside the venv\n",
    "pip install .['dev'] > /dev/null 2>&1\n",
    "\n",
    "# run pytest and capture output\n",
    "pytest_output=$(pytest --tb=short)\n",
    "\n",
    "# capture pytest exit code\n",
    "pytest_exit_code=$?\n",
    "\n",
    "# print the pytest output\n",
    "echo \"pytest_output_start:$pytest_output:pytest_output_end\"\n",
    "\n",
    "# print the pytest exit code\n",
    "echo \"pytest_exit_code:$pytest_exit_code\"\n",
    "\n",
    "# deactivate the venv\n",
    "deactivate\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed6e010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def _setup_venv_and_run_tests(output_path: str) -> List[str]:\n",
    "    output_path_resolved = Path(output_path).resolve()\n",
    "    with TemporaryDirectory() as d:\n",
    "        bash_file = Path(d) / \"run_tests.sh\"\n",
    "        write_file_contents(str(bash_file), create_venv_and_run_tests_bash_script)\n",
    "        with set_cwd(d):\n",
    "            # nosemgrep: python.lang.security.audit.subprocess-shell-true.subprocess-shell-true\n",
    "            p = subprocess.run( # nosec: B602, B603, B607 subprocess call - check for execution of untrusted input.\n",
    "                [\"bash\", \"run_tests.sh\", output_path_resolved, Path(d).resolve()],\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "            )\n",
    "            \n",
    "        # Extract exit code\n",
    "        exit_code = int(re.search('pytest_exit_code:(\\d+)', p.stdout).group(1)) # type: ignore\n",
    "        if exit_code !=0:\n",
    "            # Extract pytest output\n",
    "            pytest_output: str = re.search('pytest_output_start:(.*):pytest_output_end', p.stdout, re.DOTALL).group(1).strip() # type: ignore\n",
    "            return [pytest_output]\n",
    "        \n",
    "        return []    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db53a8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "fixture_app_code = \"\"\"\n",
    "print(\"Hi\")\n",
    "\"\"\"\n",
    "\n",
    "fixture_test_code = \"\"\"\n",
    "def test_always_pass():\n",
    "    assert True\n",
    "\"\"\"\n",
    "\n",
    "fixture_pytoml_file = \"\"\"\n",
    "requires = [\"hatchling\"]\n",
    "build-backend = \"hatchling.build\"\n",
    "\n",
    "[project]\n",
    "name = \"app\"\n",
    "version = \"0.0.1\"\n",
    "dependencies = [ \"faststream[kafka, docs]>=0.1.5\", \"pydantic\", \"ssl\", \"requests\",]\n",
    "\n",
    "[project.optional-dependencies]\n",
    "lint = [ \"black==23.9.1\",]\n",
    "static-analysis = [ \"types-PyYAML\",]\n",
    "testing = [ \"faststream[kafka, testing]>=0.1.5\", \"pytest\",]\n",
    "dev = [ \"app[lint,static-analysis,testing]\",]\n",
    "\n",
    "[tool.pytest.ini_options]\n",
    "filterwarnings = [ \"ignore::DeprecationWarning\",]\n",
    "\"\"\"\n",
    "\n",
    "with TemporaryDirectory() as d:\n",
    "    app_file = Path(d) / APPLICATION_FILE_PATH\n",
    "    test_file = Path(d) / TEST_FILE_PATH\n",
    "    toml_file = Path(d) / TOML_FILE_NAME\n",
    "    write_file_contents(app_file, fixture_app_code)\n",
    "    write_file_contents(test_file, fixture_test_code)\n",
    "    write_file_contents(toml_file, fixture_pytoml_file)\n",
    "    \n",
    "    test_init_file_path = test_file.parent / \"__init__.py\"\n",
    "    test_init_file_path.touch()\n",
    "\n",
    "    app_init_file_path = app_file.parent / \"__init__.py\"\n",
    "    app_init_file_path.touch()\n",
    "\n",
    "    actual = _setup_venv_and_run_tests(d)\n",
    "    print(actual)\n",
    "    assert actual == []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c1c9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.11.4, pytest-7.4.2, pluggy-1.3.0\n",
      "rootdir: /tmp/tmplojftuue\n",
      "configfile: pyproject.toml\n",
      "plugins: anyio-3.7.1, asyncio-0.21.1\n",
      "asyncio: mode=Mode.STRICT\n",
      "collected 0 items / 1 error\n",
      "\n",
      "==================================== ERRORS ====================================\n",
      "\u001b[31m\u001b[1m__________________ ERROR collecting tests/test_application.py __________________\u001b[0m\n",
      "\u001b[31mImportError while importing test module '/tmp/tmplojftuue/tests/test_application.py'.\n",
      "Hint: make sure your test modules/packages have valid Python names.\n",
      "Traceback:\n",
      "\u001b[1m\u001b[31m/usr/lib/python3.11/importlib/__init__.py\u001b[0m:126: in import_module\n",
      "    \u001b[94mreturn\u001b[39;49;00m _bootstrap._gcd_import(name[level:], package, level)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mtests/test_application.py\u001b[0m:2: in <module>\n",
      "    \u001b[94mimport\u001b[39;49;00m \u001b[04m\u001b[96munknown_package\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE   ModuleNotFoundError: No module named 'unknown_package'\u001b[0m\u001b[0m\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mERROR\u001b[0m tests/test_application.py\n",
      "!!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!\n",
      "\u001b[31m=============================== \u001b[31m\u001b[1m1 error\u001b[0m\u001b[31m in 0.07s\u001b[0m\u001b[31m ===============================\u001b[0m\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "fixture_app_code = \"\"\"\n",
    "print(\"Hi\")\n",
    "\"\"\"\n",
    "\n",
    "fixture_test_code = \"\"\"\n",
    "import unknown_package\n",
    "def test_always_fails():\n",
    "    assert False\n",
    "\"\"\"\n",
    "\n",
    "fixture_pytoml_file = \"\"\"\n",
    "requires = [\"hatchling\"]\n",
    "build-backend = \"hatchling.build\"\n",
    "\n",
    "[project]\n",
    "name = \"app\"\n",
    "version = \"0.0.1\"\n",
    "dependencies = [ \"faststream[kafka, docs]>=0.1.5\", \"pydantic\", \"ssl\", \"requests\",]\n",
    "\n",
    "[project.optional-dependencies]\n",
    "lint = [ \"black==23.9.1\",]\n",
    "static-analysis = [ \"types-PyYAML\",]\n",
    "testing = [ \"faststream[kafka, testing]>=0.1.5\", \"pytest\",]\n",
    "dev = [ \"app[lint,static-analysis,testing]\",]\n",
    "\n",
    "[tool.pytest.ini_options]\n",
    "filterwarnings = [ \"ignore::DeprecationWarning\",]\n",
    "\"\"\"\n",
    "\n",
    "with TemporaryDirectory() as d:\n",
    "    app_file = Path(d) / APPLICATION_FILE_PATH\n",
    "    test_file = Path(d) / TEST_FILE_PATH\n",
    "    toml_file = Path(d) / TOML_FILE_NAME\n",
    "    write_file_contents(app_file, fixture_app_code)\n",
    "    write_file_contents(test_file, fixture_test_code)\n",
    "    write_file_contents(toml_file, fixture_pytoml_file)\n",
    "    \n",
    "    test_init_file_path = test_file.parent / \"__init__.py\"\n",
    "    test_init_file_path.touch()\n",
    "\n",
    "    app_init_file_path = app_file.parent / \"__init__.py\"\n",
    "    app_init_file_path.touch()\n",
    "\n",
    "    actual = _setup_venv_and_run_tests(d)\n",
    "    print(actual[0])\n",
    "    assert actual != []\n",
    "    print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66aefb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def _stript(s: str) -> str:\n",
    "    return s.strip().strip('\"')\n",
    "\n",
    "\n",
    "def _split_app_and_test_req(response: str) -> Tuple[str, str]:\n",
    "\n",
    "    app_req, test_req = response.split(\"==== APP REQUIREMENT ====\")[1].split(\n",
    "        \"==== TEST REQUIREMENT ====\"\n",
    "    )\n",
    "    return _stript(app_req), _stript(test_req)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd08c7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('pydantic, ssl, requests', 'pytest')\n"
     ]
    }
   ],
   "source": [
    "response = \"\"\"\n",
    "==== APP REQUIREMENT ====\n",
    "\"pydantic, ssl, requests\"\n",
    "\n",
    "==== TEST REQUIREMENT ====\n",
    "\"pytest\"\n",
    "\"\"\"\n",
    "\n",
    "actual = _split_app_and_test_req(response)\n",
    "print(actual)\n",
    "expected = ('pydantic, ssl, requests', 'pytest')\n",
    "assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b577217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('pydantic, ssl, requests', '')\n"
     ]
    }
   ],
   "source": [
    "response = \"\"\"\n",
    "==== APP REQUIREMENT ====\n",
    "\"pydantic, ssl, requests\"\n",
    "\n",
    "==== TEST REQUIREMENT ====\n",
    "\"\"\"\n",
    "\n",
    "actual = _split_app_and_test_req(response)\n",
    "print(actual)\n",
    "expected = ('pydantic, ssl, requests', '')\n",
    "assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af3c476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def _update_toml_file(output_dir: str, app_req: str, test_req: str) -> None:\n",
    "    toml_file_path = f\"{output_dir}/{TOML_FILE_NAME}\"\n",
    "    toml_contents = read_file_contents(toml_file_path)\n",
    "    data = toml.loads(toml_contents)\n",
    "    \n",
    "    app_reqs = [r.strip() for r in app_req.split(\",\")]\n",
    "    test_reqs = [r.strip() for r in test_req.split(\",\")]\n",
    "    test_reqs = [r for r in test_reqs if r != \"pytest\"]\n",
    "    \n",
    "    data[\"project\"][\"dependencies\"] = data[\"project\"][\"dependencies\"] + app_reqs\n",
    "    data[\"project\"][\"optional-dependencies\"][\"testing\"] = data[\"project\"][\"optional-dependencies\"][\"testing\"] + test_reqs\n",
    "    \n",
    "    toml_string = toml.dumps(data)\n",
    "    write_file_contents(toml_file_path, toml_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbd763b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[build-system]\n",
      "requires = [ \"hatchling\",]\n",
      "\n",
      "[project]\n",
      "name = \"app\"\n",
      "version = \"0.0.1\"\n",
      "dependencies = [ \"faststream[kafka, docs]>=0.1.5\", \"pydantic\", \"ssl\", \"requests\",]\n",
      "\n",
      "[project.optional-dependencies]\n",
      "lint = [ \"black==23.9.1\",]\n",
      "static-analysis = [ \"types-PyYAML\",]\n",
      "testing = [ \"faststream[kafka, testing]>=0.1.5\",]\n",
      "dev = [ \"app[lint,static-analysis,testing]\",]\n",
      "\n",
      "[tool.pytest.ini_options]\n",
      "filterwarnings = [ \"ignore::DeprecationWarning\",]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fixture_requirements = \"\"\"[build-system]\n",
    "requires = [\"hatchling\"]\n",
    "\n",
    "[project]\n",
    "name = \"app\"\n",
    "version = \"0.0.1\"\n",
    "\n",
    "dependencies = [\n",
    "    \"faststream[kafka, docs]>=0.1.5\",\n",
    "]\n",
    "\n",
    "[project.optional-dependencies]\n",
    "lint = [\n",
    "    \"black==23.9.1\",\n",
    "]\n",
    "\n",
    "static-analysis = [\n",
    "    \"types-PyYAML\",\n",
    "]\n",
    "\n",
    "testing = [\n",
    "    \"faststream[kafka, testing]>=0.1.5\",\n",
    "]\n",
    "\n",
    "dev = [\"app[lint,static-analysis,testing]\"]\n",
    "\n",
    "[tool.pytest.ini_options]\n",
    "filterwarnings =[\"ignore::DeprecationWarning\"]\n",
    "\"\"\"\n",
    "\n",
    "app_req = \"pydantic, ssl, requests\"\n",
    "test_req = \"pytest\"\n",
    "\n",
    "expected = \"\"\"[build-system]\n",
    "requires = [ \"hatchling\",]\n",
    "\n",
    "[project]\n",
    "name = \"app\"\n",
    "version = \"0.0.1\"\n",
    "dependencies = [ \"faststream[kafka, docs]>=0.1.5\", \"pydantic\", \"ssl\", \"requests\",]\n",
    "\n",
    "[project.optional-dependencies]\n",
    "lint = [ \"black==23.9.1\",]\n",
    "static-analysis = [ \"types-PyYAML\",]\n",
    "testing = [ \"faststream[kafka, testing]>=0.1.5\",]\n",
    "dev = [ \"app[lint,static-analysis,testing]\",]\n",
    "\n",
    "[tool.pytest.ini_options]\n",
    "filterwarnings = [ \"ignore::DeprecationWarning\",]\n",
    "\"\"\"\n",
    "\n",
    "with TemporaryDirectory() as d:\n",
    "    toml_file = Path(d) / TOML_FILE_NAME\n",
    "    write_file_contents(toml_file, fixture_requirements)\n",
    "    \n",
    "    _update_toml_file(d, app_req, test_req)\n",
    "    \n",
    "    actual = read_file_contents(toml_file)\n",
    "    print(actual)\n",
    "    assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba264f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def _validate_response(\n",
    "    response: str, output_directory: str, **kwargs: Dict[str, Any]\n",
    ") -> List[str]:\n",
    "    try:\n",
    "        app_req, test_req = _split_app_and_test_req(response)\n",
    "    except (IndexError, ValueError) as e:\n",
    "        return [\n",
    "            \"Please add ==== APP REQUIREMENT ==== and ==== TEST REQUIREMENT ==== in your response\"\n",
    "        ]\n",
    "    \n",
    "    _update_toml_file(output_directory, app_req, test_req)\n",
    "    \n",
    "    return _setup_venv_and_run_tests(output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25b3b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Please add ==== APP REQUIREMENT ==== and ==== TEST REQUIREMENT ==== in your response']\n"
     ]
    }
   ],
   "source": [
    "fixture_response = \"\"\"\n",
    "\"pydantic, ssl, requests\"\n",
    "\n",
    "==== TEST REQUIREMENT ====\n",
    "\"pytest\"\n",
    "\"\"\"\n",
    "\n",
    "with TemporaryDirectory() as d:\n",
    "    expected = ['Please add ==== APP REQUIREMENT ==== and ==== TEST REQUIREMENT ==== in your response']\n",
    "    actual = _validate_response(fixture_response, d)\n",
    "    print(actual)\n",
    "    assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f743a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "fixture_app_code = \"\"\"\n",
    "print(\"Hi\")\n",
    "\"\"\"\n",
    "\n",
    "fixture_test_code = \"\"\"\n",
    "def test_always_pass():\n",
    "    assert True\n",
    "\"\"\"\n",
    "\n",
    "fixture_response = \"\"\"\n",
    "==== APP REQUIREMENT ====\n",
    "\"pydantic\"\n",
    "\n",
    "==== TEST REQUIREMENT ====\n",
    "\"pytest\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "with TemporaryDirectory() as d:\n",
    "    app_file = Path(d) / APPLICATION_FILE_PATH\n",
    "    test_file = Path(d) / TEST_FILE_PATH\n",
    "    toml_file = Path(d) / TOML_FILE_NAME\n",
    "    write_file_contents(app_file, fixture_app_code)\n",
    "    write_file_contents(test_file, fixture_test_code)\n",
    "    write_file_contents(toml_file, fixture_requirements)\n",
    "    \n",
    "    test_init_file_path = test_file.parent / \"__init__.py\"\n",
    "    test_init_file_path.touch()\n",
    "\n",
    "    app_init_file_path = app_file.parent / \"__init__.py\"\n",
    "    app_init_file_path.touch()\n",
    "    \n",
    "    expected = []\n",
    "    actual = _validate_response(fixture_response, d)\n",
    "    print(actual)\n",
    "    assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46543c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.11.4, pytest-7.4.2, pluggy-1.3.0\n",
      "rootdir: /tmp/tmpl0aw7_87\n",
      "configfile: pyproject.toml\n",
      "plugins: anyio-3.7.1, asyncio-0.21.1\n",
      "asyncio: mode=Mode.STRICT\n",
      "collected 1 item\n",
      "\n",
      "tests/test_application.py \u001b[31mF\u001b[0m\u001b[31m                                              [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_______________________________ test_always_fail _______________________________\u001b[0m\n",
      "\u001b[1m\u001b[31mtests/test_application.py\u001b[0m:3: in test_always_fail\n",
      "    \u001b[94massert\u001b[39;49;00m \u001b[94mFalse\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE   assert False\u001b[0m\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/test_application.py::\u001b[1mtest_always_fail\u001b[0m - assert False\n",
      "\u001b[31m============================== \u001b[31m\u001b[1m1 failed\u001b[0m\u001b[31m in 0.04s\u001b[0m\u001b[31m ===============================\u001b[0m\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "fixture_app_code = \"\"\"\n",
    "print(\"Hi\")\n",
    "\"\"\"\n",
    "\n",
    "fixture_test_code = \"\"\"\n",
    "def test_always_fail():\n",
    "    assert False\n",
    "\"\"\"\n",
    "\n",
    "fixture_response = \"\"\"\n",
    "==== APP REQUIREMENT ====\n",
    "\"pydantic\"\n",
    "\n",
    "==== TEST REQUIREMENT ====\n",
    "\"pytest\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "with TemporaryDirectory() as d:\n",
    "    app_file = Path(d) / APPLICATION_FILE_PATH\n",
    "    test_file = Path(d) / TEST_FILE_PATH\n",
    "    toml_file = Path(d) / TOML_FILE_NAME\n",
    "    write_file_contents(app_file, fixture_app_code)\n",
    "    write_file_contents(test_file, fixture_test_code)\n",
    "    write_file_contents(toml_file, fixture_requirements)\n",
    "    \n",
    "    test_init_file_path = test_file.parent / \"__init__.py\"\n",
    "    test_init_file_path.touch()\n",
    "\n",
    "    app_init_file_path = app_file.parent / \"__init__.py\"\n",
    "    app_init_file_path.touch()\n",
    "    \n",
    "    actual = _validate_response(fixture_response, d)\n",
    "    print(actual[0])\n",
    "    assert \"=================================== FAILURES ===================================\" in actual[0]\n",
    "    print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca0e3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@retry_on_error()  # type: ignore\n",
    "def _generate(\n",
    "    model: str,\n",
    "    prompt: str,\n",
    "    app_and_test_code: str,\n",
    "    total_usage: List[Dict[str, int]],\n",
    "    output_directory: str,\n",
    "    **kwargs,\n",
    ") -> Tuple[str, List[Dict[str, int]], bool]:\n",
    "    requirements_generator = CustomAIChat(\n",
    "        params={\n",
    "            \"temperature\": 0.2,\n",
    "        },\n",
    "        model=model,\n",
    "        user_prompt=prompt,\n",
    "    )\n",
    "    requirements_validator = ValidateAndFixResponse(\n",
    "        requirements_generator, _validate_response\n",
    "    )\n",
    "    validator_result = requirements_validator.fix(\n",
    "        app_and_test_code,\n",
    "        total_usage,\n",
    "        STEP_LOG_DIR_NAMES[\"requirements\"],\n",
    "        str(output_directory),\n",
    "        **kwargs,\n",
    "    )\n",
    "    return (\n",
    "        (validator_result, True) # type: ignore\n",
    "        if isinstance(validator_result[-1], defaultdict)\n",
    "        else validator_result\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5fc87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "model = OpenAIModel.gpt3.value\n",
    "prompt = \"Some valid prompt\"\n",
    "app_skeleton = \"some app skeleton\"\n",
    "total_usage = []\n",
    "\n",
    "test_response = \"\"\"\n",
    "==== APP REQUIREMENT ====\n",
    "\"pydantic\"\n",
    "\n",
    "==== TEST REQUIREMENT ====\n",
    "\"pytest\"\n",
    "\"\"\"\n",
    "\n",
    "fixture_app_code = \"\"\"\n",
    "print(\"Hi\")\n",
    "\"\"\"\n",
    "\n",
    "fixture_test_code = \"\"\"\n",
    "def test_always_fail():\n",
    "    assert False\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "with TemporaryDirectory() as d:\n",
    "    app_file = Path(d) / APPLICATION_FILE_PATH\n",
    "    test_file = Path(d) / TEST_FILE_PATH\n",
    "    toml_file = Path(d) / TOML_FILE_NAME\n",
    "    write_file_contents(app_file, fixture_app_code)\n",
    "    write_file_contents(test_file, fixture_test_code)\n",
    "    write_file_contents(toml_file, fixture_requirements)\n",
    "    \n",
    "    test_init_file_path = test_file.parent / \"__init__.py\"\n",
    "    test_init_file_path.touch()\n",
    "\n",
    "    app_init_file_path = app_file.parent / \"__init__.py\"\n",
    "    app_init_file_path.touch()\n",
    "\n",
    "    with mock_openai_create(test_response):\n",
    "        total_usage, is_valid_req_code = _generate(\n",
    "            model, prompt, app_skeleton, total_usage, d\n",
    "        )\n",
    "        \n",
    "    print(is_valid_req_code)\n",
    "    \n",
    "    assert not is_valid_req_code\n",
    "    assert isinstance(is_valid_req_code, bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c00123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "model = OpenAIModel.gpt3.value\n",
    "prompt = \"Some valid prompt\"\n",
    "app_skeleton = \"some app skeleton\"\n",
    "total_usage = []\n",
    "\n",
    "test_response = \"\"\"\n",
    "==== APP REQUIREMENT ====\n",
    "\"pydantic\"\n",
    "\n",
    "==== TEST REQUIREMENT ====\n",
    "\"pytest\"\n",
    "\"\"\"\n",
    "\n",
    "fixture_app_code = \"\"\"\n",
    "print(\"Hi\")\n",
    "\"\"\"\n",
    "\n",
    "fixture_test_code = \"\"\"\n",
    "def test_always_pass():\n",
    "    assert True\n",
    "\"\"\"\n",
    "\n",
    "with TemporaryDirectory() as d:\n",
    "    app_file = Path(d) / APPLICATION_FILE_PATH\n",
    "    test_file = Path(d) / TEST_FILE_PATH\n",
    "    toml_file = Path(d) / TOML_FILE_NAME\n",
    "    write_file_contents(app_file, fixture_app_code)\n",
    "    write_file_contents(test_file, fixture_test_code)\n",
    "    write_file_contents(toml_file, fixture_requirements)\n",
    "    \n",
    "    test_init_file_path = test_file.parent / \"__init__.py\"\n",
    "    test_init_file_path.touch()\n",
    "\n",
    "    app_init_file_path = app_file.parent / \"__init__.py\"\n",
    "    app_init_file_path.touch()\n",
    "\n",
    "    with mock_openai_create(test_response):\n",
    "        total_usage, is_valid_req_code = _generate(\n",
    "            model, prompt, app_skeleton, total_usage, d\n",
    "        )\n",
    "        \n",
    "    print(is_valid_req_code)\n",
    "    \n",
    "    assert is_valid_req_code\n",
    "    assert isinstance(is_valid_req_code, bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3772213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def fix_requirements_and_run_tests(\n",
    "    output_directory: str,\n",
    "    model: str,\n",
    "    total_usage: List[Dict[str, int]],\n",
    ") -> Tuple[List[Dict[str, int]], bool]:\n",
    "    with yaspin(\n",
    "        text=\"Running integration tests...\", color=\"cyan\", spinner=\"clock\"\n",
    "    ) as sp:\n",
    "        app_file = Path(output_directory) / APPLICATION_FILE_PATH\n",
    "        app_code = read_file_contents(str(app_file))\n",
    "\n",
    "        test_file = Path(output_directory) / TEST_FILE_PATH\n",
    "        test_code = read_file_contents(str(test_file))\n",
    "\n",
    "        app_and_test_code = f\"==== APP CODE ====\\n\\n{app_code}\\n\\n==== TEST CODE ====\\n\\n{test_code}\\n\\n\"\n",
    "\n",
    "        total_usage, is_requirements_file_valid = _generate(\n",
    "            model,\n",
    "            REQUIREMENTS_GENERATION_PROMPT,\n",
    "            app_and_test_code,\n",
    "            total_usage,\n",
    "            output_directory,\n",
    "        )\n",
    "\n",
    "        sp.text = \"\"\n",
    "        if is_requirements_file_valid:\n",
    "            message = \" ✔ Integration tests were successfully completed.\"\n",
    "        else:\n",
    "            message = \" ✘ Error: Integration tests failed.\"\n",
    "            sp.color = \"red\"\n",
    "\n",
    "        sp.ok(message)\n",
    "\n",
    "        return total_usage, is_requirements_file_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e48d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠹ Running integration tests... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harish/.local/lib/python3.11/site-packages/yaspin/core.py:119: UserWarning: color, on_color and attrs are not supported when running in jupyter\n",
      "  self._color = self._set_color(color) if color else color\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✔ Integration tests were successfully completed. \n",
      "True\n"
     ]
    }
   ],
   "source": [
    "model = OpenAIModel.gpt3.value\n",
    "total_usage = []\n",
    "\n",
    "test_response = \"\"\"\n",
    "==== APP REQUIREMENT ====\n",
    "\"pydantic\"\n",
    "\n",
    "==== TEST REQUIREMENT ====\n",
    "\"pytest\"\n",
    "\"\"\"\n",
    "\n",
    "fixture_app_code = \"\"\"\n",
    "print(\"Hi\")\n",
    "\"\"\"\n",
    "\n",
    "fixture_test_code = \"\"\"\n",
    "def test_always_pass():\n",
    "    assert True\n",
    "\"\"\"\n",
    "\n",
    "with TemporaryDirectory() as d:\n",
    "    app_file = Path(d) / APPLICATION_FILE_PATH\n",
    "    test_file = Path(d) / TEST_FILE_PATH\n",
    "    toml_file = Path(d) / TOML_FILE_NAME\n",
    "    write_file_contents(app_file, fixture_app_code)\n",
    "    write_file_contents(test_file, fixture_test_code)\n",
    "    write_file_contents(toml_file, fixture_requirements)\n",
    "\n",
    "    test_init_file_path = test_file.parent / \"__init__.py\"\n",
    "    test_init_file_path.touch()\n",
    "\n",
    "    app_init_file_path = app_file.parent / \"__init__.py\"\n",
    "    app_init_file_path.touch()\n",
    "\n",
    "    with mock_openai_create(test_response):\n",
    "        total_usage, is_requirements_file_valid = fix_requirements_and_run_tests(d, model, [])\n",
    "\n",
    "    print(is_requirements_file_valid)\n",
    "\n",
    "    assert is_requirements_file_valid\n",
    "    assert isinstance(is_requirements_file_valid, bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5675f956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠹ Running integration tests... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harish/.local/lib/python3.11/site-packages/yaspin/core.py:119: UserWarning: color, on_color and attrs are not supported when running in jupyter\n",
      "  self._color = self._set_color(color) if color else color\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✘ Error: Integration tests failed. \n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harish/.local/lib/python3.11/site-packages/yaspin/core.py:228: UserWarning: color, on_color and attrs are not supported when running in jupyter\n",
      "  self._color = self._set_color(value) if value else value\n"
     ]
    }
   ],
   "source": [
    "model = OpenAIModel.gpt3.value\n",
    "total_usage = []\n",
    "\n",
    "test_response = \"\"\"\n",
    "==== APP REQUIREMENT ====\n",
    "\"pydantic\"\n",
    "\n",
    "==== TEST REQUIREMENT ====\n",
    "\"pytest\"\n",
    "\"\"\"\n",
    "\n",
    "fixture_app_code = \"\"\"\n",
    "print(\"Hi\")\n",
    "\"\"\"\n",
    "\n",
    "fixture_test_code = \"\"\"\n",
    "def test_always_pass():\n",
    "    assert False\n",
    "\"\"\"\n",
    "\n",
    "with TemporaryDirectory() as d:\n",
    "    app_file = Path(d) / APPLICATION_FILE_PATH\n",
    "    test_file = Path(d) / TEST_FILE_PATH\n",
    "    toml_file = Path(d) / TOML_FILE_NAME\n",
    "    write_file_contents(app_file, fixture_app_code)\n",
    "    write_file_contents(test_file, fixture_test_code)\n",
    "    write_file_contents(toml_file, fixture_requirements)\n",
    "\n",
    "    test_init_file_path = test_file.parent / \"__init__.py\"\n",
    "    test_init_file_path.touch()\n",
    "\n",
    "    app_init_file_path = app_file.parent / \"__init__.py\"\n",
    "    app_init_file_path.touch()\n",
    "\n",
    "    with mock_openai_create(test_response):\n",
    "        total_usage, is_requirements_file_valid = fix_requirements_and_run_tests(d, model, [])\n",
    "\n",
    "    print(is_requirements_file_valid)\n",
    "\n",
    "    assert not is_requirements_file_valid\n",
    "    assert isinstance(is_requirements_file_valid, bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3718803f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
