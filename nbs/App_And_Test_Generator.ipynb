{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21705b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp _code_generator.app_and_test_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792d665d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from typing import *\n",
    "import time\n",
    "import importlib.util\n",
    "from tempfile import TemporaryDirectory\n",
    "from pathlib import Path\n",
    "import platform\n",
    "from collections import defaultdict\n",
    "import subprocess  # nosec: B404: Consider possible security implications associated with the subprocess module.\n",
    "\n",
    "from yaspin import yaspin\n",
    "\n",
    "from faststream_gen._components.logger import get_logger\n",
    "from faststream_gen._code_generator.chat import CustomAIChat, ValidateAndFixResponse\n",
    "from faststream_gen._code_generator.helper import (\n",
    "    write_file_contents,\n",
    "    read_file_contents,\n",
    "    validate_python_code,\n",
    "    retry_on_error,\n",
    "    set_cwd\n",
    ")\n",
    "from faststream_gen._code_generator.prompts import APP_AND_TEST_GENERATION_PROMPT\n",
    "from faststream_gen._code_generator.constants import (\n",
    "    APPLICATION_FILE_PATH,\n",
    "    TEST_FILE_PATH,\n",
    "    STEP_LOG_DIR_NAMES,\n",
    "    LOGS_DIR_NAME,\n",
    ")\n",
    "\n",
    "from faststream_gen._code_generator.constants import OpenAIModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381bd805",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "import unittest.mock\n",
    "\n",
    "import pytest\n",
    "import openai\n",
    "\n",
    "from faststream_gen._components.logger import suppress_timestamps\n",
    "from faststream_gen._code_generator.helper import mock_openai_create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19863bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0b5c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "suppress_timestamps()\n",
    "logger = get_logger(__name__, level=30)\n",
    "logger.info(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901e1320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "_code_fix_prompt = \"\"\"\n",
    "Your task is to correct the provided code. Your response should consist solely of valid Python code. You must follow the below rules while responding:\n",
    "\n",
    "- Do not include explanations or wrap your response in ```python tags.\n",
    "\"\"\"\n",
    "\n",
    "def _fix_generated_code(s: str) -> str:\n",
    "    ai = CustomAIChat(\n",
    "        params={\n",
    "            \"temperature\": 0.2,\n",
    "        },\n",
    "        model=OpenAIModel.gpt3.value,\n",
    "        user_prompt=_code_fix_prompt,\n",
    "    )\n",
    "    response, usage = ai(s) # todo: add this usage to total usage\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a84b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print(\"hi\")\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "fixture = \"\"\"\n",
    "print(\"hi)\n",
    "\"\"\"\n",
    "\n",
    "expected = \"\"\"print(\"hi\")\"\"\"\n",
    "\n",
    "actual = _fix_generated_code(fixture)\n",
    "print(actual)\n",
    "assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4874bc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def _split_app_and_test_code(response: str) -> Tuple[str, str]:\n",
    "    app_code, test_code = response.split(\"### application.py ###\")[1].split(\n",
    "        \"### test.py ###\"\n",
    "    )\n",
    "    return app_code, test_code\n",
    "\n",
    "\n",
    "def _validate_response(\n",
    "    response: str, output_directory: str, **kwargs: Dict[str, Any]\n",
    ") -> Tuple[List[str], str]:\n",
    "    try:\n",
    "        app_code, test_code = _split_app_and_test_code(response)\n",
    "    except (IndexError, ValueError) as e:\n",
    "        return (\n",
    "            [\"Please add ### application.py ### and ### test.py ### in your response\"],\n",
    "            response,\n",
    "        )\n",
    "\n",
    "    app_code = app_code.replace(\"### application.py ###\", \"\").strip()\n",
    "    test_code = test_code.strip()\n",
    "\n",
    "    fixed_app_code = _fix_generated_code(app_code)\n",
    "    fixed_test_code = test_code.replace(\"from application import \", \"from app.application import \") #_fix_generated_code(test_code)\n",
    "\n",
    "    app_file_name = Path(output_directory) / APPLICATION_FILE_PATH\n",
    "    test_file_name = Path(output_directory) / TEST_FILE_PATH\n",
    "\n",
    "    write_file_contents(str(app_file_name), fixed_app_code)\n",
    "    write_file_contents(str(test_file_name), fixed_test_code)\n",
    "\n",
    "    with set_cwd(output_directory):\n",
    "        cmd = [\"pytest\", \"--tb=short\"]\n",
    "        # nosemgrep: python.lang.security.audit.subprocess-shell-true.subprocess-shell-true\n",
    "        p = subprocess.run(  # nosec: B602, B603 subprocess call - check for execution of untrusted input.\n",
    "            cmd,\n",
    "            stderr=subprocess.PIPE,\n",
    "            stdout=subprocess.PIPE,\n",
    "            shell=True if platform.system() == \"Windows\" else False,\n",
    "        )\n",
    "    if p.returncode != 0:\n",
    "        response = f\"### application.py ###\\n{fixed_app_code}\\n\\n### test.py ###\\n{fixed_test_code}\\n\"\n",
    "        return ([str(p.stdout.decode(\"utf-8\"))], response)\n",
    "\n",
    "    return ([], \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b8a675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([], '')\n"
     ]
    }
   ],
   "source": [
    "fixture_response = \"\"\"\n",
    "### application.py ###\n",
    "\n",
    "print('hi')\n",
    "\n",
    "### test.py ###\n",
    "def test_always_passes():\n",
    "    assert True\n",
    "\"\"\"\n",
    "test_response = \"\"\"\n",
    "def test_always_passes():\n",
    "    assert True\n",
    "\"\"\"\n",
    "with TemporaryDirectory() as d:\n",
    "    expected = ([], \"\")\n",
    "    with mock_openai_create(test_response):\n",
    "        actual = _validate_response(fixture_response, d)\n",
    "        print(actual)\n",
    "        assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ffbe47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['Please add ### application.py ### and ### test.py ### in your response'], \"\\n### application.py ##\\n\\nprint('hi')\\n\\n### test.py ###\\ndef test_always_passes():\\n    assert True\\n\")\n"
     ]
    }
   ],
   "source": [
    "fixture_response = \"\"\"\n",
    "### application.py ##\n",
    "\n",
    "print('hi')\n",
    "\n",
    "### test.py ###\n",
    "def test_always_passes():\n",
    "    assert True\n",
    "\"\"\"\n",
    "test_response = \"\"\"\n",
    "def test_always_passes():\n",
    "    assert True\n",
    "\"\"\"\n",
    "with TemporaryDirectory() as d:\n",
    "    expected = (['Please add ### application.py ### and ### test.py ### in your response'], \"\\n### application.py ##\\n\\nprint('hi')\\n\\n### test.py ###\\ndef test_always_passes():\\n    assert True\\n\")\n",
    "    with mock_openai_create(test_response):\n",
    "        actual = _validate_response(fixture_response, d)\n",
    "        print(actual)\n",
    "        assert actual == expected, actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dff44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\x1b[1m============================= test session starts ==============================\\x1b[0m\\nplatform linux -- Python 3.11.5, pytest-7.4.2, pluggy-1.3.0\\nrootdir: /tmp/tmpjb8y5nsv\\nplugins: anyio-3.7.1, asyncio-0.21.1\\nasyncio: mode=Mode.STRICT\\ncollected 1 item\\n\\ntests/test_application.py \\x1b[31mF\\x1b[0m\\x1b[31m                                              [100%]\\x1b[0m\\n\\n=================================== FAILURES ===================================\\n\\x1b[31m\\x1b[1m______________________________ test_always_fails _______________________________\\x1b[0m\\n\\x1b[1m\\x1b[31mtests/test_application.py\\x1b[0m:2: in test_always_fails\\n    \\x1b[94massert\\x1b[39;49;00m \\x1b[94mFalse\\x1b[39;49;00m\\x1b[90m\\x1b[39;49;00m\\n\\x1b[1m\\x1b[31mE   assert False\\x1b[0m\\n\\x1b[36m\\x1b[1m=========================== short test summary info ============================\\x1b[0m\\n\\x1b[31mFAILED\\x1b[0m tests/test_application.py::\\x1b[1mtest_always_fails\\x1b[0m - assert False\\n\\x1b[31m============================== \\x1b[31m\\x1b[1m1 failed\\x1b[0m\\x1b[31m in 0.04s\\x1b[0m\\x1b[31m ===============================\\x1b[0m\\n']\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "fixture_response = \"\"\"\n",
    "### application.py ###\n",
    "\n",
    "print('hi')\n",
    "\n",
    "### test.py ###\n",
    "\n",
    "def test_always_fails():\n",
    "    assert False\n",
    "\"\"\"\n",
    "fixture_app_code = \"print('hi')\"\n",
    "\n",
    "test_response = \"\"\"\n",
    "def test_always_fails():\n",
    "    assert False\n",
    "\"\"\"\n",
    "\n",
    "with TemporaryDirectory() as d:\n",
    "    with mock_openai_create(test_response):\n",
    "        actual = _validate_response(fixture_response, d)\n",
    "        print(actual[0])\n",
    "        assert actual != [], actual\n",
    "        print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43acc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@retry_on_error()  # type: ignore\n",
    "def _generate(\n",
    "    model: str,\n",
    "    prompt: str,\n",
    "    app_skeleton: str,\n",
    "    total_usage: List[Dict[str, int]],\n",
    "    output_directory: str,\n",
    "    **kwargs,\n",
    ") -> Tuple[str, List[Dict[str, int]], bool]:\n",
    "    test_generator = CustomAIChat(\n",
    "        params={\n",
    "            \"temperature\": 0.2,\n",
    "        },\n",
    "        model=model,\n",
    "        user_prompt=prompt,\n",
    "    )\n",
    "    test_validator = ValidateAndFixResponse(test_generator, _validate_response)\n",
    "    validator_result = test_validator.fix(\n",
    "        app_skeleton,\n",
    "        total_usage,\n",
    "        STEP_LOG_DIR_NAMES[\"app\"],\n",
    "        str(output_directory),\n",
    "        **kwargs,\n",
    "    )\n",
    "    \n",
    "    return (\n",
    "        (validator_result, True) # type: ignore\n",
    "        if isinstance(validator_result[-1], defaultdict)\n",
    "        else validator_result\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51f42ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "model = OpenAIModel.gpt3.value\n",
    "prompt = \"Some valid prompt\"\n",
    "app_skeleton = \"some app skeleton\"\n",
    "total_usage = []\n",
    "\n",
    "test_response = \"\"\"\n",
    "### application.py ###\n",
    "\n",
    "print('some valid python code')\n",
    "    \n",
    "### test.py ###\n",
    "\n",
    "from app import application\n",
    "\n",
    "def test_always_passes():\n",
    "    assert True\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "with TemporaryDirectory() as d:\n",
    "    test_file_path = Path(d) / TEST_FILE_PATH\n",
    "    test_file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    init_file_path = test_file_path.parent / \"__init__.py\"\n",
    "    init_file_path.touch()\n",
    "    with mock_openai_create(test_response):\n",
    "        total_usage, is_valid_app_code = _generate(\n",
    "            model, prompt, app_skeleton, total_usage, d\n",
    "        )\n",
    "        \n",
    "    print(is_valid_app_code)\n",
    "    \n",
    "    assert is_valid_app_code\n",
    "    assert isinstance(is_valid_app_code, bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c11849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "model = OpenAIModel.gpt3.value\n",
    "prompt = \"Some invalid prompt\"\n",
    "app_skeleton = \"some invalid app skeleton\"\n",
    "total_usage = []\n",
    "\n",
    "test_response = \"\"\"\n",
    "### application.py ###\n",
    "\n",
    "print(\"invalid app code\")\n",
    "    \n",
    "### test.py ###\n",
    "\n",
    "import pytest\n",
    "\n",
    "from faststream.kafka import TestKafkaBroker\n",
    "\n",
    "from .app import CourseUpdates, broker, on_course_update\n",
    "\n",
    "\n",
    "print(\"invalid test code\")\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "with TemporaryDirectory() as d:\n",
    "    test_file_path = Path(d) / TEST_FILE_PATH\n",
    "    test_file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    init_file_path = test_file_path.parent / \"__init__.py\"\n",
    "    init_file_path.touch()\n",
    "    with mock_openai_create(test_response):\n",
    "        total_usage, is_valid_app_code = _generate(\n",
    "            model, prompt, app_skeleton, total_usage, d\n",
    "        )\n",
    "        print(is_valid_app_code)\n",
    "        assert not is_valid_app_code\n",
    "        assert isinstance(is_valid_app_code, bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d6fb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def generate_app_and_test(\n",
    "    description: str,\n",
    "    model: str,\n",
    "    output_directory: str,\n",
    "    total_usage: List[Dict[str, int]],\n",
    "    relevant_prompt_examples: str,\n",
    ") -> Tuple[List[Dict[str, int]], bool]:\n",
    "    \"\"\"Generate integration test for the FastStream app\n",
    "\n",
    "    Args:\n",
    "        description: Validated User application description\n",
    "        code_gen_directory: The directory containing the generated files.\n",
    "        relevant_prompt_examples: Relevant examples to add in the prompts.\n",
    "\n",
    "    Returns:\n",
    "        The generated integration test code for the application\n",
    "    \"\"\"\n",
    "    logger.info(\"==== Skeleton to App and Test Generation ====\")\n",
    "    with yaspin(\n",
    "        text=\"Generating application and tests (usually takes around 30 to 90 seconds)...\",\n",
    "        color=\"cyan\",\n",
    "        spinner=\"clock\",\n",
    "    ) as sp:\n",
    "        app_skeleton_file_name = Path(output_directory) / APPLICATION_FILE_PATH\n",
    "        app_skeleton = read_file_contents(str(app_skeleton_file_name))\n",
    "\n",
    "        prompt = (\n",
    "            APP_AND_TEST_GENERATION_PROMPT.replace(\n",
    "                \"==== REPLACE WITH APP DESCRIPTION ====\", description\n",
    "            )\n",
    "            .replace(\"==== RELEVANT EXAMPLES GOES HERE ====\", relevant_prompt_examples)\n",
    "            .replace(\"from .app import\", \"from app.application import\")\n",
    "        )\n",
    "\n",
    "        total_usage, is_valid_app_code = _generate(\n",
    "            model, prompt, app_skeleton, total_usage, output_directory\n",
    "        )\n",
    "        \n",
    "        sp.text = \"\"\n",
    "        if is_valid_app_code:\n",
    "            message = \" ✔ The application and the test files are generated.\"\n",
    "        else:\n",
    "            message = \" ✘ Error: Failed to generate a valid application and test code.\"\n",
    "            sp.color = \"red\"\n",
    "\n",
    "        sp.ok(message)\n",
    "\n",
    "        return total_usage, is_valid_app_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d00fc7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠹ Generating application and tests (usually takes around 30 to 90 seconds)... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harish/.local/lib/python3.11/site-packages/yaspin/core.py:119: UserWarning: color, on_color and attrs are not supported when running in jupyter\n",
      "  self._color = self._set_color(color) if color else color\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✘ Error: Failed to generate a valid application and test code.               \n",
      "[defaultdict(<class 'int'>, {'prompt_tokens': 387, 'completion_tokens': 3, 'total_tokens': 390}), defaultdict(<class 'int'>, {'prompt_tokens': 387, 'completion_tokens': 3, 'total_tokens': 390}), defaultdict(<class 'int'>, {'prompt_tokens': 387, 'completion_tokens': 3, 'total_tokens': 390})]\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harish/.local/lib/python3.11/site-packages/yaspin/core.py:228: UserWarning: color, on_color and attrs are not supported when running in jupyter\n",
      "  self._color = self._set_color(value) if value else value\n"
     ]
    }
   ],
   "source": [
    "fixture_skeleton_code = \"\"\"\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from faststream import FastStream, Logger\n",
    "from faststream.kafka import KafkaBroker\n",
    "\n",
    "\n",
    "class Product(BaseModel):\n",
    "    product_name: str = Field(..., description=\"Name of the product\")\n",
    "    currency: str = Field(..., description=\"Currency of the price\")\n",
    "    price: float = Field(..., description=\"Price of the product\")\n",
    "\n",
    "\n",
    "broker = KafkaBroker(\"localhost:9092\")\n",
    "app = FastStream(broker)\n",
    "\n",
    "\n",
    "@broker.publisher(\"change_currency\")\n",
    "@broker.subscriber(\"store_product\")\n",
    "async def on_store_product(product: Product, logger: Logger) -> Product:\n",
    "    '''Processes a message from 'store_product' topic, changes currency to 'EUR' and divides price by 7.5 if currency is 'HRK'.\n",
    "\n",
    "    Instructions:\n",
    "    1. Consume a message from 'store_product' topic.\n",
    "    2. Log the consumed message using logger.info.\n",
    "    3. Check if the currency attribute is set to 'HRK'.\n",
    "    4. If the currency is 'HRK', change the currency to 'EUR' and divide the price by 7.5.\n",
    "    5. If the currency is not 'HRK', do not modify the original message.\n",
    "    6. Publish the consumed message to 'change_currency' topic.\n",
    "\n",
    "\n",
    "    '''\n",
    "    raise NotImplementedError()\n",
    "\"\"\"\n",
    "\n",
    "fixture_description = \"\"\"\n",
    "Invalid description\n",
    "\"\"\"\n",
    "\n",
    "relevant_examples = '''no examples passed'''\n",
    "\n",
    "test_response = \"\"\"\n",
    "### application.py ###\n",
    "\n",
    "print(\"invalid app code\")\n",
    "    \n",
    "### test.py ###\n",
    "\n",
    "import pytest\n",
    "\n",
    "from faststream.kafka import TestKafkaBroker\n",
    "\n",
    "from .app import CourseUpdates, broker, on_course_update\n",
    "\n",
    "\n",
    "print(\"invalid test code\")\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "with TemporaryDirectory() as d:\n",
    "    app_skeleton_file_name = Path(d) / APPLICATION_FILE_PATH\n",
    "    write_file_contents(app_skeleton_file_name, fixture_skeleton_code)\n",
    "    \n",
    "    with mock_openai_create(test_response):    \n",
    "        usage, is_valid_app_code = generate_app_and_test(fixture_description, OpenAIModel.gpt3.value, d, [], relevant_examples)\n",
    "    \n",
    "    logs_dir = Path(d) / LOGS_DIR_NAME\n",
    "    assert logs_dir.exists()\n",
    "    \n",
    "\n",
    "assert int(usage[0][\"total_tokens\"]) > 0\n",
    "print(usage)\n",
    "\n",
    "print(is_valid_app_code)\n",
    "assert not is_valid_app_code\n",
    "assert isinstance(is_valid_app_code, bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad577186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠹ Generating application and tests (usually takes around 30 to 90 seconds)... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harish/.local/lib/python3.11/site-packages/yaspin/core.py:119: UserWarning: color, on_color and attrs are not supported when running in jupyter\n",
      "  self._color = self._set_color(color) if color else color\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✘ Error: Failed to generate a valid application and test code.               \n",
      "[defaultdict(<class 'int'>, {'prompt_tokens': 387, 'completion_tokens': 3, 'total_tokens': 390}), defaultdict(<class 'int'>, {'prompt_tokens': 387, 'completion_tokens': 3, 'total_tokens': 390}), defaultdict(<class 'int'>, {'prompt_tokens': 387, 'completion_tokens': 3, 'total_tokens': 390})]\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harish/.local/lib/python3.11/site-packages/yaspin/core.py:228: UserWarning: color, on_color and attrs are not supported when running in jupyter\n",
      "  self._color = self._set_color(value) if value else value\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 152\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28mprint\u001b[39m(usage)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28mprint\u001b[39m(is_valid_app_code)\n\u001b[0;32m--> 152\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m is_valid_app_code\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(is_valid_app_code, \u001b[38;5;28mbool\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "# todo: fix the below test\n",
    "\n",
    "fixture_skeleton_code = \"\"\"\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from faststream import FastStream, Logger\n",
    "from faststream.kafka import KafkaBroker\n",
    "\n",
    "\n",
    "class Product(BaseModel):\n",
    "    product_name: str = Field(..., description=\"Name of the product\")\n",
    "    currency: str = Field(..., description=\"Currency of the price\")\n",
    "    price: float = Field(..., description=\"Price of the product\")\n",
    "\n",
    "\n",
    "broker = KafkaBroker(\"localhost:9092\")\n",
    "app = FastStream(broker)\n",
    "\n",
    "\n",
    "@broker.publisher(\"change_currency\")\n",
    "@broker.subscriber(\"store_product\")\n",
    "async def on_store_product(product: Product, logger: Logger) -> Product:\n",
    "    '''Processes a message from 'store_product' topic, changes currency to 'EUR' and divides price by 7.5 if currency is 'HRK'.\n",
    "\n",
    "    Instructions:\n",
    "    1. Consume a message from 'store_product' topic.\n",
    "    2. Log the consumed message using logger.info.\n",
    "    3. Check if the currency attribute is set to 'HRK'.\n",
    "    4. If the currency is 'HRK', change the currency to 'EUR' and divide the price by 7.5.\n",
    "    5. If the currency is not 'HRK', do not modify the original message.\n",
    "    6. Publish the consumed message to 'change_currency' topic.\n",
    "\n",
    "\n",
    "    '''\n",
    "    raise NotImplementedError()\n",
    "\"\"\"\n",
    "\n",
    "fixture_description = \"\"\"\n",
    "Create a FastStream application using localhost broker for testing and use default port number. It should consume from 'store_product' topic an JSON encoded object with the following three attributes: product_name, currency and price. The format of the currency will be three letter string, e.g. 'EUR'. For each consumed message, check if the currency attribute is set to 'HRK'. If it is then change the currency to 'EUR' and divide the price by 7.5, if the currency is not set to 'HRK' don't change the original message. Finally, publish the consumed message to 'change_currency' topic.\n",
    "\"\"\"\n",
    "\n",
    "test_response = '''\n",
    "\n",
    "### application.py ###\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from faststream import FastStream, Logger\n",
    "from faststream.kafka import KafkaBroker\n",
    "\n",
    "\n",
    "class CourseUpdates(BaseModel):\n",
    "    course_name: str = Field(..., examples=[\"Biology\"], description=\"Course example\")\n",
    "    new_content: Optional[str] = Field(\n",
    "        default=None, examples=[\"New content\"], description=\"Content example\"\n",
    "    )\n",
    "\n",
    "\n",
    "broker = KafkaBroker(\"localhost:9092\")\n",
    "app = FastStream(broker)\n",
    "\n",
    "\n",
    "@broker.publisher(\"notify_updates\")\n",
    "@broker.subscriber(\"course_updates\")\n",
    "async def on_course_update(msg: CourseUpdates, logger: Logger) -> CourseUpdates:\n",
    "    logger.info(msg)\n",
    "\n",
    "    if msg.new_content:\n",
    "        logger.info(f\"Course has new content {msg.new_content=}\")\n",
    "        msg = CourseUpdates(\n",
    "            course_name=(\"Updated: \" + msg.course_name), new_content=msg.new_content\n",
    "        )\n",
    "    return msg\n",
    "    \n",
    "### test.py ###\n",
    "\n",
    "import pytest\n",
    "\n",
    "from faststream.kafka import TestKafkaBroker\n",
    "\n",
    "from app.application import CourseUpdates, broker, on_course_update\n",
    "\n",
    "\n",
    "@broker.subscriber(\"notify_updates\")\n",
    "async def on_notify_update(msg: CourseUpdates):\n",
    "    pass\n",
    "\n",
    "\n",
    "@pytest.mark.asyncio\n",
    "async def test_app_without_new_content():\n",
    "    async with TestKafkaBroker(broker):\n",
    "        await broker.publish(CourseUpdates(course_name=\"Biology\"), \"course_updates\")\n",
    "        on_course_update.mock.assert_called_with(\n",
    "            dict(CourseUpdates(course_name=\"Biology\"))\n",
    "        )\n",
    "        on_notify_update.mock.assert_called_with(\n",
    "            dict(CourseUpdates(course_name=\"Biology\"))\n",
    "        )\n",
    "\n",
    "\n",
    "@pytest.mark.asyncio\n",
    "async def test_app_with_new_content():\n",
    "    async with TestKafkaBroker(broker):\n",
    "        await broker.publish(\n",
    "            CourseUpdates(\n",
    "                course_name=\"Biology\", new_content=\"We have additional classes...\"\n",
    "            ),\n",
    "            \"course_updates\",\n",
    "        )\n",
    "        on_course_update.mock.assert_called_with(\n",
    "            dict(\n",
    "                CourseUpdates(\n",
    "                    course_name=\"Biology\", new_content=\"We have additional classes...\"\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        on_notify_update.mock.assert_called_with(\n",
    "            dict(\n",
    "                CourseUpdates(\n",
    "                    course_name=\"Updated: Biology\",\n",
    "                    new_content=\"We have additional classes...\",\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "'''\n",
    "\n",
    "with TemporaryDirectory() as d:\n",
    "    app_skeleton_file_name = Path(d) / APPLICATION_FILE_PATH\n",
    "    write_file_contents(app_skeleton_file_name, fixture_skeleton_code)\n",
    "    \n",
    "    test_file_path = Path(d) / TEST_FILE_PATH\n",
    "    test_file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    init_file_path = test_file_path.parent / \"__init__.py\"\n",
    "    init_file_path.touch()\n",
    "    \n",
    "    with mock_openai_create(test_response):    \n",
    "        usage, is_valid_app_code = generate_app_and_test(fixture_description, OpenAIModel.gpt3.value, d, [], relevant_examples)\n",
    "    \n",
    "    logs_dir = Path(d) / LOGS_DIR_NAME\n",
    "    assert logs_dir.exists()\n",
    "    \n",
    "\n",
    "assert int(usage[0][\"total_tokens\"]) > 0\n",
    "print(usage)\n",
    "\n",
    "print(is_valid_app_code)\n",
    "assert is_valid_app_code\n",
    "assert isinstance(is_valid_app_code, bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74d5a3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
