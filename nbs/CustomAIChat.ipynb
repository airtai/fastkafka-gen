{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87d24ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp _code_generator.chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa65c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] numexpr.utils: Note: NumExpr detected 64 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "[INFO] numexpr.utils: NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "# | export\n",
    "\n",
    "from typing import *\n",
    "import random\n",
    "import logging\n",
    "import time\n",
    "\n",
    "import openai\n",
    "from langchain.schema.document import Document\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from faststream_gen._code_generator.constants import DEFAULT_PARAMS\n",
    "from faststream_gen._components.logger import get_logger, set_level\n",
    "from faststream_gen._code_generator.prompts import SYSTEM_PROMPT\n",
    "\n",
    "from faststream_gen._components.package_data import get_root_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85048c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "from faststream_gen._components.logger import suppress_timestamps\n",
    "from faststream_gen._code_generator.constants import OpenAIModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4add3503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "logger = get_logger(__name__, level=logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a18be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: ok\n"
     ]
    }
   ],
   "source": [
    "suppress_timestamps()\n",
    "logger = get_logger(__name__, level=20)\n",
    "logger.info(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd26ff13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "# Reference: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_handle_rate_limits.ipynb\n",
    "\n",
    "\n",
    "def _retry_with_exponential_backoff(\n",
    "    initial_delay: float = 1,\n",
    "    exponential_base: float = 2,\n",
    "    jitter: bool = True,\n",
    "    max_retries: int = 10,\n",
    "    max_wait: float = 60,\n",
    "    errors: tuple = (\n",
    "        openai.error.RateLimitError,\n",
    "        openai.error.ServiceUnavailableError,\n",
    "        openai.error.APIError,\n",
    "    ),\n",
    ") -> Callable:\n",
    "    \"\"\"Retry a function with exponential backoff.\"\"\"\n",
    "\n",
    "    def decorator(\n",
    "        func: Callable[[str], Tuple[str, str]]\n",
    "    ) -> Callable[[str], Tuple[str, str]]:\n",
    "        def wrapper(*args, **kwargs):  # type: ignore\n",
    "            num_retries = 0\n",
    "            delay = initial_delay\n",
    "\n",
    "            while True:\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "\n",
    "                except errors as e:\n",
    "                    num_retries += 1\n",
    "                    if num_retries > max_retries:\n",
    "                        raise Exception(\n",
    "                            f\"Maximum number of retries ({max_retries}) exceeded.\"\n",
    "                        )\n",
    "                    delay = min(\n",
    "                        delay\n",
    "                        * exponential_base\n",
    "                        * (1 + jitter * random.random()),  # nosec\n",
    "                        max_wait,\n",
    "                    )\n",
    "                    logger.info(\n",
    "                        f\"Note: OpenAI's API rate limit reached. Command will automatically retry in {int(delay)} seconds. For more information visit: https://help.openai.com/en/articles/5955598-is-api-usage-subject-to-any-rate-limits\",\n",
    "                    )\n",
    "                    time.sleep(delay)\n",
    "\n",
    "                except Exception as e:\n",
    "                    raise e\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3912d382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "@_retry_with_exponential_backoff()\n",
    "def mock_func():\n",
    "    return \"Success\"\n",
    "\n",
    "actual = mock_func()\n",
    "expected = \"Success\"\n",
    "\n",
    "print(actual)\n",
    "assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e0da95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: Note: OpenAI's API rate limit reached. Command will automatically retry in 2 seconds. For more information visit: https://help.openai.com/en/articles/5955598-is-api-usage-subject-to-any-rate-limits\n",
      "Maximum number of retries (1) exceeded.\n"
     ]
    }
   ],
   "source": [
    "# Test max retries exceeded\n",
    "@_retry_with_exponential_backoff(max_retries=1)\n",
    "def mock_func_error():\n",
    "    raise openai.error.RateLimitError\n",
    "\n",
    "\n",
    "with pytest.raises(Exception) as e:\n",
    "    mock_func_error()\n",
    "\n",
    "print(e.value)\n",
    "assert str(e.value) == \"Maximum number of retries (1) exceeded.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f834ebb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def _get_relevant_document(query: str) -> str:\n",
    "    \"\"\"Load the vector database and retrieve the most relevant document based on the given query.\n",
    "\n",
    "    Args:\n",
    "        query: The query for relevance-based document retrieval.\n",
    "\n",
    "    Returns:\n",
    "        The content of the most relevant document as a string.\n",
    "    \"\"\"\n",
    "    db_path = get_root_data_path() / \"docs\"\n",
    "    db = FAISS.load_local(db_path, OpenAIEmbeddings()) # type: ignore\n",
    "    results = db.max_marginal_relevance_search(query, k=1, fetch_k=3)\n",
    "    results_str = \"\\n\".join([result.page_content for result in results])\n",
    "    return results_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509e0e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] faiss.loader: Loading faiss with AVX2 support.\n",
      "[INFO] faiss.loader: Successfully loaded faiss with AVX2 support.\n",
      "hide:\n",
      "  - navigation\n",
      "  - footer\n",
      "\n",
      "Release Notes\n",
      "\n",
      "FastStream is a new package based on the ideas and experiences gained from FastKafka and Propan. By joining our forces, we picked up the best from both \n"
     ]
    }
   ],
   "source": [
    "query = \"What is FastStream?\"\n",
    "actual = _get_relevant_document(query)\n",
    "print(actual[:200])\n",
    "assert len(actual) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9dc8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class CustomAIChat:\n",
    "    \"\"\"Custom class for interacting with OpenAI\n",
    "\n",
    "    Attributes:\n",
    "        model: The OpenAI model to use. If not passed, defaults to gpt-3.5-turbo-16k.\n",
    "        system_prompt: Initial system prompt to the AI model. If not passed, defaults to SYSTEM_PROMPT.\n",
    "        initial_user_prompt: Initial user prompt to the AI model.\n",
    "        params: Parameters to use while initiating the OpenAI chat model. DEFAULT_PARAMS used if not provided.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str,\n",
    "        user_prompt: Optional[str] = None,\n",
    "        params: Dict[str, float] = DEFAULT_PARAMS,\n",
    "        semantic_search_query: Optional[str] = None,\n",
    "    ):\n",
    "        \"\"\"Instantiates a new CustomAIChat object.\n",
    "\n",
    "        Args:\n",
    "            model: The OpenAI model to use. If not passed, defaults to gpt-3.5-turbo-16k.\n",
    "            user_prompt: The user prompt to the AI model.\n",
    "            params: Parameters to use while initiating the OpenAI chat model. DEFAULT_PARAMS used if not provided.\n",
    "            semantic_search_query: A query string to fetch relevant documents from the database\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.messages = [\n",
    "            {\"role\": role, \"content\": content}\n",
    "            for role, content in [\n",
    "                (\"system\", SYSTEM_PROMPT),\n",
    "                (\"user\", self._get_doc(semantic_search_query)),\n",
    "                (\"user\", user_prompt),\n",
    "            ]\n",
    "            if content is not None\n",
    "        ]\n",
    "        self.params = params\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_doc(semantic_search_query: Optional[str] = None) -> str:\n",
    "        if semantic_search_query is None:\n",
    "            return \"\"\n",
    "        return _get_relevant_document(semantic_search_query)\n",
    "    \n",
    "    @_retry_with_exponential_backoff()\n",
    "    def __call__(self, user_prompt: str) -> Tuple[str, Dict[str, int]]:\n",
    "        \"\"\"Call OpenAI API chat completion endpoint and generate a response.\n",
    "\n",
    "        Args:\n",
    "            user_prompt: A string containing user's input prompt.\n",
    "\n",
    "        Returns:\n",
    "            A tuple with AI's response message content and the total number of tokens used while generating the response.\n",
    "        \"\"\"\n",
    "        self.messages.append(\n",
    "            {\"role\": \"user\", \"content\": f\"{user_prompt}\\n==== YOUR RESPONSE ====\\n\"}\n",
    "        )\n",
    "        prompt_str = \"\\n\\n\".join([f\"===Role:{m['role']}===\\n\\nMessage:\\n{m['content']}\" for m in self.messages])\n",
    "        logger.info(f\"\\n\\nPrompt to the model: \\n\\n{prompt_str}\")\n",
    "        \n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=self.model,\n",
    "            messages=self.messages,\n",
    "            temperature=self.params[\"temperature\"],\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            response[\"choices\"][0][\"message\"][\"content\"],\n",
    "            response[\"usage\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d9caa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: \n",
      "\n",
      "Prompt to the model: \n",
      "\n",
      "===Role:system===\n",
      "\n",
      "Message:\n",
      "\n",
      "You are an expert Python developer, tasked to generate executable Python code as a part of your work with the FastStream framework. \n",
      "\n",
      "You are to abide by the following guidelines:\n",
      "\n",
      "1. You must never enclose the generated Python code with ``` python. It is mandatory that the output is a valid and executable Python code. Please ensure this rule is never broken.\n",
      "\n",
      "2. Some prompts might require you to generate code that contains async functions. For example:\n",
      "\n",
      "async def app_setup(context: ContextRepo):\n",
      "    raise NotImplementedError()\n",
      "\n",
      "In such cases, it is necessary to add the \"import asyncio\" statement at the top of the code. \n",
      "\n",
      "You will encounter sections marked as:\n",
      "\n",
      "==== APP DESCRIPTION: ====\n",
      "\n",
      "These sections contain the description of the FastStream app you need to implement. Treat everything below this line, until the end of the prompt, as the description to follow for the app implementation.\n",
      "\n",
      "\n",
      "===Role:user===\n",
      "\n",
      "Message:\n",
      "\n",
      "\n",
      "===Role:user===\n",
      "\n",
      "Message:\n",
      "\n",
      "You should respond with 0, 1 or 2 and nothing else. Below are your rules:\n",
      "\n",
      "==== RULES: ====\n",
      "\n",
      "If the ==== APP DESCRIPTION: ==== section is not related to FastKafka or contains violence, self-harm, harassment/threatening or hate/threatening information then you should respond with 0.\n",
      "\n",
      "If the ==== APP DESCRIPTION: ==== section is related to FastKafka but focuses on what is it and its general information then you should respond with 1. \n",
      "\n",
      "If the ==== APP DESCRIPTION: ==== section is related to FastKafka but focuses how to use it and instructions to create a new app then you should respond with 2. \n",
      "\n",
      "\n",
      "===Role:user===\n",
      "\n",
      "Message:\n",
      "Name the tallest mountain in the world\n",
      "==== YOUR RESPONSE ====\n",
      "\n",
      "0\n",
      "{\n",
      "  \"prompt_tokens\": 348,\n",
      "  \"completion_tokens\": 1,\n",
      "  \"total_tokens\": 349\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "TEST_INITIAL_USER_PROMPT = \"\"\"\n",
    "You should respond with 0, 1 or 2 and nothing else. Below are your rules:\n",
    "\n",
    "==== RULES: ====\n",
    "\n",
    "If the ==== APP DESCRIPTION: ==== section is not related to FastKafka or contains violence, self-harm, harassment/threatening or hate/threatening information then you should respond with 0.\n",
    "\n",
    "If the ==== APP DESCRIPTION: ==== section is related to FastKafka but focuses on what is it and its general information then you should respond with 1. \n",
    "\n",
    "If the ==== APP DESCRIPTION: ==== section is related to FastKafka but focuses how to use it and instructions to create a new app then you should respond with 2. \n",
    "\"\"\"\n",
    "\n",
    "ai = CustomAIChat(user_prompt = TEST_INITIAL_USER_PROMPT, model=OpenAIModel.gpt3.value)\n",
    "response, usage = ai(\"Name the tallest mountain in the world\")\n",
    "\n",
    "print(response)\n",
    "print(usage)\n",
    "\n",
    "assert response == \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d47b1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
