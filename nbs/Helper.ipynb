{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a26be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp _code_generator.helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0885aad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from typing import *\n",
    "import os\n",
    "import re\n",
    "import functools\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "from tempfile import TemporaryDirectory\n",
    "from pathlib import Path\n",
    "from contextlib import contextmanager\n",
    "import unittest.mock\n",
    "import zipfile\n",
    "\n",
    "import typer\n",
    "import requests\n",
    "from langchain.schema.document import Document\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from faststream_gen._components.logger import get_logger, set_level\n",
    "from faststream_gen._components.logger import suppress_timestamps\n",
    "from faststream_gen._code_generator.constants import OPENAI_KEY_EMPTY_ERROR, OPENAI_KEY_NOT_SET_ERROR, TOKEN_TYPES\n",
    "from faststream_gen._components.package_data import get_root_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775e6d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest.mock import patch\n",
    "\n",
    "from faststream_gen._code_generator.constants import FASTSTREAM_DOCS_DIR_SUFFIX, FASTSTREAM_REPO_ZIP_URL, OpenAIModel\n",
    "\n",
    "import pytest\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc3a5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "logger = get_logger(__name__, level=logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e7cae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: ok\n"
     ]
    }
   ],
   "source": [
    "suppress_timestamps()\n",
    "logger = get_logger(__name__, level=20)\n",
    "logger.info(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c0388d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def set_logger_level(func: Callable[..., Any]) -> Callable[..., Any]:\n",
    "    \"\"\"Decorator to set the logger level based on verbosity.\n",
    "\n",
    "    Args:\n",
    "        func: The function to be decorated.\n",
    "\n",
    "    Returns:\n",
    "        The decorated function.\n",
    "    \"\"\"\n",
    "\n",
    "    @functools.wraps(func)\n",
    "    def wrapper_decorator(*args, **kwargs): # type: ignore\n",
    "        if (\"verbose\" in kwargs) and kwargs[\"verbose\"]:\n",
    "            set_level(logging.INFO)\n",
    "        else:\n",
    "            set_level(logging.WARNING)\n",
    "        return func(*args, **kwargs)\n",
    "\n",
    "    return wrapper_decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ff90c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@set_logger_level\n",
    "def _test_logger():\n",
    "    logger.debug(\"INFO\")\n",
    "    logger.info(\"WARNING\")\n",
    "\n",
    "    \n",
    "_test_logger()\n",
    "display(logger.getEffectiveLevel())\n",
    "assert logger.getEffectiveLevel() == logging.WARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0104b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: WARNING\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@set_logger_level\n",
    "def _test_logger(**kwargs):\n",
    "    logger.debug(\"INFO\")\n",
    "    logger.info(\"WARNING\")\n",
    "\n",
    "    \n",
    "_test_logger(verbose=True)\n",
    "display(logger.getEffectiveLevel())\n",
    "assert logger.getEffectiveLevel() == logging.INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfcc221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "\n",
    "def ensure_openai_api_key_set() -> None:\n",
    "    \"\"\"Ensure the 'OPENAI_API_KEY' environment variable is set and is not empty.\n",
    "\n",
    "    Raises:\n",
    "        KeyError: If the 'OPENAI_API_KEY' environment variable is not found.\n",
    "        ValueError: If the 'OPENAI_API_KEY' environment variable is found but its value is empty.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "        if openai_api_key == \"\":\n",
    "            raise ValueError(OPENAI_KEY_EMPTY_ERROR)\n",
    "    except KeyError:\n",
    "        raise KeyError(OPENAI_KEY_NOT_SET_ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46ae154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: OPENAI_API_KEY cannot be empty. Please set a valid OpenAI API key in OPENAI_API_KEY environment variable and try again.\n",
      "You can generate API keys in the OpenAI web interface. See https://platform.openai.com/account/api-keys for details.\n"
     ]
    }
   ],
   "source": [
    "with patch.dict(os.environ, {\"OPENAI_API_KEY\": \"\"}):\n",
    "    with pytest.raises(ValueError) as e:\n",
    "        ensure_openai_api_key_set()\n",
    "\n",
    "print(e.value)\n",
    "assert str(e.value) == OPENAI_KEY_EMPTY_ERROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c9eb8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Error: OPENAI_API_KEY not found in environment variables. Set a valid OpenAI API key in OPENAI_API_KEY environment variable and try again. You can generate API keys in the OpenAI web interface. See https://platform.openai.com/account/api-keys for details.'\n"
     ]
    }
   ],
   "source": [
    "with patch.dict(os.environ, {}, clear=True):\n",
    "    with pytest.raises(KeyError) as e:\n",
    "        ensure_openai_api_key_set()\n",
    "        \n",
    "print(e.value)\n",
    "assert str(e.value) == f\"'{OPENAI_KEY_NOT_SET_ERROR}'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4407ef35",
   "metadata": {},
   "outputs": [],
   "source": [
    "with patch.dict(os.environ, {\"OPENAI_API_KEY\": \"INVALID_KEY\"}):\n",
    "    ensure_openai_api_key_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea47487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def add_tokens_usage(usage_list: List[Dict[str, int]]) -> Dict[str, int]:\n",
    "    \"\"\"Add list of OpenAI \"usage\" dictionaries by categories defined in TOKEN_TYPES (prompt_tokens, completion_tokens and total_tokens).\n",
    "\n",
    "    Args:\n",
    "        usage_list: List of OpenAI \"usage\" dictionaries\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, int]: Dictionary where the keys are TOKEN_TYPES and their values are the sum of OpenAI \"usage\" dictionaries\n",
    "    \"\"\"\n",
    "    added_tokens: Dict[str, int] = defaultdict(int)\n",
    "    for usage in usage_list:\n",
    "        for token_type in TOKEN_TYPES:\n",
    "            added_tokens[token_type] += usage[token_type]\n",
    "            \n",
    "    return added_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbe035e",
   "metadata": {},
   "outputs": [],
   "source": [
    "usage = {\n",
    "    \"prompt_tokens\": 129,\n",
    "    \"completion_tokens\": 1,\n",
    "    \"total_tokens\": 130\n",
    "  }\n",
    "assert add_tokens_usage([usage, usage]) == {\n",
    "    \"prompt_tokens\": 258,\n",
    "    \"completion_tokens\": 2,\n",
    "    \"total_tokens\": 260\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66530414",
   "metadata": {},
   "outputs": [],
   "source": [
    "usage = {\n",
    "    \"prompt_tokens\": 129,\n",
    "    \"completion_tokens\": 1,\n",
    "    \"total_tokens\": 130\n",
    "  }\n",
    "assert add_tokens_usage([defaultdict(int), usage]) == {\n",
    "    \"prompt_tokens\": 129,\n",
    "    \"completion_tokens\": 1,\n",
    "    \"total_tokens\": 130\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf352d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "examples_delimiter = {\n",
    "    \"description\": {\n",
    "        \"start\": \"==== description.txt starts ====\",\n",
    "        \"end\": \"==== description.txt ends ====\",\n",
    "    },\n",
    "    \"skeleton\": {\n",
    "        \"start\": \"==== app_skeleton.py starts ====\",\n",
    "        \"end\": \"==== app_skeleton.py ends ====\",\n",
    "    },\n",
    "    \"app\": {\n",
    "        \"start\": \"==== app.py starts ====\",\n",
    "        \"end\": \"==== app.py ends ====\",\n",
    "    },\n",
    "    \"test_app\": {\n",
    "        \"start\": \"==== test_app.py starts ====\",\n",
    "        \"end\": \"==== test_app.py ends ====\",\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def _split_text(text: str, delimiter: Dict[str, str]) -> str:\n",
    "    return text.split(delimiter[\"start\"])[-1].split(delimiter[\"end\"])[0]\n",
    "\n",
    "\n",
    "def _format_examples(parent_docs_str: List[str]) -> Dict[str, str]:\n",
    "    \"\"\"Format and extract examples from parent document.\n",
    "\n",
    "    Args:\n",
    "        parent_docs_str (List[str]): A list of parent document strings containing example sections.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, List[str]]: A dictionary with sections as keys and lists of formatted examples as values.\n",
    "    \"\"\"\n",
    "    ret_val = {\"description_to_skeleton\": \"\", \"skeleton_to_app_and_test\": \"\"}\n",
    "    for d in parent_docs_str:\n",
    "        description = _split_text(d, examples_delimiter[\"description\"])\n",
    "        skeleton = _split_text(d, examples_delimiter[\"skeleton\"])\n",
    "        app = _split_text(d, examples_delimiter[\"app\"])\n",
    "        test_app = _split_text(d, examples_delimiter[\"test_app\"])\n",
    "\n",
    "        ret_val[\n",
    "            \"description_to_skeleton\"\n",
    "        ] += f\"\\n==== EXAMPLE APP DESCRIPTION ====\\n{description}\\n\\n==== YOUR RESPONSE ====\\n\\n{skeleton}\"\n",
    "        ret_val[\n",
    "            \"skeleton_to_app_and_test\"\n",
    "        ] += f\"\\n==== EXAMPLE APP DESCRIPTION ====\\n{description}\\n\\n==== EXAMPLE APP SKELETON ====\\n{skeleton}\\n==== YOUR RESPONSE ====\\n\\n### application.py ###\\n{app}\\n### test.py ###\\n{test_app}\"\n",
    "\n",
    "    return ret_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff2ef39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'description_to_skeleton': '\\n==== EXAMPLE APP DESCRIPTION ====\\n\\ndescription.txt\\n\\n\\n==== YOUR RESPONSE ====\\n\\n\\napp_skeleton.py\\n', 'skeleton_to_app_and_test': '\\n==== EXAMPLE APP DESCRIPTION ====\\n\\ndescription.txt\\n\\n\\n==== EXAMPLE APP SKELETON ====\\n\\napp_skeleton.py\\n\\n==== YOUR RESPONSE ====\\n\\n### application.py ###\\n\\napp.py\\n\\n### test.py ###\\n\\ntest_app.py\\n'}\n"
     ]
    }
   ],
   "source": [
    "fixture = [\n",
    "    \"\"\"\n",
    "==== description.txt starts ====\n",
    "description.txt\n",
    "==== description.txt ends ====\n",
    "==== app_skeleton.py starts ====\n",
    "app_skeleton.py\n",
    "==== app_skeleton.py ends ====\n",
    "==== app.py starts ====\n",
    "app.py\n",
    "==== app.py ends ====\n",
    "==== test_app.py starts ====\n",
    "test_app.py\n",
    "==== test_app.py ends ====\n",
    "\"\"\"\n",
    "]\n",
    "expected = {\n",
    "    \"description_to_skeleton\": \"\\n==== EXAMPLE APP DESCRIPTION ====\\n\\ndescription.txt\\n\\n\\n==== YOUR RESPONSE ====\\n\\n\\napp_skeleton.py\\n\",\n",
    "    \"skeleton_to_app_and_test\": \"\\n==== EXAMPLE APP DESCRIPTION ====\\n\\ndescription.txt\\n\\n\\n==== EXAMPLE APP SKELETON ====\\n\\napp_skeleton.py\\n\\n==== YOUR RESPONSE ====\\n\\n### application.py ###\\n\\napp.py\\n\\n### test.py ###\\n\\ntest_app.py\\n\",\n",
    "}\n",
    "\n",
    "actual = _format_examples(fixture)\n",
    "print(actual)\n",
    "\n",
    "assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380e8cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def get_relevant_prompt_examples(query: str) -> Dict[str, str]:\n",
    "    \"\"\"Load the vector database and retrieve the most relevant examples based on the given query for each step.\n",
    "\n",
    "    Args:\n",
    "        query: The query for relevance-based document retrieval.\n",
    "\n",
    "    Returns:\n",
    "        The dictionary of the most relevant examples for each step.\n",
    "    \"\"\"\n",
    "    db_path = get_root_data_path() / \"examples\"\n",
    "    db = FAISS.load_local(db_path, OpenAIEmbeddings()) # type: ignore\n",
    "    results = db.similarity_search(query, k=3, fetch_k=5)\n",
    "    results_page_content = [r.page_content for r in results]\n",
    "    prompt_examples = _format_examples(results_page_content)\n",
    "    return prompt_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed9f83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] faiss.loader: Loading faiss with AVX2 support.\n",
      "[INFO] faiss.loader: Successfully loaded faiss with AVX2 support.\n",
      "\n",
      "==== EXAMPLE APP DESCRIPTION ====\n",
      "\n",
      "Develop a FastStream application using localhost kafka broker.\n",
      "The app should consume messages from the input_data topic.\n",
      "The input message is a JSON encoded object including two attributes:\n",
      "    - x: float\n",
      "    - y: float\n",
      "    - time: datetime\n",
      "\n",
      "input_data topic should use partition key.\n",
      "While consuming the message, increment x and y attributes by 1 and publish that message to the output_data topic.\n",
      "The same partition key should be used in the input_data and output_data topic.\n",
      "\n",
      "\n",
      "\n",
      "==== YOUR RESPONSE ====\n",
      "\n",
      "\n",
      "from datetime import datetime\n",
      "\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "from faststream import Context, FastStream, Logger\n",
      "from faststream.kafka import KafkaBroker\n",
      "\n",
      "\n",
      "class Point(BaseModel):\n",
      "    x: float = Field(\n",
      "        ..., examples=[0.5], description=\"The X Coordinate in the coordinate system\"\n",
      "    )\n",
      "    y: float = Field(\n",
      "        ..., examples=[0.5], description=\"The Y Coordinate in the coordinate system\"\n",
      "    )\n",
      "    time: datetime = Field(\n",
      "        ...,\n",
      "        examples=[\"2020-04-23 10:20:30.400000\"],\n",
      "        description=\"The timestamp of the record\",\n",
      "    )\n",
      "\n",
      "\n",
      "broker = KafkaBroker(\"localhost:9092\")\n",
      "app = FastStream(broker)\n",
      "\n",
      "\n",
      "to_output_data = broker.publisher(\"output_data\")\n",
      "\n",
      "\n",
      "@broker.subscriber(\"input_data\")\n",
      "async def on_input_data(\n",
      "    msg: Point, logger: Logger, key: bytes = Context(\"message.raw_message.key\")\n",
      ") -> None:\n",
      "    \"\"\"\n",
      "    Processes a message from the 'input_data' topic.\n",
      "    Increment msg x and y attributes with 1 and publish that message to the output_data topic.\n",
      "    The same partition key should be used in the input_data and output_data topic.\n",
      "\n",
      "    Instructions:\n",
      "    1. Consume a message from 'input_data' topic.\n",
      "    2. Create a new message object (do not directly modify the original).\n",
      "    3. Increment msg x and y attributes with 1.\n",
      "    4. Publish that message to the output_data topic (The same partition key should be used in the input_data and output_data topic).\n",
      "    \"\"\"\n",
      "    raise NotImplementedError()\n",
      "\n",
      "\n",
      "==== EXAMPLE APP DESCRIPTION ====\n",
      "\n",
      "Develop a FastStream application using localhost kafka broker.\n",
      "The app should consume messages from the input_data topic.\n",
      "The input message is a JSON encoded object including two attributes:\n",
      "    - x: float\n",
      "    - y: float\n",
      "\n",
      "While consuming the message, increment x and y attributes by 1 and publish that message to the output_data topic.\n",
      "Use messages attribute x as a partition key when publishing to output_data topic.\n",
      "\n",
      "\n",
      "\n",
      "==== YOUR RESPONSE ====\n",
      "\n",
      "\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "from faststream import FastStream, Logger\n",
      "from faststream.kafka import KafkaBroker\n",
      "\n",
      "\n",
      "class Point(BaseModel):\n",
      "    x: float = Field(\n",
      "        ..., examples=[0.5], description=\"The X Coordinate in the coordinate system\"\n",
      "    )\n",
      "    y: float = Field(\n",
      "        ..., examples=[0.5], description=\"The Y Coordinate in the coordinate system\"\n",
      "    )\n",
      "\n",
      "\n",
      "broker = KafkaBroker(\"localhost:9092\")\n",
      "app = FastStream(broker)\n",
      "\n",
      "\n",
      "to_output_data = broker.publisher(\"output_data\")\n",
      "\n",
      "\n",
      "@broker.subscriber(\"input_data\")\n",
      "async def on_input_data(msg: Point, logger: Logger) -> None:\n",
      "    \"\"\"\n",
      "    Processes a message from the 'input_data' topic.\n",
      "    Increment msg x and y attributes with 1 and publish that message to the output_data topic.\n",
      "    Publish that message to the output_data topic\n",
      "    Use messages attribute x as a partition key when publishing to output_data topic.\n",
      "\n",
      "    Instructions:\n",
      "    1. Consume a message from 'input_data' topic.\n",
      "    2. Create a new message object (do not directly modify the original).\n",
      "    3. Increment msg x and y attributes with 1.\n",
      "    4. Publish that message to the output_data topic (Use messages attribute x as a partition key).\n",
      "    \"\"\"\n",
      "    raise NotImplementedError()\n",
      "\n",
      "\n",
      "==== EXAMPLE APP DESCRIPTION ====\n",
      "\n",
      "Develop a FastStream application using localhost kafka broker.\n",
      "The app should consume messages from the input_data topic.\n",
      "The input message is a JSON encoded object including two attributes:\n",
      "    - x: float\n",
      "    - y: float\n",
      "    - time: datetime\n",
      "\n",
      "input_data topic should use partition key.\n",
      "\n",
      "Keep all the previous messages in the memory.\n",
      "While consuming the message, add all x elements from the memory (x_sum) and all y from the memory (y_sum) and publish the message with x_sum and y_sum to the output_data topic.\n",
      "The same partition key should be used in the input_data and output_data topic.\n",
      "\n",
      "\n",
      "\n",
      "==== YOUR RESPONSE ====\n",
      "\n",
      "\n",
      "from datetime import datetime\n",
      "from typing import List\n",
      "\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "from faststream import Context, ContextRepo, FastStream, Logger\n",
      "from faststream.kafka import KafkaBroker\n",
      "\n",
      "\n",
      "class Point(BaseModel):\n",
      "    x: float = Field(\n",
      "        ..., examples=[0.5], description=\"The X Coordinate in the coordinate system\"\n",
      "    )\n",
      "    y: float = Field(\n",
      "        ..., examples=[0.5], description=\"The Y Coordinate in the coordinate system\"\n",
      "    )\n",
      "    time: datetime = Field(\n",
      "        ...,\n",
      "        examples=[\"2020-04-23 10:20:30.400000\"],\n",
      "        description=\"The timestamp of the record\",\n",
      "    )\n",
      "\n",
      "\n",
      "broker = KafkaBroker(\"localhost:9092\")\n",
      "app = FastStream(broker)\n",
      "\n",
      "\n",
      "to_output_data = broker.publisher(\"output_data\")\n",
      "\n",
      "\n",
      "@app.on_startup\n",
      "async def app_setup(context: ContextRepo):\n",
      "    \"\"\"\n",
      "    Set all necessary global variables inside ContextRepo object:\n",
      "        Set message_history for storing all input messages\n",
      "    \"\"\"\n",
      "    raise NotImplementedError()\n",
      "\n",
      "\n",
      "@broker.subscriber(\"input_data\")\n",
      "async def on_input_data(\n",
      "    msg: Point,\n",
      "    logger: Logger,\n",
      "    message_history: List[Point] = Context(),\n",
      "    key: bytes = Context(\"message.raw_message.key\"),\n",
      ") -> None:\n",
      "    \"\"\"\n",
      "    Processes a message from the 'input_data' topic.\n",
      "    Add all x elements from the memory (x_sum) and all y from the memory (y_sum) and publish the message with x_sum and y_sum to the output_data topic.\n",
      "    The same partition key should be used in the input_data and output_data topic.\n",
      "\n",
      "    Instructions:\n",
      "    1. Consume a message from 'input_data' topic.\n",
      "    2. Create a new message object (do not directly modify the original).\n",
      "    3. Add all x elements from the memory (x_sum) and all y from the memory (y_sum)\n",
      "    4. Publish the message with x_sum and y_sum to the output_data topic. (The same partition key should be used in the input_data and output_data topic).\n",
      "    \"\"\"\n",
      "    raise NotImplementedError()\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "Create a FastStream application using localhost broker for testing and use the default port number. \n",
    "It should consume messages from the \"input_data\" topic, where each message is a JSON encoded object containing a single attribute: 'data'. \n",
    "For each consumed message, create a new message object and increment the value of the data attribute by 1. Finally, send the modified message to the 'output_data' topic.\n",
    "\"\"\"\n",
    "\n",
    "actual = get_relevant_prompt_examples(query)\n",
    "\n",
    "\n",
    "\n",
    "assert \"==== EXAMPLE APP DESCRIPTION ====\" in actual[\"description_to_skeleton\"]\n",
    "assert \"==== app_skeleton.py starts ====\" not in actual[\"description_to_skeleton\"]\n",
    "print(actual[\"description_to_skeleton\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe64187d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def strip_white_spaces(description: str) -> str:\n",
    "    \"\"\"Remove and strip excess whitespaces from a given description\n",
    "\n",
    "    Args:\n",
    "        description: The description string to be processed.\n",
    "\n",
    "    Returns:\n",
    "        The cleaned description string.\n",
    "    \"\"\"\n",
    "    pattern = re.compile(r\"\\s+\")\n",
    "    return pattern.sub(\" \", description).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de76500c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a lot of whitespaces\n"
     ]
    }
   ],
   "source": [
    "fixture = \"\"\"\n",
    "    I have   a                  lot\n",
    "                of whitespaces\n",
    "                \n",
    "                \n",
    "\"\"\"\n",
    "\n",
    "expected = \"I have a lot of whitespaces\"\n",
    "actual = strip_white_spaces(fixture)\n",
    "print(actual)\n",
    "assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9245b2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def write_file_contents(output_file: str, contents: str) -> None:\n",
    "    \"\"\"Write the given contents to the specified output file.\n",
    "\n",
    "    Args:\n",
    "        output_file: The path to the output file where the contents will be written.\n",
    "        contents: The contents to be written to the output file.\n",
    "\n",
    "    Raises:\n",
    "        OSError: If there is an issue while attempting to save the file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        Path(output_file).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(contents)\n",
    "\n",
    "    except OSError as e:\n",
    "        raise OSError(\n",
    "            f\"Error: Failed to save file at '{output_file}' due to: '{e}'. Please ensure that the specified 'output_path' is valid and that you have the necessary permissions to write files to it.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615aaccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tmp3_wc1os5/grand-parent/parent/child/application.py\n",
      "\n",
      "\n",
      "print(\"Hello World\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contents = \"\"\"\n",
    "print(\"Hello World\")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "with TemporaryDirectory() as d:\n",
    "    output_path = f\"{str(d)}/grand-parent/parent/child\"\n",
    "    output_file = f\"{output_path}/application.py\"\n",
    "    \n",
    "    write_file_contents(output_file, contents)\n",
    "    \n",
    "    with open(output_file, 'r', encoding=\"utf-8\") as f:\n",
    "        actual = f.read()\n",
    "    print(f\"{output_file}\\n\\n{actual}\")\n",
    "\n",
    "assert actual == contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6383e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def mock_openai_create(test_response):\n",
    "    mock_choices = {\n",
    "        \"choices\": [{\"message\": {\"content\": test_response}}],\n",
    "        \"usage\": { \n",
    "            \"prompt_tokens\": 129,\n",
    "            \"completion_tokens\": 1,\n",
    "            \"total_tokens\": 130\n",
    "        },\n",
    "    }\n",
    "\n",
    "    with unittest.mock.patch(\"openai.ChatCompletion\") as mock:\n",
    "        mock.create.return_value = mock_choices\n",
    "        yield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539a8279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a mock response\n"
     ]
    }
   ],
   "source": [
    "test_response = \"This is a mock response\"\n",
    "\n",
    "with mock_openai_create(test_response):\n",
    "    response = openai.ChatCompletion.create()\n",
    "    ret_val = response['choices'][0]['message']['content']\n",
    "    print(ret_val)\n",
    "    assert ret_val == test_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f92646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def _fetch_content(url: str) -> requests.models.Response: # type: ignore\n",
    "    \"\"\"Fetch content from a URL using an HTTP GET request.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL to fetch content from.\n",
    "\n",
    "    Returns:\n",
    "        Response: The response object containing the content and HTTP status.\n",
    "\n",
    "    Raises:\n",
    "        requests.exceptions.Timeout: If the request times out.\n",
    "        requests.exceptions.RequestException: If an error occurs during the request.\n",
    "    \"\"\"\n",
    "    attempt = 0\n",
    "    while attempt < 4:\n",
    "        try:\n",
    "            response = requests.get(url, timeout=50)\n",
    "            response.raise_for_status()  # Raises an exception for HTTP errors\n",
    "            return response\n",
    "        except requests.exceptions.Timeout:\n",
    "            if attempt == 3:  # If this was the fourth attempt, raise the Timeout exception\n",
    "                raise requests.exceptions.Timeout(\n",
    "                    \"Request timed out. Please check your internet connection or try again later.\"\n",
    "                )\n",
    "            time.sleep(1)  # Sleep for one second before retrying\n",
    "            attempt += 1\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            raise requests.exceptions.RequestException(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1717c00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<!doctype html>\\n<html lang=\"en\" dir=\"ltr\" class=\"plugin-pages plugin-id-default\">\\n<head>\\n<meta charset=\"UTF-8\">\\n<meta name=\"generator\" content=\"Docusaurus v2.4.0\">\\n<title data-rh=\"true\">Effortless Kaf'\n"
     ]
    }
   ],
   "source": [
    "response = _fetch_content(\"https://fastkafka.airt.ai/\")\n",
    "print(response.content[:200])\n",
    "assert len(response.content) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2eb0a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def download_and_extract_faststream_archive(url: str) -> Generator[Path, None, None]:\n",
    "    with TemporaryDirectory() as d:\n",
    "        try:\n",
    "            input_path = Path(f\"{d}/archive.zip\")\n",
    "            extrated_path = Path(f\"{d}/extrated_path\")\n",
    "            extrated_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            response = _fetch_content(url)\n",
    "\n",
    "            with open(input_path, \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "\n",
    "            with zipfile.ZipFile(input_path, \"r\") as zip_ref:\n",
    "                for member in zip_ref.namelist():\n",
    "                    zip_ref.extract(member, extrated_path)\n",
    "\n",
    "            yield extrated_path\n",
    "\n",
    "        except Exception as e:\n",
    "            fg = typer.colors.RED\n",
    "            typer.secho(f\"Unexpected internal error: {e}\", err=True, fg=fg)\n",
    "            raise typer.Exit(code=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164f58a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['api', 'kafka', 'getting-started', 'index', 'release', 'rabbit']\n"
     ]
    }
   ],
   "source": [
    "with download_and_extract_faststream_archive(FASTSTREAM_REPO_ZIP_URL) as extracted_path:\n",
    "    files = [p.stem for p in list(Path(extracted_path/FASTSTREAM_DOCS_DIR_SUFFIX).glob(\"*\"))]\n",
    "    print(files)\n",
    "    assert \"index\" in files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987c48c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d18126",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
