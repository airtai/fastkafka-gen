{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a26be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp _code_generator.helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bb7431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from typing import *\n",
    "import random\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "import functools\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from tempfile import TemporaryDirectory\n",
    "import importlib.util\n",
    "import os\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "import openai\n",
    "from fastcore.foundation import patch\n",
    "\n",
    "from fastkafka_gen._components.logger import get_logger, set_level\n",
    "from fastkafka_gen._code_generator.prompts import SYSTEM_PROMPT, DEFAULT_FASTKAFKA_PROMPT\n",
    "from fastkafka_gen._code_generator.constants import DEFAULT_PARAMS, DEFAULT_MODEL, MAX_RETRIES, ASYNC_API_SPEC_FILE_NAME, APPLICATION_FILE_NAME, TOKEN_TYPES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a580a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import sys\n",
    "import unittest.mock\n",
    "\n",
    "from fastkafka_gen._components.logger import suppress_timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25822c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c529a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: ok\n"
     ]
    }
   ],
   "source": [
    "suppress_timestamps()\n",
    "logger = get_logger(__name__, level=20)\n",
    "logger.info(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b99493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "@contextmanager\n",
    "def add_dir_to_sys_path(dir_: str) -> Generator[None, None, None]:\n",
    "    \"\"\"Add a directory path to sys.path\n",
    "\n",
    "    Args:\n",
    "        dir_ : the path to add to sys.path\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If dir_ is None\n",
    "    \"\"\"\n",
    "    dir_path = Path(dir_).absolute().resolve(strict=True)\n",
    "    original_path = sys.path[:]\n",
    "    sys.path.insert(0, str(dir_path))\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        sys.path = original_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b775a775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tmp6am0pwes\n"
     ]
    }
   ],
   "source": [
    "with TemporaryDirectory() as d:\n",
    "    print(d)\n",
    "    with add_dir_to_sys_path(d):\n",
    "        sys_path = [p.split(\"/\")[-1] for p in sys.path[:]]\n",
    "        assert d.split(\"/\")[-1] in sys_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c112cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def write_file_contents(output_file: str, contents: str) -> None:\n",
    "    \"\"\"Write the given contents to the specified output file.\n",
    "\n",
    "    Args:\n",
    "        output_file: The path to the output file where the contents will be written.\n",
    "        contents: The contents to be written to the output file.\n",
    "\n",
    "    Raises:\n",
    "        OSError: If there is an issue while attempting to save the file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        Path(output_file).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(contents)\n",
    "\n",
    "    except OSError as e:\n",
    "        raise OSError(\n",
    "            f\"Error: Failed to save file at '{output_file}' due to: '{e}'. Please ensure that the specified 'output_path' is valid and that you have the necessary permissions to write files to it.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8631d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tmp8ab8vdha/grand-parent/parent/child/application.py\n",
      "\n",
      "\n",
      "print(\"Hello World\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contents = \"\"\"\n",
    "print(\"Hello World\")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "with TemporaryDirectory() as d:\n",
    "    output_path = f\"{str(d)}/grand-parent/parent/child\"\n",
    "    output_file = f\"{output_path}/application.py\"\n",
    "    \n",
    "    write_file_contents(output_file, contents)\n",
    "    \n",
    "    with open(output_file, 'r', encoding=\"utf-8\") as f:\n",
    "        actual = f.read()\n",
    "    print(f\"{output_file}\\n\\n{actual}\")\n",
    "\n",
    "assert actual == contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57edc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def read_file_contents(output_file: str) -> str:\n",
    "    \"\"\"Read and return the contents from the specified file.\n",
    "\n",
    "    Args:\n",
    "        output_file: The path to the file to be read.\n",
    "\n",
    "    Returns:\n",
    "        The contents of the file as string.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the specified file does not exist.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            contents = f.read()\n",
    "        return contents\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Error: The file '{output_file}' does not exist. Please ensure that the specified 'output_path' is valid and that you have the necessary permissions to access it.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a02beb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tmp3ls75y8a/grand-parent/parent/child/application.py\n",
      "\n",
      "\n",
      "print(\"Hello World\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contents = \"\"\"\n",
    "print(\"Hello World\")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "with TemporaryDirectory() as d:\n",
    "    output_path = f\"{str(d)}/grand-parent/parent/child\"\n",
    "    output_file = f\"{output_path}/application.py\"\n",
    "    \n",
    "    write_file_contents(output_file, contents)\n",
    "    \n",
    "    actual = read_file_contents(output_file)\n",
    "    print(f\"{output_file}\\n\\n{actual}\")\n",
    "\n",
    "assert actual == contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f529c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ExceptionInfo FileNotFoundError(\"Error: The file '/tmp/tmp3r80yi08/grand-parent/parent/child/application.py' does not exist. Please ensure that the specified 'output_path' is valid and that you have the necessary permissions to access it.\") tblen=2>\n"
     ]
    }
   ],
   "source": [
    "contents = \"\"\"\n",
    "print(\"Hello World\")\n",
    "\"\"\"\n",
    "\n",
    "with pytest.raises(FileNotFoundError) as e:\n",
    "    with TemporaryDirectory() as d:\n",
    "        output_path = f\"{str(d)}/grand-parent/parent/child\"\n",
    "        output_file = f\"{output_path}/application.py\"\n",
    "\n",
    "        actual = read_file_contents(output_file)\n",
    "\n",
    "print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e0491e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def validate_python_code(code: str) -> List[str]:\n",
    "    \"\"\"Validate and report errors in the provided Python code.\n",
    "\n",
    "    Args:\n",
    "        code: The Python code as a string.\n",
    "\n",
    "    Returns:\n",
    "        A list of error messages encountered during validation. If no errors occur, an empty list is returned.\n",
    "    \"\"\"\n",
    "    with TemporaryDirectory() as d:\n",
    "        try:\n",
    "            temp_file = Path(d) / APPLICATION_FILE_NAME\n",
    "            write_file_contents(str(temp_file), code)\n",
    "\n",
    "            # Import the module using importlib\n",
    "            spec = importlib.util.spec_from_file_location(\"tmp_module\", temp_file)\n",
    "            module = importlib.util.module_from_spec(spec) # type: ignore\n",
    "            spec.loader.exec_module(module) # type: ignore\n",
    "\n",
    "        except Exception as e:\n",
    "            return [ f\"{type(e).__name__}: {e}\"]\n",
    "\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd67467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "fixture = \"\"\"\n",
    "import os\n",
    "def say_hello():\n",
    "    print(\"hello\")\n",
    "\"\"\"\n",
    "\n",
    "actual = validate_python_code(fixture)\n",
    "expected = []\n",
    "\n",
    "print(actual)\n",
    "assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c440dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"ModuleNotFoundError: No module named 'invalid_module'\"]\n"
     ]
    }
   ],
   "source": [
    "fixture = \"\"\"\n",
    "import os\n",
    "import invalid_module\n",
    "def say_hello():\n",
    "    print(\"hello\")\n",
    "\"\"\"\n",
    "\n",
    "actual = validate_python_code(fixture)\n",
    "expected = [\"ModuleNotFoundError: No module named 'invalid_module'\"]\n",
    "\n",
    "print(actual)\n",
    "assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07db0664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"SyntaxError: expected ':' (application.py, line 3)\"]\n"
     ]
    }
   ],
   "source": [
    "fixture = \"\"\"\n",
    "import os\n",
    "def say_hello()\n",
    "    print(\"hello\")\n",
    "\"\"\"\n",
    "\n",
    "actual = validate_python_code(fixture)\n",
    "expected = (\n",
    "    [\"SyntaxError: invalid syntax (application.py, line 3)\"]\n",
    "    if sys.version_info < (3, 10)\n",
    "    else [\"SyntaxError: expected ':' (application.py, line 3)\"]\n",
    ")\n",
    "\n",
    "print(actual)\n",
    "assert (\n",
    "    actual == expected\n",
    "), f\"actual = {actual} - expected = {expected} - sys.version_info = {sys.version_info}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791a8b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def set_logger_level(func: Callable[..., Any]) -> Callable[..., Any]:\n",
    "    \"\"\"Decorator to set the logger level based on verbosity.\n",
    "\n",
    "    Args:\n",
    "        func: The function to be decorated.\n",
    "\n",
    "    Returns:\n",
    "        The decorated function.\n",
    "    \"\"\"\n",
    "\n",
    "    @functools.wraps(func)\n",
    "    def wrapper_decorator(*args, **kwargs): # type: ignore\n",
    "        if (\"verbose\" in kwargs) and kwargs[\"verbose\"]:\n",
    "            set_level(logging.INFO)\n",
    "        else:\n",
    "            set_level(logging.WARNING)\n",
    "        return func(*args, **kwargs)\n",
    "\n",
    "    return wrapper_decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35821f6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@set_logger_level\n",
    "def _test_logger():\n",
    "    logger.debug(\"INFO\")\n",
    "    logger.info(\"WARNING\")\n",
    "\n",
    "    \n",
    "_test_logger()\n",
    "display(logger.getEffectiveLevel())\n",
    "assert logger.getEffectiveLevel() == logging.WARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946ed3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: WARNING\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@set_logger_level\n",
    "def _test_logger(**kwargs):\n",
    "    logger.debug(\"INFO\")\n",
    "    logger.info(\"WARNING\")\n",
    "\n",
    "    \n",
    "_test_logger(verbose=True)\n",
    "display(logger.getEffectiveLevel())\n",
    "assert logger.getEffectiveLevel() == logging.INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8ee713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "# Reference: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_handle_rate_limits.ipynb\n",
    "\n",
    "\n",
    "def _retry_with_exponential_backoff(\n",
    "    initial_delay: float = 1,\n",
    "    exponential_base: float = 2,\n",
    "    jitter: bool = True,\n",
    "    max_retries: int = 10,\n",
    "    max_wait: float = 60,\n",
    "    errors: tuple = (\n",
    "        openai.error.RateLimitError,\n",
    "        openai.error.ServiceUnavailableError,\n",
    "        openai.error.APIError,\n",
    "    ),\n",
    ") -> Callable:\n",
    "    \"\"\"Retry a function with exponential backoff.\"\"\"\n",
    "\n",
    "    def decorator(\n",
    "        func: Callable[[str], Tuple[str, str]]\n",
    "    ) -> Callable[[str], Tuple[str, str]]:\n",
    "        def wrapper(*args, **kwargs):  # type: ignore\n",
    "            num_retries = 0\n",
    "            delay = initial_delay\n",
    "\n",
    "            while True:\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "\n",
    "                except errors as e:\n",
    "                    num_retries += 1\n",
    "                    if num_retries > max_retries:\n",
    "                        raise Exception(\n",
    "                            f\"Maximum number of retries ({max_retries}) exceeded.\"\n",
    "                        )\n",
    "                    delay = min(\n",
    "                        delay\n",
    "                        * exponential_base\n",
    "                        * (1 + jitter * random.random()),  # nosec\n",
    "                        max_wait,\n",
    "                    )\n",
    "                    logger.info(\n",
    "                        f\"Note: OpenAI's API rate limit reached. Command will automatically retry in {int(delay)} seconds. For more information visit: https://help.openai.com/en/articles/5955598-is-api-usage-subject-to-any-rate-limits\",\n",
    "                    )\n",
    "                    time.sleep(delay)\n",
    "\n",
    "                except Exception as e:\n",
    "                    raise e\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234e0a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "@_retry_with_exponential_backoff()\n",
    "def mock_func():\n",
    "    return \"Success\"\n",
    "\n",
    "actual = mock_func()\n",
    "expected = \"Success\"\n",
    "\n",
    "print(actual)\n",
    "assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f323384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: Note: OpenAI's API rate limit reached. Command will automatically retry in 3 seconds. For more information visit: https://help.openai.com/en/articles/5955598-is-api-usage-subject-to-any-rate-limits\n",
      "Maximum number of retries (1) exceeded.\n"
     ]
    }
   ],
   "source": [
    "# Test max retries exceeded\n",
    "@_retry_with_exponential_backoff(max_retries=1)\n",
    "def mock_func_error():\n",
    "    raise openai.error.RateLimitError\n",
    "\n",
    "\n",
    "with pytest.raises(Exception) as e:\n",
    "    mock_func_error()\n",
    "\n",
    "print(e.value)\n",
    "assert str(e.value) == \"Maximum number of retries (1) exceeded.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc8b40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class CustomAIChat:\n",
    "    \"\"\"Custom class for interacting with OpenAI\n",
    "\n",
    "    Attributes:\n",
    "        model: The OpenAI model to use. If not passed, defaults to gpt-3.5-turbo-16k.\n",
    "        system_prompt: Initial system prompt to the AI model. If not passed, defaults to SYSTEM_PROMPT.\n",
    "        initial_user_prompt: Initial user prompt to the AI model.\n",
    "        params: Parameters to use while initiating the OpenAI chat model. DEFAULT_PARAMS used if not provided.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Optional[str] = DEFAULT_MODEL,\n",
    "        user_prompt: Optional[str] = None,\n",
    "        params: Dict[str, float] = DEFAULT_PARAMS,\n",
    "    ):\n",
    "        \"\"\"Instantiates a new CustomAIChat object.\n",
    "\n",
    "        Args:\n",
    "            model: The OpenAI model to use. If not passed, defaults to gpt-3.5-turbo-16k.\n",
    "            user_prompt: The user prompt to the AI model.\n",
    "            params: Parameters to use while initiating the OpenAI chat model. DEFAULT_PARAMS used if not provided.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.messages = [\n",
    "            {\"role\": role, \"content\": content}\n",
    "            for role, content in [\n",
    "                (\"system\", SYSTEM_PROMPT),\n",
    "                (\"user\", DEFAULT_FASTKAFKA_PROMPT),\n",
    "                (\"user\", user_prompt),\n",
    "            ]\n",
    "            if content is not None\n",
    "        ]\n",
    "        self.params = params\n",
    "\n",
    "    @_retry_with_exponential_backoff()\n",
    "    def __call__(self, user_prompt: str) -> Tuple[str, Dict[str, int]]:\n",
    "        \"\"\"Call OpenAI API chat completion endpoint and generate a response.\n",
    "\n",
    "        Args:\n",
    "            user_prompt: A string containing user's input prompt.\n",
    "\n",
    "        Returns:\n",
    "            A tuple with AI's response message content and the total number of tokens used while generating the response.\n",
    "        \"\"\"\n",
    "        self.messages.append(\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        )\n",
    "                \n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=self.model,\n",
    "            messages=self.messages,\n",
    "            temperature=self.params[\"temperature\"],\n",
    "        )\n",
    "        \n",
    "        return (\n",
    "            response[\"choices\"][0][\"message\"][\"content\"],\n",
    "            response[\"usage\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f5a04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "{\n",
      "  \"prompt_tokens\": 2135,\n",
      "  \"completion_tokens\": 1,\n",
      "  \"total_tokens\": 2136\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "TEST_INITIAL_USER_PROMPT = \"\"\"\n",
    "You should respond with 0, 1 or 2 and nothing else. Below are your rules:\n",
    "\n",
    "==== RULES: ====\n",
    "\n",
    "If the ==== APP DESCRIPTION: ==== section is not related to FastKafka or contains violence, self-harm, harassment/threatening or hate/threatening information then you should respond with 0.\n",
    "\n",
    "If the ==== APP DESCRIPTION: ==== section is related to FastKafka but focuses on what is it and its general information then you should respond with 1. \n",
    "\n",
    "If the ==== APP DESCRIPTION: ==== section is related to FastKafka but focuses how to use it and instructions to create a new app then you should respond with 2. \n",
    "\"\"\"\n",
    "\n",
    "ai = CustomAIChat(user_prompt = TEST_INITIAL_USER_PROMPT)\n",
    "response, usage = ai(\"Name the tallest mountain in the world\")\n",
    "\n",
    "print(response)\n",
    "print(usage)\n",
    "\n",
    "assert response == \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a2d87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@contextmanager\n",
    "def mock_openai_create(test_response):\n",
    "    mock_choices = {\n",
    "        \"choices\": [{\"message\": {\"content\": test_response}}],\n",
    "        \"usage\": { \n",
    "            \"prompt_tokens\": 129,\n",
    "            \"completion_tokens\": 1,\n",
    "            \"total_tokens\": 130\n",
    "        },\n",
    "    }\n",
    "\n",
    "    with unittest.mock.patch(\"openai.ChatCompletion\") as mock:\n",
    "        mock.create.return_value = mock_choices\n",
    "        yield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e27da2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a mock response\n"
     ]
    }
   ],
   "source": [
    "test_response = \"This is a mock response\"\n",
    "\n",
    "with mock_openai_create(test_response):\n",
    "    response = openai.ChatCompletion.create()\n",
    "    ret_val = response['choices'][0]['message']['content']\n",
    "    print(ret_val)\n",
    "    assert ret_val == test_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8fc36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class ValidateAndFixResponse:\n",
    "    \"\"\"Generates and validates response from OpenAI\n",
    "\n",
    "    Attributes:\n",
    "        generate: A callable object for generating responses.\n",
    "        validate: A callable object for validating responses.\n",
    "        max_attempts: An optional integer specifying the maximum number of attempts to generate and validate a response.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        generate: Callable[..., Any],\n",
    "        validate: Callable[..., Any],\n",
    "        max_attempts: Optional[int] = MAX_RETRIES,\n",
    "    ):\n",
    "        self.generate = generate\n",
    "        self.validate = validate\n",
    "        self.max_attempts = max_attempts\n",
    "\n",
    "    def construct_prompt_with_error_msg(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        response: str,\n",
    "        errors: str,\n",
    "    ) -> str:\n",
    "        \"\"\"Construct prompt message along with the error message.\n",
    "\n",
    "        Args:\n",
    "            prompt: The original prompt string.\n",
    "            response: The invalid response string from OpenAI.\n",
    "            errors: The errors which needs to be fixed in the invalid response.\n",
    "\n",
    "        Returns:\n",
    "            A string combining the original prompt, invalid response, and the error message.\n",
    "        \"\"\"\n",
    "        prompt_with_errors = (\n",
    "            prompt\n",
    "            + f\"\\n\\n==== RESPONSE WITH ISSUES ====\\n\\n{response}\"\n",
    "            + f\"\\n\\nRead the contents of ==== RESPONSE WITH ISSUES ==== section and fix the below mentioned issues:\\n\\n{errors}\"\n",
    "        )\n",
    "        return prompt_with_errors\n",
    "\n",
    "    def fix(\n",
    "        self, prompt: str, total_usage: List[Dict[str, int]], use_prompt_in_validation: bool = False\n",
    "    ) -> Tuple[str, List[Dict[str, int]]]:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a39d512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some prompt\n",
      "\n",
      "==== RESPONSE WITH ISSUES ====\n",
      "\n",
      "some response\n",
      "\n",
      "Read the contents of ==== RESPONSE WITH ISSUES ==== section and fix the below mentioned issues:\n",
      "\n",
      "error 1\n",
      "error 2\n",
      "error 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def fixture_generate(initial_prompt):\n",
    "    return \"some response\"\n",
    "\n",
    "def fixture_validate(response):\n",
    "    return []\n",
    "\n",
    "prompt = \"some prompt\"\n",
    "response = \"some response\"\n",
    "errors = \"\"\"error 1\n",
    "error 2\n",
    "error 3\n",
    "\"\"\"\n",
    "\n",
    "expected = \"\"\"some prompt\n",
    "\n",
    "==== RESPONSE WITH ISSUES ====\n",
    "\n",
    "some response\n",
    "\n",
    "Read the contents of ==== RESPONSE WITH ISSUES ==== section and fix the below mentioned issues:\n",
    "\n",
    "error 1\n",
    "error 2\n",
    "error 3\n",
    "\"\"\"\n",
    "\n",
    "fix_response = ValidateAndFixResponse(fixture_generate, fixture_validate)\n",
    "actual = fix_response.construct_prompt_with_error_msg(prompt, response, errors)\n",
    "print(actual)\n",
    "\n",
    "assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba664129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def add_tokens_usage(usage_list: List[Dict[str, int]]) -> Dict[str, int]:\n",
    "    \"\"\"Add list of OpenAI \"usage\" dictionaries by categories defined in TOKEN_TYPES (prompt_tokens, completion_tokens and total_tokens).\n",
    "\n",
    "    Args:\n",
    "        usage_list: List of OpenAI \"usage\" dictionaries\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, int]: Dictionary where the keys are TOKEN_TYPES and their values are the sum of OpenAI \"usage\" dictionaries\n",
    "    \"\"\"\n",
    "    added_tokens: Dict[str, int] = defaultdict(int)\n",
    "    for usage in usage_list:\n",
    "        for token_type in TOKEN_TYPES:\n",
    "            added_tokens[token_type] += usage[token_type]\n",
    "            \n",
    "    return added_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634c1d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "usage = {\n",
    "    \"prompt_tokens\": 129,\n",
    "    \"completion_tokens\": 1,\n",
    "    \"total_tokens\": 130\n",
    "  }\n",
    "assert add_tokens_usage([usage, usage]) == {\n",
    "    \"prompt_tokens\": 258,\n",
    "    \"completion_tokens\": 2,\n",
    "    \"total_tokens\": 260\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa1de89",
   "metadata": {},
   "outputs": [],
   "source": [
    "usage = {\n",
    "    \"prompt_tokens\": 129,\n",
    "    \"completion_tokens\": 1,\n",
    "    \"total_tokens\": 130\n",
    "  }\n",
    "assert add_tokens_usage([defaultdict(int), usage]) == {\n",
    "    \"prompt_tokens\": 129,\n",
    "    \"completion_tokens\": 1,\n",
    "    \"total_tokens\": 130\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbefa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch  # type: ignore\n",
    "def fix(\n",
    "    self: ValidateAndFixResponse, prompt: str, total_usage: List[Dict[str, int]], use_prompt_in_validation: bool = False\n",
    ") -> Tuple[str, List[Dict[str, int]]]:\n",
    "    \"\"\"Fix the response from OpenAI until no errors remain or maximum number of attempts is reached.\n",
    "\n",
    "    Args:\n",
    "        prompt: The initial prompt string.\n",
    "        use_prompt_in_validation: Flag indicating whether to use the prompt while validating the generated response. This will be useful\n",
    "            while validating the test code. Because the tests will run against the app code.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        str: The generated response that has passed the validation.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the maximum number of attempts is exceeded and the response has not successfully passed the validation.\n",
    "    \"\"\"\n",
    "    iterations = 0\n",
    "    initial_prompt = prompt\n",
    "    total_tokens_usage: Dict[str, int] = defaultdict(int)\n",
    "    try:\n",
    "        while True:\n",
    "            response, usage = self.generate(prompt)\n",
    "            total_tokens_usage = add_tokens_usage([total_tokens_usage, usage])\n",
    "            errors = (\n",
    "                self.validate(response, prompt)\n",
    "                if use_prompt_in_validation\n",
    "                else self.validate(response)\n",
    "            )\n",
    "            if len(errors) == 0:\n",
    "                total_usage.append(total_tokens_usage)\n",
    "                return response, total_usage\n",
    "            error_str = \"\\n\".join(errors)\n",
    "#             logger.info(\n",
    "#                 f\"Validation failed due to the following errors, trying again...\\n{error_str}\\n\\nBelow is the invalid response with the mentioned errors:\\n\\n{response}\\n\\n\"\n",
    "#             )\n",
    "            prompt = self.construct_prompt_with_error_msg(\n",
    "                initial_prompt, response, error_str\n",
    "            )\n",
    "            logger.info(\n",
    "                f\"Validation failed due to the following errors, trying again...\\n{error_str}\\n\\nBelow is the updated prompt message along with the previously generated invalid response:\\n{prompt}\"\n",
    "            )\n",
    "            iterations += 1\n",
    "            if self.max_attempts is not None and iterations >= self.max_attempts:\n",
    "                raise ValueError(\n",
    "                    f\"Maximum number of retries ({self.max_attempts}) exceeded. Unable to fix the following issues. Please try again...\\n{error_str}\\n\\n\"\n",
    "                )\n",
    "    except:\n",
    "        total_usage.append(total_tokens_usage)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74e159a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some Valid response\n"
     ]
    }
   ],
   "source": [
    "fixture_initial_prompt = \"some valid prompt\"\n",
    "expected = \"Some Valid response\"\n",
    "\n",
    "def fixture_generate(initial_prompt):\n",
    "    usage = {\n",
    "        \"prompt_tokens\": 129,\n",
    "        \"completion_tokens\": 1,\n",
    "        \"total_tokens\": 130\n",
    "    }\n",
    "    return expected, usage\n",
    "\n",
    "def fixture_validate(response):\n",
    "    return []\n",
    "\n",
    "v = ValidateAndFixResponse(fixture_generate, fixture_validate)\n",
    "actual, tokens = v.fix(fixture_initial_prompt, [])\n",
    "print(actual)\n",
    "assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbaf85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: Validation failed due to the following errors, trying again...\n",
      "error 1\n",
      "error 2\n",
      "\n",
      "Below is the invalid response with the mentioned errors:\n",
      "\n",
      "some invalid response\n",
      "\n",
      "\n",
      "[INFO] __main__: Validation failed due to the following errors, trying again...\n",
      "error 1\n",
      "error 2\n",
      "\n",
      "Below is the invalid response with the mentioned errors:\n",
      "\n",
      "some invalid response\n",
      "\n",
      "\n",
      "Maximum number of retries (2) exceeded. Unable to fix the following issues. Please try again...\n",
      "error 1\n",
      "error 2\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fixture_initial_prompt = \"some invalid prompt\"\n",
    "max_attempts = 2\n",
    "\n",
    "def fixture_generate(initial_prompt):\n",
    "    usage = {\n",
    "        \"prompt_tokens\": 129,\n",
    "        \"completion_tokens\": 1,\n",
    "        \"total_tokens\": 130\n",
    "    }\n",
    "    return \"some invalid response\", usage\n",
    "\n",
    "def fixture_validate(response):\n",
    "    return [\"error 1\", \"error 2\"]\n",
    "\n",
    "expected = \"\"\"Maximum number of retries (2) exceeded. Unable to fix the following issues. Please try again...\n",
    "error 1\n",
    "error 2\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "with pytest.raises(ValueError) as e:\n",
    "    v = ValidateAndFixResponse(fixture_generate, fixture_validate, max_attempts)\n",
    "    actual = v.fix(fixture_initial_prompt, [])\n",
    "print(e.value)\n",
    "assert str(e.value) == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b994a2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81eb352",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f9cb69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
