{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8991cc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp _testing.benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d22434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from typing import *\n",
    "import shutil\n",
    "from tempfile import TemporaryDirectory\n",
    "from pathlib import Path\n",
    "from contextlib import contextmanager\n",
    "import os\n",
    "import subprocess # nosec: B404: Consider possible security implications associated with the subprocess module.\n",
    "\n",
    "import typer\n",
    "from yaspin import yaspin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a965ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typer.testing import CliRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30f9571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "@contextmanager\n",
    "def _set_cwd(cwd_path: Union[Path, str]) -> Generator:\n",
    "    \"\"\"Set the current working directory for the duration of the context manager.\n",
    "\n",
    "    Args:\n",
    "        cwd_path: The path to the new working directory.\n",
    "\n",
    "    !!! note\n",
    "\n",
    "        The above docstring is autogenerated by docstring-gen library (https://github.com/airtai/docstring-gen)\n",
    "    \"\"\"\n",
    "    cwd_path = Path(cwd_path)\n",
    "    original_cwd = os.getcwd()\n",
    "    os.chdir(cwd_path)\n",
    "\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        os.chdir(original_cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bb917e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # | export\n",
    "\n",
    "# def get_fixtures_path() -> Path:\n",
    "#     return Path(__file__).parent.parent.parent / \"fixtures\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79915c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "app = typer.Typer(\n",
    "    short_help=\"Run benchmark against pre-defined example app descriptions\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357a13dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@app.command(\n",
    "    \"run_benchmark\",\n",
    "    help=\"Run benchmark against pre-defined example app descriptions\",\n",
    ")\n",
    "def benchmark(\n",
    "    fixtures_path: str = typer.Argument(\n",
    "        ...,\n",
    "        help=\"The path to the pre-defined example app descriptions\",\n",
    "    )\n",
    ") -> None:\n",
    "    fixtures_path_obj = Path(fixtures_path).resolve()\n",
    "\n",
    "    app_descriptions = [\n",
    "        filename\n",
    "        for filename in fixtures_path_obj.glob(\"*.txt\")\n",
    "        if \"-log\" not in filename.stem\n",
    "    ]\n",
    "    no_of_description_files = len(app_descriptions)\n",
    "    typer.secho(\n",
    "        f\"Total app description files: {no_of_description_files}\", fg=typer.colors.CYAN\n",
    "    )\n",
    "    success_cnt = 0\n",
    "    for i, app_description in enumerate(app_descriptions):\n",
    "        with yaspin(\n",
    "            text=f\"{i+1}/{no_of_description_files} Generating app for: {app_description.name}\",\n",
    "            color=\"cyan\",\n",
    "            spinner=\"clock\",\n",
    "        ) as sp:\n",
    "            with TemporaryDirectory() as d:\n",
    "                with _set_cwd(d):\n",
    "                    cli_command = (\n",
    "                        f\"faststream_gen -i {app_description} -o {d}/new-project -v\"\n",
    "                    )\n",
    "                    try:\n",
    "                        # nosemgrep: python.lang.security.audit.subprocess-shell-true.subprocess-shell-true\n",
    "                        result = subprocess.run( # nosec: B602, B603 subprocess call - check for execution of untrusted input.\n",
    "                            cli_command,\n",
    "                            shell=True,\n",
    "                            check=True,\n",
    "                            stdout=subprocess.PIPE,\n",
    "                            stderr=subprocess.PIPE,\n",
    "                            text=True,\n",
    "                        )\n",
    "                        sp.text = \"\"\n",
    "                        sp.ok(\n",
    "                            f\" ✔ App successfully generated for: {app_description.name}\"\n",
    "                        )\n",
    "                        success_cnt +=1\n",
    "                    except Exception as e:\n",
    "                        sp.text = \"\"\n",
    "                        sp.color = \"red\"\n",
    "                        sp.ok(\n",
    "                            f\" ✘ Error: App generated failed for: {app_description.name}\"\n",
    "                        )\n",
    "                    finally:\n",
    "                        shutil.copy(\n",
    "                            Path(d) / \"faststream-log.txt\",\n",
    "                            fixtures_path_obj / f\"{Path(app_description).stem}-log.txt\",\n",
    "                        )\n",
    "    typer.secho(f\"Success rate: {(success_cnt/no_of_description_files) * 100} %\", fg=typer.colors.CYAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49afd8f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">                                                                                                                   </span>\n",
       "<span style=\"font-weight: bold\"> </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Usage: </span><span style=\"font-weight: bold\">run_benchmark [OPTIONS] FIXTURES_PATH                                                                      </span>\n",
       "<span style=\"font-weight: bold\">                                                                                                                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m                                                                                                                   \u001b[0m\n",
       "\u001b[1m \u001b[0m\u001b[1;33mUsage: \u001b[0m\u001b[1mrun_benchmark [OPTIONS] FIXTURES_PATH\u001b[0m\u001b[1m                                                                     \u001b[0m\u001b[1m \u001b[0m\n",
       "\u001b[1m                                                                                                                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> Run benchmark against pre-defined example app descriptions                                                        \n",
       "                                                                                                                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       " Run benchmark against pre-defined example app descriptions                                                        \n",
       "                                                                                                                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">╭─ Arguments ─────────────────────────────────────────────────────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">*</span>    fixtures_path      <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">TEXT</span>  The path to the pre-defined example app descriptions <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[default: None]</span> <span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f\">[required]</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>\n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m╭─\u001b[0m\u001b[2m Arguments \u001b[0m\u001b[2m────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[2m─╮\u001b[0m\n",
       "\u001b[2m│\u001b[0m \u001b[31m*\u001b[0m    fixtures_path      \u001b[1;33mTEXT\u001b[0m  The path to the pre-defined example app descriptions \u001b[2m[default: None]\u001b[0m \u001b[2;31m[required]\u001b[0m   \u001b[2m│\u001b[0m\n",
       "\u001b[2m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">╭─ Options ───────────────────────────────────────────────────────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">--install-completion</span>          Install completion for the current shell.                                         <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>\n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">--show-completion</span>             Show completion for the current shell, to copy it or customize the installation.  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>\n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">--help</span>                        Show this message and exit.                                                       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>\n",
       "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m╭─\u001b[0m\u001b[2m Options \u001b[0m\u001b[2m──────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[2m─╮\u001b[0m\n",
       "\u001b[2m│\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-install\u001b[0m\u001b[1;36m-completion\u001b[0m          Install completion for the current shell.                                         \u001b[2m│\u001b[0m\n",
       "\u001b[2m│\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-show\u001b[0m\u001b[1;36m-completion\u001b[0m             Show completion for the current shell, to copy it or customize the installation.  \u001b[2m│\u001b[0m\n",
       "\u001b[2m│\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-help\u001b[0m                        Show this message and exit.                                                       \u001b[2m│\u001b[0m\n",
       "\u001b[2m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "runner = CliRunner()\n",
    "result = runner.invoke(app, [\"benchmark\", \"--help\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb09d959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('/tmp/tmp_da15059/hello_world.txt'), PosixPath('/tmp/tmp_da15059/hello_world-log.txt')]\n",
      "23-09-21 23:57:35.628 [INFO] faststream_gen.cli: Project generation started.\n",
      "23-09-21 23:57:35.628 [INFO] faststream_gen.cli: Reading application description from '/tmp/tmp_da15059/hello_world.txt'.\n",
      "23-09-21 23:57:35.628 [INFO] faststream_gen._code_generator.app_description_validator: ==== App description validation ====\n",
      "23-09-21 23:57:35.630 [INFO] faiss.loader: Loading faiss with AVX2 support.\n",
      "23-09-21 23:57:35.641 [INFO] faiss.loader: Successfully loaded faiss with AVX2 support.\n",
      "23-09-21 23:57:36.240 [INFO] faststream_gen._code_generator.helper: ************************************************************************************************************************\n",
      "23-09-21 23:57:36.241 [INFO] faststream_gen._code_generator.helper: \n",
      "\n",
      "Prompt to the model: \n",
      "\n",
      "===Role:system===\n",
      "\n",
      "Message:\n",
      "\n",
      "You are an expert Python developer, tasked to generate executable Python code as a part of your work with the FastStream framework. \n",
      "\n",
      "You are to abide by the following guidelines:\n",
      "\n",
      "1. You must never enclose the generated Python code with ``` python. It is mandatory that the output is a valid and executable Python code. Please ensure this rule is never broken.\n",
      "\n",
      "2. Some prompts might require you to generate code that contains async functions. For example:\n",
      "\n",
      "async def app_setup(context: ContextRepo):\n",
      "    raise NotImplementedError()\n",
      "\n",
      "In such cases, it is necessary to add the \"import asyncio\" statement at the top of the code. \n",
      "\n",
      "You will encounter sections marked as:\n",
      "\n",
      "==== APP DESCRIPTION: ====\n",
      "\n",
      "These sections contain the description of the FastStream app you need to implement. Treat everything below this line, until the end of the prompt, as the description to follow for the app implementation.\n",
      "\n",
      "\n",
      "===Role:user===\n",
      "\n",
      "Message:\n",
      "hide:\n",
      "  - navigation\n",
      "  - footer\n",
      "\n",
      "FastStream\n",
      "\n",
      "Effortless event stream integration for your services\n",
      "\n",
      "Features\n",
      "\n",
      "FastStream simplifies the process of writing producers and consumers for message queues, handling all the\n",
      "parsing, networking and documentatino generation automatically.\n",
      "\n",
      "Making streaming microservices has never been easier. Designed with junior developers in mind, FastStream simplifies your work while keeping the door open for more advanced use-cases. Here's a look at the core features that make FastStream a go-to framework for modern, data-centric microservices.\n",
      "\n",
      "Multiple Brokers: FastStream provides a unified API to work across multiple message brokers (Kafka, RabbitMQ support)\n",
      "\n",
      "Pydantic Validation: Leverage Pydantic's{.external-link target=\"_blank\"} validation capabilities to serialize and validates incoming messages\n",
      "\n",
      "Automatic Docs: Stay ahead with automatic AsyncAPI{.external-link target=\"_blank\"} documentation.\n",
      "\n",
      "Intuitive: full typed editor support makes your development experience smooth, catching errors before they reach runtime\n",
      "\n",
      "Powerful Dependency Injection System: Manage your service dependencies efficiently with FastStream's built-in DI system.\n",
      "\n",
      "Testable: supports in-memory tests, making your CI/CD pipeline faster and more reliable\n",
      "\n",
      "Extendable: use extensions for lifespans, custom serialization and middlewares\n",
      "\n",
      "Integrations: FastStream is fully compatible with any HTTP framework you want (FastAPI especially)\n",
      "\n",
      "Built for Automatic Code Generation: FastStream is optimized for automatic code generation using advanced models like GPT and Llama\n",
      "\n",
      "That's FastStream in a nutshell—easy, efficient, and powerful. Whether you're just starting with streaming microservices or looking to scale, FastStream has got you covered.\n",
      "\n",
      "History\n",
      "\n",
      "FastStream is a new package based on the ideas and experiences gained from FastKafka{.external-link target=\"_blank\"} and Propan{.external-link target=\"_blank\"}. By joining our forces, we picked up the best from both packages and created the unified way to write services capable of processing streamed data regradless of the underliying protocol. We'll continue to maintain both packages, but new development will be in this project. If you are starting a new service, this package is the recommended way to do it.\n",
      "\n",
      "Install\n",
      "\n",
      "=== \"Kafka\"\n",
      "    sh\n",
      "    pip install faststream[kafka]\n",
      "\n",
      "=== \"RabbitMQ\"\n",
      "    sh\n",
      "    pip install faststream[rabbit]\n",
      "\n",
      "Writing app code\n",
      "\n",
      "FastStream brokers provide convenient function decorators #!python @broker.subscriber\n",
      "and #!python @broker.publisher to allow you to delegate the actual process of\n",
      "\n",
      "consuming and producing data to Event queues, and\n",
      "\n",
      "decoding and encoding JSON encoded messages\n",
      "\n",
      "These decorators make it easy to specify the processing logic for your consumers and producers, allowing you to focus on the core business logic of your application without worrying about the underlying integration.\n",
      "\n",
      "Also, FastStream uses Pydantic{.external-link target=\"_blank\"} to parse input\n",
      "JSON-encoded data into Python objects, making it easy to work with structured data in your applications, so you can serialize you input messages just using type annotations.\n",
      "\n",
      "Here is an example python app using FastStream that consumes data from an incoming data stream and outputs the data to another one.\n",
      "\n",
      "=== \"Kafka\"\n",
      "    ```python linenums=\"1\" hl_lines=\"9\"\n",
      "from faststream import FastStream\n",
      "from faststream.kafka import KafkaBroker\n",
      "\n",
      "broker = KafkaBroker(\"localhost:9092\")\n",
      "app = FastStream(broker)\n",
      "\n",
      "@broker.subscriber(\"in-topic\")\n",
      "@broker.publisher(\"out-topic\")\n",
      "async def handle_msg(user: str, user_id: int) -> str:\n",
      "    return f\"User: {user_id} - {user} registered\"\n",
      "    ```\n",
      "\n",
      "=== \"RabbitMQ\"\n",
      "    ```python linenums=\"1\" hl_lines=\"9\"\n",
      "from faststream import FastStream\n",
      "from faststream.rabbit import RabbitBroker\n",
      "\n",
      "broker = RabbitBroker(\"amqp://guest:guest@localhost:5672/\")\n",
      "app = FastStream(broker)\n",
      "\n",
      "@broker.subscriber(\"in-queue\")\n",
      "@broker.publisher(\"out-queue\")\n",
      "async def handle_msg(user: str, user_id: int) -> str:\n",
      "    return f\"User: {user_id} - {user} registered\"\n",
      "    ```\n",
      "\n",
      "Also, Pydantic’s BaseModel{.external-link target=\"_blank\"} class allows you\n",
      "to define messages using a declarative syntax, making it easy to specify the fields and types of your messages.\n",
      "\n",
      "=== \"Kafka\"\n",
      "    ```python linenums=\"1\" hl_lines=\"1 8 14\"\n",
      "from pydantic import BaseModel, Field, PositiveInt\n",
      "from faststream import FastStream\n",
      "from faststream.kafka import KafkaBroker\n",
      "\n",
      "broker = KafkaBroker(\"localhost:9092\")\n",
      "app = FastStream(broker)\n",
      "\n",
      "class User(BaseModel):\n",
      "    user: str = Field(..., examples=[\"John\"])\n",
      "    user_id: PositiveInt = Field(..., examples=[\"1\"])\n",
      "\n",
      "@broker.subscriber(\"in-topic\")\n",
      "@broker.publisher(\"out-topic\")\n",
      "async def handle_msg(data: User) -> str:\n",
      "    return f\"User: {data.user} - {data.user_id} registered\"\n",
      "    ```\n",
      "\n",
      "=== \"RabbitMQ\"\n",
      "    ```python linenums=\"1\" hl_lines=\"1 8 14\"\n",
      "from pydantic import BaseModel, Field, PositiveInt\n",
      "from faststream import FastStream\n",
      "from faststream.rabbit import RabbitBroker\n",
      "\n",
      "broker = RabbitBroker(\"amqp://guest:guest@localhost:5672/\")\n",
      "app = FastStream(broker)\n",
      "\n",
      "class User(BaseModel):\n",
      "    user: str = Field(..., examples=[\"John\"])\n",
      "    user_id: PositiveInt = Field(..., examples=[\"1\"])\n",
      "\n",
      "@broker.subscriber(\"in-queue\")\n",
      "@broker.publisher(\"out-queue\")\n",
      "async def handle_msg(data: User) -> str:\n",
      "    return f\"User: {data.user} - {data.user_id} registered\"\n",
      "    ```\n",
      "\n",
      "Testing the service\n",
      "\n",
      "The service can be tested using the TestBroker context managers which, by default, puts the Broker into \"testing mode\".\n",
      "\n",
      "The Tester will redirect your subscriber and publisher decorated functions to the InMemory brokers so that you can quickly test your app without the need for a running broker and all its dependencies.\n",
      "\n",
      "Using pytest, the test for our service would look like this:\n",
      "\n",
      "=== \"Kafka\"\n",
      "    ```python linenums=\"1\" hl_lines=\"3 10 18-19\"\n",
      "    # Code above omitted 👆\n",
      "\n",
      "import pytest\n",
      "import pydantic\n",
      "from faststream.kafka import TestKafkaBroker\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_correct():\n",
      "    async with TestKafkaBroker(broker) as br:\n",
      "        await br.publish({\n",
      "            \"user\": \"John\",\n",
      "            \"user_id\": 1,\n",
      "        }, \"in-topic\")\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_invalid():\n",
      "    async with TestKafkaBroker(broker) as br:\n",
      "        with pytest.raises(pydantic.ValidationError):\n",
      "            await br.publish(\"wrong message\", \"in-topic\")\n",
      "    ```\n",
      "\n",
      "=== \"RabbitMQ\"\n",
      "    ```python linenums=\"1\" hl_lines=\"3 10 18-19\"\n",
      "    # Code above omitted 👆\n",
      "\n",
      "import pytest\n",
      "import pydantic\n",
      "from faststream.rabbit import TestRabbitBroker\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_correct():\n",
      "    async with TestRabbitBroker(broker) as br:\n",
      "        await br.publish({\n",
      "            \"user\": \"John\",\n",
      "            \"user_id\": 1,\n",
      "        }, \"in-queue\")\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_invalid():\n",
      "    async with TestRabbitBroker(broker) as br:\n",
      "        with pytest.raises(pydantic.ValidationError):\n",
      "            await br.publish(\"wrong message\", \"in-queue\")\n",
      "    ```\n",
      "\n",
      "Running the application\n",
      "\n",
      "The application can be started using builtin FastStream CLI command.\n",
      "\n",
      "To run the service, use the FastStream CLI command and pass the module (in this case, the file where the app implementation is located) and the app simbol to the command.\n",
      "\n",
      "shell\n",
      "faststream run basic:app\n",
      "\n",
      "After running the command you should see the following output:\n",
      "\n",
      "shell\n",
      "INFO     - FastStream app starting...\n",
      "INFO     - input_data |            - `HandleMsg` waiting for messages\n",
      "INFO     - FastStream app started successfully! To exit press CTRL+C\n",
      "\n",
      "Also, FastStream provides you a great hotreload feature to improve your Development Experience\n",
      "\n",
      "shell\n",
      "faststream run basic:app --reload\n",
      "\n",
      "And multiprocessing horizontal scaling feature as well\n",
      "\n",
      "shell\n",
      "faststream run basic:app --workers 3\n",
      "\n",
      "You can know more about CLI features here{.internal-link}\n",
      "\n",
      "Project Documentation\n",
      "\n",
      "FastStream automatically generates documentation for your project according to the AsyncAPI{.external-link target=\"_blank\"} specification. You can work with both generated artifacts and place a Web view of your documentation on resources available to related teams.\n",
      "\n",
      "The availability of such documentation significantly simplifies the integration of services: you can immediately see what channels and message format the application works with. And most importantly, it won't cost anything - FastStream has already created the docs for you!\n",
      "\n",
      "Dependencies\n",
      "\n",
      "FastStream (thanks to FastDepend{.external-link target=\"_blank\"}) has a dependency management system close to pytest fixtures and FastAPI Depends at the same time. Function arguments declare which dependencies you want are needed, and a special decorator delivers them from the global Context object.\n",
      "\n",
      "```python linenums=\"1\" hl_lines=\"9-10\"\n",
      "from faststream import Depends, Logger\n",
      "async def base_dep(user_id: int) -> bool:\n",
      "    return True\n",
      "\n",
      "@broker.subscriber(\"in-test\")\n",
      "async def base_handler(user: str,\n",
      "                       logger: Logger,\n",
      "                       dep: bool = Depends(base_dep)):\n",
      "    assert dep is True\n",
      "    logger.info(user)\n",
      "```\n",
      "\n",
      "HTTP Frameworks integrations\n",
      "\n",
      "Any Framework\n",
      "\n",
      "You can use FastStream MQBrokers without FastStream application.\n",
      "Just start and stop them according to your application lifespan.\n",
      "\n",
      "{! includes/index/integrations.md !}\n",
      "\n",
      "FastAPI Plugin\n",
      "\n",
      "Also, FastStream can be used as part of FastAPI.\n",
      "\n",
      "Just import a StreamRouter you need and declare message handler with the same #!python @router.subscriber(...) and #!python @router.publisher(...) decorators.\n",
      "\n",
      "!!! tip\n",
      "    When used this way, FastStream does not utilize its own dependency and serialization system, but integrates into FastAPI.\n",
      "    That is, you can use Depends, BackgroundTasks and other FastAPI tools as if it were a regular HTTP endpoint.\n",
      "\n",
      "{! includes/getting_started/integrations/fastapi/1.md !}\n",
      "\n",
      "!!! note\n",
      "    More integration features can be found here{.internal-link}\n",
      "\n",
      "Stay in touch\n",
      "\n",
      "Please show your support and stay in touch by:\n",
      "\n",
      "giving our GitHub repository a star, and\n",
      "\n",
      "joining our Discord server\n",
      "\n",
      "Your support helps us to stay in touch with you and encourages us to\n",
      "continue developing and improving the library. Thank you for your\n",
      "support!\n",
      "\n",
      "Contributors\n",
      "\n",
      "Thanks for all of these amazing peoples made the project better!\n",
      "\n",
      "===Role:user===\n",
      "\n",
      "Message:\n",
      "\n",
      "You should provide a response of 0, 1, 2, or 3, and nothing else, based on the following rules:\n",
      "\n",
      "==== RULES: ====\n",
      "\n",
      "If the ==== APP DESCRIPTION: ==== section is not related to FastStream or contains violence, self-harm, harassment/threatening, or hate/threatening information, respond with 0.\n",
      "\n",
      "If the ==== APP DESCRIPTION: ==== section is related to FastStream but primarily provides general information about FastStream and what it is, respond with 1.\n",
      "\n",
      "If it is NOT possible to infer the topic name or there is no explanation about the business logic in the ==== APP DESCRIPTION: ==== section, respond with 2. This is crucial.\n",
      "\n",
      "If the ==== APP DESCRIPTION: ==== section is related to FastStream, provides instructions on which topic the messages should be consumed/produced, and includes at least one defined topic, respond with 3.\n",
      "\n",
      "Here are few examples for your understanding:\n",
      "\n",
      "\n",
      "==== EXAMPLE APP DESCRIPTION ====\n",
      "Generate a new FastStream app, which has a producer function and a consumer function \n",
      "==== YOUR RESPONSE ====\n",
      "2\n",
      "\n",
      "==== EXAMPLE APP DESCRIPTION ====\n",
      "In App description 1, the user has not defined the message structure or the topic name to publish/subscribe. As a result, you should respond with 2. \n",
      "==== YOUR RESPONSE ====\n",
      "2\n",
      "\n",
      "==== EXAMPLE APP DESCRIPTION ====\n",
      "Create a FastStream application.\n",
      "==== YOUR RESPONSE ====\n",
      "2\n",
      "\n",
      "==== EXAMPLE APP DESCRIPTION ====\n",
      "create FastStream app where message has user_data attribute.\n",
      "==== YOUR RESPONSE ====\n",
      "2\n",
      "\n",
      "==== EXAMPLE APP DESCRIPTION ====\n",
      "FastStream app with for consuming messages from the hello topic\n",
      "==== YOUR RESPONSE ====\n",
      "3\n",
      "\n",
      "==== EXAMPLE APP DESCRIPTION ====\n",
      "Write a FastStream application with with one consumer function and two producer functions. The consumer function should receive the a message posted on \"new_joinee\" topic. The message should contain \"employee_name\", \"age\", \"location\" and \"experience\" attributes. After consuming the consumer function should send the details to the \"project_team\" and \"admin_team\" topics. Use only localhost broker==== Response 5 ====\n",
      "==== YOUR RESPONSE ====\n",
      "3\n",
      "\n",
      "==== EXAMPLE APP DESCRIPTION ====\n",
      "Develop a new FastStream application that consumes JSON-encoded objects from the \"receive_order\" topic. These objects include attributes like \"name\" and \"quantity.\" Upon consumption, enhance the message by adding a \"location\" attribute set to \"Zagreb.\" Subsequently, forward the modified message to the \"place_order\" topic. After this, send another message to the \"update_inventory\" topic. This message should include a \"quantity\" attribute that corresponds to the received quantity value. No authentication is required.==== Response 6 ====\n",
      "==== YOUR RESPONSE ====\n",
      "3\n",
      "\n",
      "==== EXAMPLE APP DESCRIPTION ====\n",
      "Who are you\n",
      "==== YOUR RESPONSE ====\n",
      "0\n",
      "\n",
      "==== EXAMPLE APP DESCRIPTION ====\n",
      "What is the latest vesion of FastStream\n",
      "==== YOUR RESPONSE ====\n",
      "1\n",
      "\n",
      "Please respond only with numbers 0, 1, 2 or 3 (WITH NO ADDITIONAL TEXT!)\n",
      "\n",
      "==== APP DESCRIPTION: ====\n",
      "\n",
      "\n",
      "\n",
      "===Role:user===\n",
      "\n",
      "Message:\n",
      "Create a FastStream application using localhost broker for testing and use the default port number. It should consume messages from the \"input_data\" topic, where each message is a JSON encoded object containing a single attribute: 'data'. For each consumed message, create a new message object and increment the value of the data attribute by 1. Finally, send the modified message to the 'output_data' topic.\n",
      "==== YOUR RESPONSE ====\n",
      "\n",
      "23-09-21 23:57:36.241 [INFO] faststream_gen._code_generator.helper: ************************************************************************************************************************\n",
      "23-09-21 23:57:36.931 [INFO] faststream_gen._code_generator.app_skeleton_generator: ==== Description to Skeleton Generation ====\n",
      "23-09-21 23:57:36.932 [INFO] faststream_gen._code_generator.helper: ************************************************************************************************************************\n",
      "23-09-21 23:57:36.932 [INFO] faststream_gen._code_generator.helper: \n",
      "\n",
      "Prompt to the model: \n",
      "\n",
      "===Role:system===\n",
      "\n",
      "Message:\n",
      "\n",
      "You are an expert Python developer, tasked to generate executable Python code as a part of your work with the FastStream framework. \n",
      "\n",
      "You are to abide by the following guidelines:\n",
      "\n",
      "1. You must never enclose the generated Python code with ``` python. It is mandatory that the output is a valid and executable Python code. Please ensure this rule is never broken.\n",
      "\n",
      "2. Some prompts might require you to generate code that contains async functions. For example:\n",
      "\n",
      "async def app_setup(context: ContextRepo):\n",
      "    raise NotImplementedError()\n",
      "\n",
      "In such cases, it is necessary to add the \"import asyncio\" statement at the top of the code. \n",
      "\n",
      "You will encounter sections marked as:\n",
      "\n",
      "==== APP DESCRIPTION: ====\n",
      "\n",
      "These sections contain the description of the FastStream app you need to implement. Treat everything below this line, until the end of the prompt, as the description to follow for the app implementation.\n",
      "\n",
      "\n",
      "===Role:user===\n",
      "\n",
      "Message:\n",
      "\n",
      "\n",
      "===Role:user===\n",
      "\n",
      "Message:\n",
      "\n",
      "\n",
      "\n",
      "Generate skeleton code for FastStream applications based on provided app descriptions in the \"==== USER APP DESCRIPTION ====\" section, adhering to these guidelines:\n",
      "\n",
      "    - Avoid implementing business logic of ANY function. Instead, write \"raise NotImplementedError()\" and create Google-style docstrings to describe their intended functionality when handling received or produced messages. In each docstring, include a clear instruction to \"log the consumed message using logger.info\" for subscriber functions.\n",
      "\n",
      "    - Ensure the generated code aligns with the specific app description requirements.\n",
      "\n",
      "    - Provide a clear and organized starting point for developers.\n",
      "\n",
      "    - Your response must contain only valid Python code, saveable as a .py script; no additional text is allowed.\n",
      "    \n",
      "    - DO NOT enclose the response within back-ticks. Meaning NEVER ADD ```python to your response.\n",
      "\n",
      "\n",
      "The goal is to offer developers a structured starting point that matches the example app descriptions, aiding in FastStream application development. Follow the example patterns provided for reference.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==== EXAMPLE APP DESCRIPTION ====\n",
      "\n",
      "Develop a FastStream application using localhost kafka broker.\n",
      "The app should consume messages from the input_data topic.\n",
      "The input message is a JSON encoded object including two attributes:\n",
      "    - x: float\n",
      "    - y: float\n",
      "\n",
      "While consuming the message, increment x and y attributes by 1 and publish that message to the output_data topic.\n",
      "Use messages attribute x as a partition key when publishing to output_data topic.\n",
      "\n",
      "\n",
      "\n",
      "==== YOUR RESPONSE ====\n",
      "\n",
      "\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "from faststream import FastStream, Logger\n",
      "from faststream.kafka import KafkaBroker\n",
      "\n",
      "\n",
      "class Point(BaseModel):\n",
      "    x: float = Field(\n",
      "        ..., examples=[0.5], description=\"The X Coordinate in the coordinate system\"\n",
      "    )\n",
      "    y: float = Field(\n",
      "        ..., examples=[0.5], description=\"The Y Coordinate in the coordinate system\"\n",
      "    )\n",
      "\n",
      "\n",
      "broker = KafkaBroker(\"localhost:9092\")\n",
      "app = FastStream(broker)\n",
      "\n",
      "\n",
      "to_output_data = broker.publisher(\"output_data\")\n",
      "\n",
      "\n",
      "@broker.subscriber(\"input_data\")\n",
      "async def on_input_data(msg: Point, logger: Logger) -> None:\n",
      "    \"\"\"\n",
      "    Processes a message from the 'input_data' topic.\n",
      "    Increment msg x and y attributes with 1 and publish that message to the output_data topic.\n",
      "    Publish that message to the output_data topic\n",
      "    Use messages attribute x as a partition key when publishing to output_data topic.\n",
      "\n",
      "    Instructions:\n",
      "    1. Consume a message from 'input_data' topic.\n",
      "    2. Create a new message object (do not directly modify the original).\n",
      "    3. Increment msg x and y attributes with 1.\n",
      "    4. Publish that message to the output_data topic (Use messages attribute x as a partition key).\n",
      "    \"\"\"\n",
      "    raise NotImplementedError()\n",
      "\n",
      "\n",
      "==== EXAMPLE APP DESCRIPTION ====\n",
      "\n",
      "Develop a FastStream application using localhost kafka broker.\n",
      "The app should consume messages from the input_data topic.\n",
      "The input message is a JSON encoded object including two attributes:\n",
      "    - x: float\n",
      "    - y: float\n",
      "\n",
      "input_data topic should use partition key.\n",
      "While consuming the message, increment x and y attributes by 1 and publish that message to the output_data topic.\n",
      "The same partition key should be used in the input_data and output_data topic.\n",
      "\n",
      "\n",
      "\n",
      "==== YOUR RESPONSE ====\n",
      "\n",
      "\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "from faststream import Context, FastStream, Logger\n",
      "from faststream.kafka import KafkaBroker\n",
      "\n",
      "\n",
      "class Point(BaseModel):\n",
      "    x: float = Field(\n",
      "        ..., examples=[0.5], description=\"The X Coordinate in the coordinate system\"\n",
      "    )\n",
      "    y: float = Field(\n",
      "        ..., examples=[0.5], description=\"The Y Coordinate in the coordinate system\"\n",
      "    )\n",
      "\n",
      "\n",
      "broker = KafkaBroker(\"localhost:9092\")\n",
      "app = FastStream(broker)\n",
      "\n",
      "\n",
      "to_output_data = broker.publisher(\"output_data\")\n",
      "\n",
      "\n",
      "@broker.subscriber(\"input_data\")\n",
      "async def on_input_data(\n",
      "    msg: Point, logger: Logger, key: bytes = Context(\"message.raw_message.key\")\n",
      ") -> None:\n",
      "    \"\"\"\n",
      "    Processes a message from the 'input_data' topic.\n",
      "    Increment msg x and y attributes with 1 and publish that message to the output_data topic.\n",
      "    The same partition key should be used in the input_data and output_data topic.\n",
      "\n",
      "    Instructions:\n",
      "    1. Consume a message from 'input_data' topic.\n",
      "    2. Create a new message object (do not directly modify the original).\n",
      "    3. Increment msg x and y attributes with 1.\n",
      "    4. Publish that message to the output_data topic (The same partition key should be used in the input_data and output_data topic).\n",
      "    \"\"\"\n",
      "    raise NotImplementedError()\n",
      "\n",
      "\n",
      "==== EXAMPLE APP DESCRIPTION ====\n",
      "\n",
      "Develop a FastStream application using localhost kafka broker.\n",
      "The app should consume messages from the input_data topic.\n",
      "The input message is a JSON encoded object including two attributes:\n",
      "    - x: float\n",
      "    - y: float\n",
      "\n",
      "input_data topic should use partition key.\n",
      "\n",
      "Keep all the previous messages in the memory.\n",
      "While consuming the message, add all x elements from the memory (x_sum) and all y from the memory (y_sum) and publish the message with x_sum and y_sum to the output_data topic.\n",
      "The same partition key should be used in the input_data and output_data topic.\n",
      "\n",
      "\n",
      "\n",
      "==== YOUR RESPONSE ====\n",
      "\n",
      "\n",
      "from typing import List\n",
      "\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "from faststream import Context, ContextRepo, FastStream, Logger\n",
      "from faststream.kafka import KafkaBroker\n",
      "\n",
      "\n",
      "class Point(BaseModel):\n",
      "    x: float = Field(\n",
      "        ..., examples=[0.5], description=\"The X Coordinate in the coordinate system\"\n",
      "    )\n",
      "    y: float = Field(\n",
      "        ..., examples=[0.5], description=\"The Y Coordinate in the coordinate system\"\n",
      "    )\n",
      "\n",
      "\n",
      "broker = KafkaBroker(\"localhost:9092\")\n",
      "app = FastStream(broker)\n",
      "\n",
      "\n",
      "to_output_data = broker.publisher(\"output_data\")\n",
      "\n",
      "\n",
      "@app.on_startup\n",
      "async def app_setup(context: ContextRepo):\n",
      "    \"\"\"\n",
      "    Set all necessary global variables inside ContextRepo object:\n",
      "        Set message_history for storing all input messages\n",
      "    \"\"\"\n",
      "    raise NotImplementedError()\n",
      "\n",
      "\n",
      "@broker.subscriber(\"input_data\")\n",
      "async def on_input_data(\n",
      "    msg: Point,\n",
      "    logger: Logger,\n",
      "    message_history: List[Point] = Context(),\n",
      "    key: bytes = Context(\"message.raw_message.key\"),\n",
      ") -> None:\n",
      "    \"\"\"\n",
      "    Processes a message from the 'input_data' topic.\n",
      "    Add all x elements from the memory (x_sum) and all y from the memory (y_sum) and publish the message with x_sum and y_sum to the output_data topic.\n",
      "    The same partition key should be used in the input_data and output_data topic.\n",
      "\n",
      "    Instructions:\n",
      "    1. Consume a message from 'input_data' topic.\n",
      "    2. Create a new message object (do not directly modify the original).\n",
      "    3. Add all x elements from the memory (x_sum) and all y from the memory (y_sum)\n",
      "    4. Publish the message with x_sum and y_sum to the output_data topic. (The same partition key should be used in the input_data and output_data topic).\n",
      "    \"\"\"\n",
      "    raise NotImplementedError()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==== USER APP DESCRIPTION: ====\n",
      "\n",
      "\n",
      "\n",
      "===Role:user===\n",
      "\n",
      "Message:\n",
      "Create a FastStream application using localhost broker for testing and use the default port number. It should consume messages from the \"input_data\" topic, where each message is a JSON encoded object containing a single attribute: 'data'. For each consumed message, create a new message object and increment the value of the data attribute by 1. Finally, send the modified message to the 'output_data' topic.\n",
      "==== YOUR RESPONSE ====\n",
      "\n",
      "23-09-21 23:57:36.932 [INFO] faststream_gen._code_generator.helper: ************************************************************************************************************************\n",
      "23-09-21 23:57:42.723 [INFO] faststream_gen._code_generator.app_and_test_generator: ==== Skeleton to App and Test Generation ====\n",
      "23-09-21 23:57:42.724 [INFO] faststream_gen._code_generator.helper: ************************************************************************************************************************\n",
      "23-09-21 23:57:42.724 [INFO] faststream_gen._code_generator.helper: \n",
      "\n",
      "Prompt to the model: \n",
      "\n",
      "===Role:system===\n",
      "\n",
      "Message:\n",
      "\n",
      "You are an expert Python developer, tasked to generate executable Python code as a part of your work with the FastStream framework. \n",
      "\n",
      "You are to abide by the following guidelines:\n",
      "\n",
      "1. You must never enclose the generated Python code with ``` python. It is mandatory that the output is a valid and executable Python code. Please ensure this rule is never broken.\n",
      "\n",
      "2. Some prompts might require you to generate code that contains async functions. For example:\n",
      "\n",
      "async def app_setup(context: ContextRepo):\n",
      "    raise NotImplementedError()\n",
      "\n",
      "In such cases, it is necessary to add the \"import asyncio\" statement at the top of the code. \n",
      "\n",
      "You will encounter sections marked as:\n",
      "\n",
      "==== APP DESCRIPTION: ====\n",
      "\n",
      "These sections contain the description of the FastStream app you need to implement. Treat everything below this line, until the end of the prompt, as the description to follow for the app implementation.\n",
      "\n",
      "\n",
      "===Role:user===\n",
      "\n",
      "Message:\n",
      "\n",
      "\n",
      "===Role:user===\n",
      "\n",
      "Message:\n",
      "\n",
      "You will be provided with a description about FastStream application in ==== APP DESCRIPTION ==== section and the skeleton code in ==== APP SKELETON ==== section. Your goal is to generate both the application and the test code based on the provided ==== APP SKELETON ==== section and the description ==== APP DESCRIPTION ==== in a single Python file. Your response will be split the application.py and the test.py files and saved to the disc.\n",
      "\n",
      "Input:\n",
      "\n",
      "You will be given skeleton code and the description for a FastStream application in ==== APP SKELETON ==== section and ==== APP DESCRIPTION ==== section respectively. The skeleton code and the description will have implementation details.\n",
      "\n",
      "Output:\n",
      "\n",
      "You need to understand the business logic mentioned in in ==== APP SKELETON ==== section and ==== APP DESCRIPTION ==== sections and generate the following:\n",
      "\n",
      "    - The application code by implementing the methods decorated with @broker.subscriber and @broker.publisher in the docstring\n",
      "    - The test code which tests the functionality of the generated application file\n",
      "\n",
      "You need to generate a single valid Python file containing both the application and the test code. Remember that the application and the test code will be split into two and saved to the disc as application.py and test.py. So while writing the test code, make sure to import all the necessary symbols from the application.py\n",
      "\n",
      "Application Code:\n",
      "\n",
      "Generate the entire application code from the provided skeleton. You must implement all of the business logic specified in the docstrings of the @broker.subscriber and @broker.publisher decorated functions in the skeleton code provided in the \"==== APP SKELETON ====\" section. When implementing business logic for methods decorated with @broker.subscriber and @broker.publisher, strictly follow the instructions in the docstring. Other parts of the code should not be changed.\n",
      "\n",
      "Test Code:\n",
      "\n",
      "Create test code using the TestBroker context managers to validate the functionality of the application code.\n",
      "\n",
      "The FastStream apps can be tested using the TestBroker context managers which, by default, puts the Broker into \"testing mode\". The Tester will redirect your subscriber and publisher decorated functions to the InMemory brokers so that you can quickly test your app without the need for a running broker and all its dependencies.\n",
      "\n",
      "While generating the application and the test code for FastStream application based on provided application code in the ==== APP SKELETON: ==== section, adhering to these guidelines:\n",
      "\n",
      "    - You need to generate the app code and the test code for the application code mentioned in ==== APP SKELETON: ==== in a single valid python file\n",
      "    - Follow the PEP 8 Style Guide for Python while writing the code\n",
      "    - Write optimised and readable Code\n",
      "    - Output only a valid executable python code. No other extra text should be included in your response.\n",
      "    - DO NOT enclose the response within back-ticks. Meaning NEVER ADD ```python to your response.\n",
      "    - Never try to explain and reason your answers. Only return a valid python code.\n",
      "    - Your response should be divided into two section, ### application.py ### which contains the application code and ### test.py ### which contains the test code.\n",
      "    - Remember, your response must have only two seperators ### application.py ### and ### test.py ###. DO NOT add any other separators (e.g. ### main.py ###).\n",
      "    - NEVER initialize publisher variable inside functions. You can use broker.publish function only in the test functions. In ALL OTHER functions, use publisher!\n",
      "    - ALWAYS initialize publisher variable (If you need to publish message) after defining app variable:\n",
      "\n",
      "        Example 1:\n",
      "        app = FastStream(broker)\n",
      "        publisher = broker.publisher(\"new_data\")\n",
      "\n",
      "        Example 2:\n",
      "        app = FastStream(broker)\n",
      "        new_data_publisher = broker.publisher(\"new_data\")\n",
      "\n",
      "Guidelines for writing tests: \n",
      "    - When a function is decorated @broker.publisher in the application code, remember that this function will always publish the message irrespective of the conditions mentioned in the \"==== APP SKELETON ====\" section. In such cases, never use mock.assert_not_called() while testing the function. For example:\n",
      "    - When you test a function in the application which is decorated with @broker.publisher(\"output_data\"), you should never use on_output_data.mock.assert_not_called() in your tests. Never ever break this rule. Because the decorator @broker.publisher will always publish the message irrespective of the conditions mentioned in the decorated function.\n",
      "\n",
      "Below are few examples for your understanding:\n",
      "\n",
      "\n",
      "==== EXAMPLE APP DESCRIPTION ====\n",
      "\n",
      "Develop a FastStream application using localhost kafka broker.\n",
      "The app should consume messages from the input_data topic.\n",
      "The input message is a JSON encoded object including two attributes:\n",
      "    - x: float\n",
      "    - y: float\n",
      "\n",
      "While consuming the message, increment x and y attributes by 1 and publish that message to the output_data topic.\n",
      "Use messages attribute x as a partition key when publishing to output_data topic.\n",
      "\n",
      "\n",
      "\n",
      "==== EXAMPLE APP SKELETON ====\n",
      "\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "from faststream import FastStream, Logger\n",
      "from faststream.kafka import KafkaBroker\n",
      "\n",
      "\n",
      "class Point(BaseModel):\n",
      "    x: float = Field(\n",
      "        ..., examples=[0.5], description=\"The X Coordinate in the coordinate system\"\n",
      "    )\n",
      "    y: float = Field(\n",
      "        ..., examples=[0.5], description=\"The Y Coordinate in the coordinate system\"\n",
      "    )\n",
      "\n",
      "\n",
      "broker = KafkaBroker(\"localhost:9092\")\n",
      "app = FastStream(broker)\n",
      "\n",
      "\n",
      "to_output_data = broker.publisher(\"output_data\")\n",
      "\n",
      "\n",
      "@broker.subscriber(\"input_data\")\n",
      "async def on_input_data(msg: Point, logger: Logger) -> None:\n",
      "    \"\"\"\n",
      "    Processes a message from the 'input_data' topic.\n",
      "    Increment msg x and y attributes with 1 and publish that message to the output_data topic.\n",
      "    Publish that message to the output_data topic\n",
      "    Use messages attribute x as a partition key when publishing to output_data topic.\n",
      "\n",
      "    Instructions:\n",
      "    1. Consume a message from 'input_data' topic.\n",
      "    2. Create a new message object (do not directly modify the original).\n",
      "    3. Increment msg x and y attributes with 1.\n",
      "    4. Publish that message to the output_data topic (Use messages attribute x as a partition key).\n",
      "    \"\"\"\n",
      "    raise NotImplementedError()\n",
      "\n",
      "\n",
      "==== YOUR RESPONSE ====\n",
      "\n",
      "### application.py ###\n",
      "\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "from faststream import FastStream, Logger\n",
      "from faststream.kafka import KafkaBroker\n",
      "\n",
      "\n",
      "class Point(BaseModel):\n",
      "    x: float = Field(\n",
      "        ..., examples=[0.5], description=\"The X Coordinate in the coordinate system\"\n",
      "    )\n",
      "    y: float = Field(\n",
      "        ..., examples=[0.5], description=\"The Y Coordinate in the coordinate system\"\n",
      "    )\n",
      "\n",
      "\n",
      "broker = KafkaBroker(\"localhost:9092\")\n",
      "app = FastStream(broker)\n",
      "\n",
      "\n",
      "to_output_data = broker.publisher(\"output_data\")\n",
      "\n",
      "\n",
      "@broker.subscriber(\"input_data\")\n",
      "async def on_input_data(msg: Point, logger: Logger) -> None:\n",
      "    logger.info(f\"{msg=}\")\n",
      "    incremented_point = Point(x=msg.x + 1, y=msg.y + 1)\n",
      "    key = str(msg.x).encode(\"utf-8\")\n",
      "    await to_output_data.publish(incremented_point, key=key)\n",
      "\n",
      "\n",
      "### test.py ###\n",
      "\n",
      "import pytest\n",
      "\n",
      "from faststream import Context\n",
      "from faststream.kafka import TestKafkaBroker\n",
      "\n",
      "from application import Point, broker, on_input_data\n",
      "\n",
      "\n",
      "@broker.subscriber(\"output_data\")\n",
      "async def on_output_data(msg: Point, key: bytes = Context(\"message.raw_message.key\")):\n",
      "    pass\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_point_was_incremented():\n",
      "    async with TestKafkaBroker(broker):\n",
      "        await broker.publish(Point(x=1.0, y=2.0), \"input_data\")\n",
      "        on_input_data.mock.assert_called_with(dict(Point(x=1.0, y=2.0)))\n",
      "        on_output_data.mock.assert_called_with(dict(Point(x=2.0, y=3.0)))\n",
      "\n",
      "\n",
      "==== EXAMPLE APP DESCRIPTION ====\n",
      "\n",
      "Develop a FastStream application using localhost kafka broker.\n",
      "The app should consume messages from the input_data topic.\n",
      "The input message is a JSON encoded object including two attributes:\n",
      "    - x: float\n",
      "    - y: float\n",
      "\n",
      "input_data topic should use partition key.\n",
      "While consuming the message, increment x and y attributes by 1 and publish that message to the output_data topic.\n",
      "The same partition key should be used in the input_data and output_data topic.\n",
      "\n",
      "\n",
      "\n",
      "==== EXAMPLE APP SKELETON ====\n",
      "\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "from faststream import Context, FastStream, Logger\n",
      "from faststream.kafka import KafkaBroker\n",
      "\n",
      "\n",
      "class Point(BaseModel):\n",
      "    x: float = Field(\n",
      "        ..., examples=[0.5], description=\"The X Coordinate in the coordinate system\"\n",
      "    )\n",
      "    y: float = Field(\n",
      "        ..., examples=[0.5], description=\"The Y Coordinate in the coordinate system\"\n",
      "    )\n",
      "\n",
      "\n",
      "broker = KafkaBroker(\"localhost:9092\")\n",
      "app = FastStream(broker)\n",
      "\n",
      "\n",
      "to_output_data = broker.publisher(\"output_data\")\n",
      "\n",
      "\n",
      "@broker.subscriber(\"input_data\")\n",
      "async def on_input_data(\n",
      "    msg: Point, logger: Logger, key: bytes = Context(\"message.raw_message.key\")\n",
      ") -> None:\n",
      "    \"\"\"\n",
      "    Processes a message from the 'input_data' topic.\n",
      "    Increment msg x and y attributes with 1 and publish that message to the output_data topic.\n",
      "    The same partition key should be used in the input_data and output_data topic.\n",
      "\n",
      "    Instructions:\n",
      "    1. Consume a message from 'input_data' topic.\n",
      "    2. Create a new message object (do not directly modify the original).\n",
      "    3. Increment msg x and y attributes with 1.\n",
      "    4. Publish that message to the output_data topic (The same partition key should be used in the input_data and output_data topic).\n",
      "    \"\"\"\n",
      "    raise NotImplementedError()\n",
      "\n",
      "\n",
      "==== YOUR RESPONSE ====\n",
      "\n",
      "### application.py ###\n",
      "\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "from faststream import Context, FastStream, Logger\n",
      "from faststream.kafka import KafkaBroker\n",
      "\n",
      "\n",
      "class Point(BaseModel):\n",
      "    x: float = Field(\n",
      "        ..., examples=[0.5], description=\"The X Coordinate in the coordinate system\"\n",
      "    )\n",
      "    y: float = Field(\n",
      "        ..., examples=[0.5], description=\"The Y Coordinate in the coordinate system\"\n",
      "    )\n",
      "\n",
      "\n",
      "broker = KafkaBroker(\"localhost:9092\")\n",
      "app = FastStream(broker)\n",
      "\n",
      "\n",
      "to_output_data = broker.publisher(\"output_data\")\n",
      "\n",
      "\n",
      "@broker.subscriber(\"input_data\")\n",
      "async def on_input_data(\n",
      "    msg: Point, logger: Logger, key: bytes = Context(\"message.raw_message.key\")\n",
      ") -> None:\n",
      "    logger.info(f\"{msg=}\")\n",
      "    incremented_point = Point(x=msg.x + 1, y=msg.y + 1)\n",
      "    await to_output_data.publish(incremented_point, key=key)\n",
      "\n",
      "\n",
      "### test.py ###\n",
      "\n",
      "import pytest\n",
      "\n",
      "from faststream import Context\n",
      "from faststream.kafka import TestKafkaBroker\n",
      "\n",
      "from application import Point, broker, on_input_data\n",
      "\n",
      "\n",
      "@broker.subscriber(\"output_data\")\n",
      "async def on_output_data(msg: Point, key: bytes = Context(\"message.raw_message.key\")):\n",
      "    pass\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_point_was_incremented():\n",
      "    async with TestKafkaBroker(broker):\n",
      "        await broker.publish(Point(x=1.0, y=2.0), \"input_data\", key=b\"key\")\n",
      "        on_input_data.mock.assert_called_with(dict(Point(x=1.0, y=2.0)))\n",
      "        on_output_data.mock.assert_called_with(dict(Point(x=2.0, y=3.0)))\n",
      "\n",
      "\n",
      "==== EXAMPLE APP DESCRIPTION ====\n",
      "\n",
      "Develop a FastStream application using localhost kafka broker.\n",
      "The app should consume messages from the input_data topic.\n",
      "The input message is a JSON encoded object including two attributes:\n",
      "    - x: float\n",
      "    - y: float\n",
      "\n",
      "input_data topic should use partition key.\n",
      "\n",
      "Keep all the previous messages in the memory.\n",
      "While consuming the message, add all x elements from the memory (x_sum) and all y from the memory (y_sum) and publish the message with x_sum and y_sum to the output_data topic.\n",
      "The same partition key should be used in the input_data and output_data topic.\n",
      "\n",
      "\n",
      "\n",
      "==== EXAMPLE APP SKELETON ====\n",
      "\n",
      "from typing import List\n",
      "\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "from faststream import Context, ContextRepo, FastStream, Logger\n",
      "from faststream.kafka import KafkaBroker\n",
      "\n",
      "\n",
      "class Point(BaseModel):\n",
      "    x: float = Field(\n",
      "        ..., examples=[0.5], description=\"The X Coordinate in the coordinate system\"\n",
      "    )\n",
      "    y: float = Field(\n",
      "        ..., examples=[0.5], description=\"The Y Coordinate in the coordinate system\"\n",
      "    )\n",
      "\n",
      "\n",
      "broker = KafkaBroker(\"localhost:9092\")\n",
      "app = FastStream(broker)\n",
      "\n",
      "\n",
      "to_output_data = broker.publisher(\"output_data\")\n",
      "\n",
      "\n",
      "@app.on_startup\n",
      "async def app_setup(context: ContextRepo):\n",
      "    \"\"\"\n",
      "    Set all necessary global variables inside ContextRepo object:\n",
      "        Set message_history for storing all input messages\n",
      "    \"\"\"\n",
      "    raise NotImplementedError()\n",
      "\n",
      "\n",
      "@broker.subscriber(\"input_data\")\n",
      "async def on_input_data(\n",
      "    msg: Point,\n",
      "    logger: Logger,\n",
      "    message_history: List[Point] = Context(),\n",
      "    key: bytes = Context(\"message.raw_message.key\"),\n",
      ") -> None:\n",
      "    \"\"\"\n",
      "    Processes a message from the 'input_data' topic.\n",
      "    Add all x elements from the memory (x_sum) and all y from the memory (y_sum) and publish the message with x_sum and y_sum to the output_data topic.\n",
      "    The same partition key should be used in the input_data and output_data topic.\n",
      "\n",
      "    Instructions:\n",
      "    1. Consume a message from 'input_data' topic.\n",
      "    2. Create a new message object (do not directly modify the original).\n",
      "    3. Add all x elements from the memory (x_sum) and all y from the memory (y_sum)\n",
      "    4. Publish the message with x_sum and y_sum to the output_data topic. (The same partition key should be used in the input_data and output_data topic).\n",
      "    \"\"\"\n",
      "    raise NotImplementedError()\n",
      "\n",
      "\n",
      "==== YOUR RESPONSE ====\n",
      "\n",
      "### application.py ###\n",
      "\n",
      "from typing import List\n",
      "\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "from faststream import Context, ContextRepo, FastStream, Logger\n",
      "from faststream.kafka import KafkaBroker\n",
      "\n",
      "\n",
      "class Point(BaseModel):\n",
      "    x: float = Field(\n",
      "        ..., examples=[0.5], description=\"The X Coordinate in the coordinate system\"\n",
      "    )\n",
      "    y: float = Field(\n",
      "        ..., examples=[0.5], description=\"The Y Coordinate in the coordinate system\"\n",
      "    )\n",
      "\n",
      "\n",
      "broker = KafkaBroker(\"localhost:9092\")\n",
      "app = FastStream(broker)\n",
      "\n",
      "\n",
      "to_output_data = broker.publisher(\"output_data\")\n",
      "\n",
      "\n",
      "@app.on_startup\n",
      "async def app_setup(context: ContextRepo):\n",
      "    message_history: List[Point] = []\n",
      "    context.set_global(\"message_history\", message_history)\n",
      "\n",
      "\n",
      "@broker.subscriber(\"input_data\")\n",
      "async def on_input_data(\n",
      "    msg: Point,\n",
      "    logger: Logger,\n",
      "    message_history: List[Point] = Context(),\n",
      "    key: bytes = Context(\"message.raw_message.key\"),\n",
      ") -> None:\n",
      "    logger.info(f\"{msg=}\")\n",
      "\n",
      "    message_history.append(msg)\n",
      "\n",
      "    x_sum = 0\n",
      "    y_sum = 0\n",
      "    for msg in message_history:\n",
      "        x_sum += msg.x\n",
      "        y_sum += msg.y\n",
      "\n",
      "    point_sum = Point(x=x_sum, y=y_sum)\n",
      "    await to_output_data.publish(point_sum, key=key)\n",
      "\n",
      "\n",
      "### test.py ###\n",
      "\n",
      "import pytest\n",
      "\n",
      "from faststream import Context, TestApp\n",
      "from faststream.kafka import TestKafkaBroker\n",
      "\n",
      "from application import Point, app, broker\n",
      "\n",
      "\n",
      "@broker.subscriber(\"output_data\")\n",
      "async def on_output_data(msg: Point, key: bytes = Context(\"message.raw_message.key\")):\n",
      "    pass\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_point_was_incremented():\n",
      "    async with TestKafkaBroker(broker):\n",
      "        async with TestApp(app):\n",
      "            await broker.publish(Point(x=1.0, y=2.0), \"input_data\", key=b\"point_key\")\n",
      "            await broker.publish(Point(x=1.0, y=2.0), \"input_data\", key=b\"point_key\")\n",
      "\n",
      "            on_output_data.mock.assert_called_with(dict(Point(x=2.0, y=4.0)))\n",
      "\n",
      "\n",
      "\n",
      "You need to avoid the below common issues while generating the test code. The potential fixes for the common issues are also given for your reference. You should use this information as an additional reference while generating the test:\n",
      "\n",
      "==== Error ====\n",
      "\n",
      "AssertionError: Expected 'mock' to not have been called. Called 2 times.\n",
      "\n",
      "==== ERROR CODE ====\n",
      "async with TestKafkaBroker(broker):\n",
      "await broker.publish(DataBasic(data=0.2), \"input_data\")\n",
      "\n",
      "        on_input_data.mock.assert_called_with(dict(DataBasic(data=0.2)))\n",
      "        on_output_data.mock.assert_not_called() # ERROR IN THIS LINE\n",
      "\n",
      "==== FIXED CODE ====\n",
      "\n",
      "    async with TestKafkaBroker(broker):\n",
      "        await broker.publish(DataBasic(data=0.2), \"input_data\")\n",
      "\n",
      "        on_input_data.mock.assert_called_with(dict(DataBasic(data=0.2)))\n",
      "        on_output_data.mock.assert_called_once_with(dict(DataBasic(data=1.2))) # ERROR FIXED IN THIS LINE\n",
      "\n",
      "==== Error ====\n",
      "\n",
      "pydantic_core.\\_pydantic_core.ValidationError: 1 validation error for ResponseModel\n",
      "\n",
      "==== ERROR CODE ====\n",
      "@broker.subscriber(\"output_data\")\n",
      "async def on_output_data(msg: DataBasic) -> DataBasic: # ERROR IN THIS LINE\n",
      "pass\n",
      "\n",
      "==== FIXED CODE ====\n",
      "\n",
      "@broker.subscriber(\"output_data\")\n",
      "async def on_output_data(msg: DataBasic): # ERROR FIXED IN THIS LINE\n",
      "pass\n",
      "\n",
      "\n",
      "==== APP DESCRIPTION ====\n",
      "\n",
      "Create a FastStream application using localhost broker for testing and use the default port number. It should consume messages from the \"input_data\" topic, where each message is a JSON encoded object containing a single attribute: 'data'. For each consumed message, create a new message object and increment the value of the data attribute by 1. Finally, send the modified message to the 'output_data' topic.\n",
      "\n",
      "\n",
      "==== APP SKELETON ====\n",
      "\n",
      "\n",
      "\n",
      "===Role:user===\n",
      "\n",
      "Message:\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "from faststream import FastStream, Logger\n",
      "from faststream.kafka import KafkaBroker\n",
      "\n",
      "\n",
      "class InputMessage(BaseModel):\n",
      "    data: int = Field(\n",
      "        ..., examples=[5], description=\"The data attribute in the input message\"\n",
      "    )\n",
      "\n",
      "\n",
      "class OutputMessage(BaseModel):\n",
      "    data: int = Field(\n",
      "        ..., examples=[6], description=\"The modified data attribute in the output message\"\n",
      "    )\n",
      "\n",
      "\n",
      "broker = KafkaBroker(\"localhost\")\n",
      "app = FastStream(broker)\n",
      "\n",
      "\n",
      "to_output_data = broker.publisher(\"output_data\")\n",
      "\n",
      "\n",
      "@broker.subscriber(\"input_data\")\n",
      "async def on_input_data(msg: InputMessage, logger: Logger) -> None:\n",
      "    \"\"\"\n",
      "    Processes a message from the 'input_data' topic.\n",
      "    Create a new message object and increment the value of the data attribute by 1.\n",
      "    Send the modified message to the 'output_data' topic.\n",
      "\n",
      "    Instructions:\n",
      "    1. Consume a message from 'input_data' topic.\n",
      "    2. Create a new message object (do not directly modify the original).\n",
      "    3. Increment the value of the data attribute by 1.\n",
      "    4. Send the modified message to the 'output_data' topic.\n",
      "    \"\"\"\n",
      "    raise NotImplementedError()\n",
      "==== YOUR RESPONSE ====\n",
      "\n",
      "23-09-21 23:57:42.724 [INFO] faststream_gen._code_generator.helper: ************************************************************************************************************************\n",
      "23-09-21 23:57:48.833 [INFO] faststream_gen._code_generator.helper: Validation failed, trying again...Errors:\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.11.5, pytest-7.4.2, pluggy-1.3.0\n",
      "rootdir: /tmp\n",
      "plugins: anyio-3.7.1, asyncio-0.21.1\n",
      "asyncio: mode=Mode.STRICT\n",
      "collected 1 item\n",
      "\n",
      "../tmp9by9fmhm/test.py \u001b[31mF\u001b[0m\u001b[31m                                                 [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_______________________________ test_modify_data _______________________________\u001b[0m\n",
      "\u001b[1m\u001b[31m../tmp9by9fmhm/test.py\u001b[0m:19: in test_modify_data\n",
      "    \u001b[94masync\u001b[39;49;00m \u001b[94mwith\u001b[39;49;00m app:\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE   TypeError: 'FastStream' object does not support the asynchronous context manager protocol\u001b[0m\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m ../tmp9by9fmhm/test.py::\u001b[1mtest_modify_data\u001b[0m - TypeError: 'FastStream' object does not support the asynchronous context ma...\n",
      "\u001b[31m============================== \u001b[31m\u001b[1m1 failed\u001b[0m\u001b[31m in 0.57s\u001b[0m\u001b[31m ===============================\u001b[0m\n",
      "\n",
      "23-09-21 23:57:48.833 [INFO] faststream_gen._code_generator.helper: ************************************************************************************************************************\n",
      "23-09-21 23:57:48.833 [INFO] faststream_gen._code_generator.helper: \n",
      "\n",
      "Prompt to the model: \n",
      "\n",
      "===Role:system===\n",
      "\n",
      "Message:\n",
      "\n",
      "You are an expert Python developer, tasked to generate executable Python code as a part of your work with the FastStream framework. \n",
      "\n",
      "You are to abide by the following guidelines:\n",
      "\n",
      "1. You must never enclose the generated Python code with ``` python. It is mandatory that the output is a valid and executable Python code. Please ensure this rule is never broken.\n",
      "\n",
      "2. Some prompts might require you to generate code that contains async functions. For example:\n",
      "\n",
      "async def app_setup(context: ContextRepo):\n",
      "    raise NotImplementedError()\n",
      "\n",
      "In such cases, it is necessary to add the \"import asyncio\" statement at the top of the code. \n",
      "\n",
      "You will encounter sections marked as:\n",
      "\n",
      "==== APP DESCRIPTION: ====\n",
      "\n",
      "These sections contain the description of the FastStream app you need to implement. Treat everything below this line, until the end of the prompt, as the description to follow for the app implementation.\n",
      "\n",
      "\n",
      "===Role:user===\n",
      "\n",
      "Message:\n",
      "\n",
      "\n",
      "===Role:user===\n",
      "\n",
      "Message:\n",
      "\n",
      "You will be provided with a description about FastStream application in ==== APP DESCRIPTION ==== section and the skeleton code in ==== APP SKELETON ==== section. Your goal is to generate both the application and the test code based on the provided ==== APP SKELETON ==== section and the description ==== APP DESCRIPTION ==== in a single Python file. Your response will be split the application.py and the test.py files and saved to the disc.\n",
      "\n",
      "Input:\n",
      "\n",
      "You will be given skeleton code and the description for a FastStream application in ==== APP SKELETON ==== section and ==== APP DESCRIPTION ==== section respectively. The skeleton code and the description will have implementation details.\n",
      "\n",
      "Output:\n",
      "\n",
      "You need to understand the business logic mentioned in in ==== APP SKELETON ==== section and ==== APP DESCRIPTION ==== sections and generate the following:\n",
      "\n",
      "    - The application code by implementing the methods decorated with @broker.subscriber and @broker.publisher in the docstring\n",
      "    - The test code which tests the functionality of the generated application file\n",
      "\n",
      "You need to generate a single valid Python file containing both the application and the test code. Remember that the application and the test code will be split into two and saved to the disc as application.py and test.py. So while writing the test code, make sure to import all the necessary symbols from the application.py\n",
      "\n",
      "Application Code:\n",
      "\n",
      "Generate the entire application code from the provided skeleton. You must implement all of the business logic specified in the docstrings of the @broker.subscriber and @broker.publisher decorated functions in the skeleton code provided in the \"==== APP SKELETON ====\" section. When implementing business logic for methods decorated with @broker.subscriber and @broker.publisher, strictly follow the instructions in the docstring. Other parts of the code should not be changed.\n",
      "\n",
      "Test Code:\n",
      "\n",
      "Create test code using the TestBroker context managers to validate the functionality of the application code.\n",
      "\n",
      "The FastStream apps can be tested using the TestBroker context managers which, by default, puts the Broker into \"testing mode\". The Tester will redirect your subscriber and publisher decorated functions to the InMemory brokers so that you can quickly test your app without the need for a running broker and all its dependencies.\n",
      "\n",
      "While generating the application and the test code for FastStream application based on provided application code in the ==== APP SKELETON: ==== section, adhering to these guidelines:\n",
      "\n",
      "    - You need to generate the app code and the test code for the application code mentioned in ==== APP SKELETON: ==== in a single valid python file\n",
      "    - Follow the PEP 8 Style Guide for Python while writing the code\n",
      "    - Write optimised and readable Code\n",
      "    - Output only a valid executable python code. No other extra text should be included in your response.\n",
      "    - DO NOT enclose the response within back-ticks. Meaning NEVER ADD ```python to your response.\n",
      "    - Never try to explain and reason your answers. Only return a valid python code.\n",
      "    - Your response should be divided into two section, ### application.py ### which contains the application code and ### test.py ### which contains the test code.\n",
      "    - Remember, your response must have only two seperators ### application.py ### and ### test.py ###. DO NOT add any other separators (e.g. ### main.py ###).\n",
      "    - NEVER initialize publisher variable inside functions. You can use broker.publish function only in the test functions. In ALL OTHER functions, use publisher!\n",
      "    - ALWAYS initialize publisher variable (If you need to publish message) after defining app variable:\n",
      "\n",
      "        Example 1:\n",
      "        app = FastStream(broker)\n",
      "        publisher = broker.publisher(\"new_data\")\n",
      "\n",
      "        Example 2:\n",
      "        app = FastStream(broker)\n",
      "        new_data_publisher = broker.publisher(\"new_data\")\n",
      "\n",
      "Guidelines for writing tests: \n",
      "    - When a function is decorated @broker.publisher in the application code, remember that this function will always publish the message irrespective of the conditions mentioned in the \"==== APP SKELETON ====\" section. In such cases, never use mock.assert_not_called() while testing the function. For example:\n",
      "    - When you test a function in the application which is decorated with @broker.publisher(\"output_data\"), you should never use on_output_data.mock.assert_not_called() in your tests. Never ever break this rule. Because the decorator @broker.publisher will always publish the message irrespective of the conditions mentioned in the decorated function.\n",
      "\n",
      "Below are few examples for your understanding:\n",
      "\n",
      "\n",
      "==== EXAMPLE APP DESCRIPTION ====\n",
      "\n",
      "Develop a FastStream application using localhost kafka broker.\n",
      "The app should consume messages from the input_data topic.\n",
      "The input message is a JSON encoded object including two attributes:\n",
      "    - x: float\n",
      "    - y: float\n",
      "\n",
      "While consuming the message, increment x and y attributes by 1 and publish that message to the output_data topic.\n",
      "Use messages attribute x as a partition key when publishing to output_data topic.\n",
      "\n",
      "\n",
      "\n",
      "==== EXAMPLE APP SKELETON ====\n",
      "\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "from faststream import FastStream, Logger\n",
      "from faststream.kafka import KafkaBroker\n",
      "\n",
      "\n",
      "class Point(BaseModel):\n",
      "    x: float = Field(\n",
      "        ..., examples=[0.5], description=\"The X Coordinate in the coordinate system\"\n",
      "    )\n",
      "    y: float = Field(\n",
      "        ..., examples=[0.5], description=\"The Y Coordinate in the coordinate system\"\n",
      "    )\n",
      "\n",
      "\n",
      "broker = KafkaBroker(\"localhost:9092\")\n",
      "app = FastStream(broker)\n",
      "\n",
      "\n",
      "to_output_data = broker.publisher(\"output_data\")\n",
      "\n",
      "\n",
      "@broker.subscriber(\"input_data\")\n",
      "async def on_input_data(msg: Point, logger: Logger) -> None:\n",
      "    \"\"\"\n",
      "    Processes a message from the 'input_data' topic.\n",
      "    Increment msg x and y attributes with 1 and publish that message to the output_data topic.\n",
      "    Publish that message to the output_data topic\n",
      "    Use messages attribute x as a partition key when publishing to output_data topic.\n",
      "\n",
      "    Instructions:\n",
      "    1. Consume a message from 'input_data' topic.\n",
      "    2. Create a new message object (do not directly modify the original).\n",
      "    3. Increment msg x and y attributes with 1.\n",
      "    4. Publish that message to the output_data topic (Use messages attribute x as a partition key).\n",
      "    \"\"\"\n",
      "    raise NotImplementedError()\n",
      "\n",
      "\n",
      "==== YOUR RESPONSE ====\n",
      "\n",
      "### application.py ###\n",
      "\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "from faststream import FastStream, Logger\n",
      "from faststream.kafka import KafkaBroker\n",
      "\n",
      "\n",
      "class Point(BaseModel):\n",
      "    x: float = Field(\n",
      "        ..., examples=[0.5], description=\"The X Coordinate in the coordinate system\"\n",
      "    )\n",
      "    y: float = Field(\n",
      "        ..., examples=[0.5], description=\"The Y Coordinate in the coordinate system\"\n",
      "    )\n",
      "\n",
      "\n",
      "broker = KafkaBroker(\"localhost:9092\")\n",
      "app = FastStream(broker)\n",
      "\n",
      "\n",
      "to_output_data = broker.publisher(\"output_data\")\n",
      "\n",
      "\n",
      "@broker.subscriber(\"input_data\")\n",
      "async def on_input_data(msg: Point, logger: Logger) -> None:\n",
      "    logger.info(f\"{msg=}\")\n",
      "    incremented_point = Point(x=msg.x + 1, y=msg.y + 1)\n",
      "    key = str(msg.x).encode(\"utf-8\")\n",
      "    await to_output_data.publish(incremented_point, key=key)\n",
      "\n",
      "\n",
      "### test.py ###\n",
      "\n",
      "import pytest\n",
      "\n",
      "from faststream import Context\n",
      "from faststream.kafka import TestKafkaBroker\n",
      "\n",
      "from application import Point, broker, on_input_data\n",
      "\n",
      "\n",
      "@broker.subscriber(\"output_data\")\n",
      "async def on_output_data(msg: Point, key: bytes = Context(\"message.raw_message.key\")):\n",
      "    pass\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_point_was_incremented():\n",
      "    async with TestKafkaBroker(broker):\n",
      "        await broker.publish(Point(x=1.0, y=2.0), \"input_data\")\n",
      "        on_input_data.mock.assert_called_with(dict(Point(x=1.0, y=2.0)))\n",
      "        on_output_data.mock.assert_called_with(dict(Point(x=2.0, y=3.0)))\n",
      "\n",
      "\n",
      "==== EXAMPLE APP DESCRIPTION ====\n",
      "\n",
      "Develop a FastStream application using localhost kafka broker.\n",
      "The app should consume messages from the input_data topic.\n",
      "The input message is a JSON encoded object including two attributes:\n",
      "    - x: float\n",
      "    - y: float\n",
      "\n",
      "input_data topic should use partition key.\n",
      "While consuming the message, increment x and y attributes by 1 and publish that message to the output_data topic.\n",
      "The same partition key should be used in the input_data and output_data topic.\n",
      "\n",
      "\n",
      "\n",
      "==== EXAMPLE APP SKELETON ====\n",
      "\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "from faststream import Context, FastStream, Logger\n",
      "from faststream.kafka import KafkaBroker\n",
      "\n",
      "\n",
      "class Point(BaseModel):\n",
      "    x: float = Field(\n",
      "        ..., examples=[0.5], description=\"The X Coordinate in the coordinate system\"\n",
      "    )\n",
      "    y: float = Field(\n",
      "        ..., examples=[0.5], description=\"The Y Coordinate in the coordinate system\"\n",
      "    )\n",
      "\n",
      "\n",
      "broker = KafkaBroker(\"localhost:9092\")\n",
      "app = FastStream(broker)\n",
      "\n",
      "\n",
      "to_output_data = broker.publisher(\"output_data\")\n",
      "\n",
      "\n",
      "@broker.subscriber(\"input_data\")\n",
      "async def on_input_data(\n",
      "    msg: Point, logger: Logger, key: bytes = Context(\"message.raw_message.key\")\n",
      ") -> None:\n",
      "    \"\"\"\n",
      "    Processes a message from the 'input_data' topic.\n",
      "    Increment msg x and y attributes with 1 and publish that message to the output_data topic.\n",
      "    The same partition key should be used in the input_data and output_data topic.\n",
      "\n",
      "    Instructions:\n",
      "    1. Consume a message from 'input_data' topic.\n",
      "    2. Create a new message object (do not directly modify the original).\n",
      "    3. Increment msg x and y attributes with 1.\n",
      "    4. Publish that message to the output_data topic (The same partition key should be used in the input_data and output_data topic).\n",
      "    \"\"\"\n",
      "    raise NotImplementedError()\n",
      "\n",
      "\n",
      "==== YOUR RESPONSE ====\n",
      "\n",
      "### application.py ###\n",
      "\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "from faststream import Context, FastStream, Logger\n",
      "from faststream.kafka import KafkaBroker\n",
      "\n",
      "\n",
      "class Point(BaseModel):\n",
      "    x: float = Field(\n",
      "        ..., examples=[0.5], description=\"The X Coordinate in the coordinate system\"\n",
      "    )\n",
      "    y: float = Field(\n",
      "        ..., examples=[0.5], description=\"The Y Coordinate in the coordinate system\"\n",
      "    )\n",
      "\n",
      "\n",
      "broker = KafkaBroker(\"localhost:9092\")\n",
      "app = FastStream(broker)\n",
      "\n",
      "\n",
      "to_output_data = broker.publisher(\"output_data\")\n",
      "\n",
      "\n",
      "@broker.subscriber(\"input_data\")\n",
      "async def on_input_data(\n",
      "    msg: Point, logger: Logger, key: bytes = Context(\"message.raw_message.key\")\n",
      ") -> None:\n",
      "    logger.info(f\"{msg=}\")\n",
      "    incremented_point = Point(x=msg.x + 1, y=msg.y + 1)\n",
      "    await to_output_data.publish(incremented_point, key=key)\n",
      "\n",
      "\n",
      "### test.py ###\n",
      "\n",
      "import pytest\n",
      "\n",
      "from faststream import Context\n",
      "from faststream.kafka import TestKafkaBroker\n",
      "\n",
      "from application import Point, broker, on_input_data\n",
      "\n",
      "\n",
      "@broker.subscriber(\"output_data\")\n",
      "async def on_output_data(msg: Point, key: bytes = Context(\"message.raw_message.key\")):\n",
      "    pass\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_point_was_incremented():\n",
      "    async with TestKafkaBroker(broker):\n",
      "        await broker.publish(Point(x=1.0, y=2.0), \"input_data\", key=b\"key\")\n",
      "        on_input_data.mock.assert_called_with(dict(Point(x=1.0, y=2.0)))\n",
      "        on_output_data.mock.assert_called_with(dict(Point(x=2.0, y=3.0)))\n",
      "\n",
      "\n",
      "==== EXAMPLE APP DESCRIPTION ====\n",
      "\n",
      "Develop a FastStream application using localhost kafka broker.\n",
      "The app should consume messages from the input_data topic.\n",
      "The input message is a JSON encoded object including two attributes:\n",
      "    - x: float\n",
      "    - y: float\n",
      "\n",
      "input_data topic should use partition key.\n",
      "\n",
      "Keep all the previous messages in the memory.\n",
      "While consuming the message, add all x elements from the memory (x_sum) and all y from the memory (y_sum) and publish the message with x_sum and y_sum to the output_data topic.\n",
      "The same partition key should be used in the input_data and output_data topic.\n",
      "\n",
      "\n",
      "\n",
      "==== EXAMPLE APP SKELETON ====\n",
      "\n",
      "from typing import List\n",
      "\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "from faststream import Context, ContextRepo, FastStream, Logger\n",
      "from faststream.kafka import KafkaBroker\n",
      "\n",
      "\n",
      "class Point(BaseModel):\n",
      "    x: float = Field(\n",
      "        ..., examples=[0.5], description=\"The X Coordinate in the coordinate system\"\n",
      "    )\n",
      "    y: float = Field(\n",
      "        ..., examples=[0.5], description=\"The Y Coordinate in the coordinate system\"\n",
      "    )\n",
      "\n",
      "\n",
      "broker = KafkaBroker(\"localhost:9092\")\n",
      "app = FastStream(broker)\n",
      "\n",
      "\n",
      "to_output_data = broker.publisher(\"output_data\")\n",
      "\n",
      "\n",
      "@app.on_startup\n",
      "async def app_setup(context: ContextRepo):\n",
      "    \"\"\"\n",
      "    Set all necessary global variables inside ContextRepo object:\n",
      "        Set message_history for storing all input messages\n",
      "    \"\"\"\n",
      "    raise NotImplementedError()\n",
      "\n",
      "\n",
      "@broker.subscriber(\"input_data\")\n",
      "async def on_input_data(\n",
      "    msg: Point,\n",
      "    logger: Logger,\n",
      "    message_history: List[Point] = Context(),\n",
      "    key: bytes = Context(\"message.raw_message.key\"),\n",
      ") -> None:\n",
      "    \"\"\"\n",
      "    Processes a message from the 'input_data' topic.\n",
      "    Add all x elements from the memory (x_sum) and all y from the memory (y_sum) and publish the message with x_sum and y_sum to the output_data topic.\n",
      "    The same partition key should be used in the input_data and output_data topic.\n",
      "\n",
      "    Instructions:\n",
      "    1. Consume a message from 'input_data' topic.\n",
      "    2. Create a new message object (do not directly modify the original).\n",
      "    3. Add all x elements from the memory (x_sum) and all y from the memory (y_sum)\n",
      "    4. Publish the message with x_sum and y_sum to the output_data topic. (The same partition key should be used in the input_data and output_data topic).\n",
      "    \"\"\"\n",
      "    raise NotImplementedError()\n",
      "\n",
      "\n",
      "==== YOUR RESPONSE ====\n",
      "\n",
      "### application.py ###\n",
      "\n",
      "from typing import List\n",
      "\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "from faststream import Context, ContextRepo, FastStream, Logger\n",
      "from faststream.kafka import KafkaBroker\n",
      "\n",
      "\n",
      "class Point(BaseModel):\n",
      "    x: float = Field(\n",
      "        ..., examples=[0.5], description=\"The X Coordinate in the coordinate system\"\n",
      "    )\n",
      "    y: float = Field(\n",
      "        ..., examples=[0.5], description=\"The Y Coordinate in the coordinate system\"\n",
      "    )\n",
      "\n",
      "\n",
      "broker = KafkaBroker(\"localhost:9092\")\n",
      "app = FastStream(broker)\n",
      "\n",
      "\n",
      "to_output_data = broker.publisher(\"output_data\")\n",
      "\n",
      "\n",
      "@app.on_startup\n",
      "async def app_setup(context: ContextRepo):\n",
      "    message_history: List[Point] = []\n",
      "    context.set_global(\"message_history\", message_history)\n",
      "\n",
      "\n",
      "@broker.subscriber(\"input_data\")\n",
      "async def on_input_data(\n",
      "    msg: Point,\n",
      "    logger: Logger,\n",
      "    message_history: List[Point] = Context(),\n",
      "    key: bytes = Context(\"message.raw_message.key\"),\n",
      ") -> None:\n",
      "    logger.info(f\"{msg=}\")\n",
      "\n",
      "    message_history.append(msg)\n",
      "\n",
      "    x_sum = 0\n",
      "    y_sum = 0\n",
      "    for msg in message_history:\n",
      "        x_sum += msg.x\n",
      "        y_sum += msg.y\n",
      "\n",
      "    point_sum = Point(x=x_sum, y=y_sum)\n",
      "    await to_output_data.publish(point_sum, key=key)\n",
      "\n",
      "\n",
      "### test.py ###\n",
      "\n",
      "import pytest\n",
      "\n",
      "from faststream import Context, TestApp\n",
      "from faststream.kafka import TestKafkaBroker\n",
      "\n",
      "from application import Point, app, broker\n",
      "\n",
      "\n",
      "@broker.subscriber(\"output_data\")\n",
      "async def on_output_data(msg: Point, key: bytes = Context(\"message.raw_message.key\")):\n",
      "    pass\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_point_was_incremented():\n",
      "    async with TestKafkaBroker(broker):\n",
      "        async with TestApp(app):\n",
      "            await broker.publish(Point(x=1.0, y=2.0), \"input_data\", key=b\"point_key\")\n",
      "            await broker.publish(Point(x=1.0, y=2.0), \"input_data\", key=b\"point_key\")\n",
      "\n",
      "            on_output_data.mock.assert_called_with(dict(Point(x=2.0, y=4.0)))\n",
      "\n",
      "\n",
      "\n",
      "You need to avoid the below common issues while generating the test code. The potential fixes for the common issues are also given for your reference. You should use this information as an additional reference while generating the test:\n",
      "\n",
      "==== Error ====\n",
      "\n",
      "AssertionError: Expected 'mock' to not have been called. Called 2 times.\n",
      "\n",
      "==== ERROR CODE ====\n",
      "async with TestKafkaBroker(broker):\n",
      "await broker.publish(DataBasic(data=0.2), \"input_data\")\n",
      "\n",
      "        on_input_data.mock.assert_called_with(dict(DataBasic(data=0.2)))\n",
      "        on_output_data.mock.assert_not_called() # ERROR IN THIS LINE\n",
      "\n",
      "==== FIXED CODE ====\n",
      "\n",
      "    async with TestKafkaBroker(broker):\n",
      "        await broker.publish(DataBasic(data=0.2), \"input_data\")\n",
      "\n",
      "        on_input_data.mock.assert_called_with(dict(DataBasic(data=0.2)))\n",
      "        on_output_data.mock.assert_called_once_with(dict(DataBasic(data=1.2))) # ERROR FIXED IN THIS LINE\n",
      "\n",
      "==== Error ====\n",
      "\n",
      "pydantic_core.\\_pydantic_core.ValidationError: 1 validation error for ResponseModel\n",
      "\n",
      "==== ERROR CODE ====\n",
      "@broker.subscriber(\"output_data\")\n",
      "async def on_output_data(msg: DataBasic) -> DataBasic: # ERROR IN THIS LINE\n",
      "pass\n",
      "\n",
      "==== FIXED CODE ====\n",
      "\n",
      "@broker.subscriber(\"output_data\")\n",
      "async def on_output_data(msg: DataBasic): # ERROR FIXED IN THIS LINE\n",
      "pass\n",
      "\n",
      "\n",
      "==== APP DESCRIPTION ====\n",
      "\n",
      "Create a FastStream application using localhost broker for testing and use the default port number. It should consume messages from the \"input_data\" topic, where each message is a JSON encoded object containing a single attribute: 'data'. For each consumed message, create a new message object and increment the value of the data attribute by 1. Finally, send the modified message to the 'output_data' topic.\n",
      "\n",
      "\n",
      "==== APP SKELETON ====\n",
      "\n",
      "\n",
      "\n",
      "===Role:user===\n",
      "\n",
      "Message:\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "from faststream import FastStream, Logger\n",
      "from faststream.kafka import KafkaBroker\n",
      "\n",
      "\n",
      "class InputMessage(BaseModel):\n",
      "    data: int = Field(\n",
      "        ..., examples=[5], description=\"The data attribute in the input message\"\n",
      "    )\n",
      "\n",
      "\n",
      "class OutputMessage(BaseModel):\n",
      "    data: int = Field(\n",
      "        ..., examples=[6], description=\"The modified data attribute in the output message\"\n",
      "    )\n",
      "\n",
      "\n",
      "broker = KafkaBroker(\"localhost\")\n",
      "app = FastStream(broker)\n",
      "\n",
      "\n",
      "to_output_data = broker.publisher(\"output_data\")\n",
      "\n",
      "\n",
      "@broker.subscriber(\"input_data\")\n",
      "async def on_input_data(msg: InputMessage, logger: Logger) -> None:\n",
      "    \"\"\"\n",
      "    Processes a message from the 'input_data' topic.\n",
      "    Create a new message object and increment the value of the data attribute by 1.\n",
      "    Send the modified message to the 'output_data' topic.\n",
      "\n",
      "    Instructions:\n",
      "    1. Consume a message from 'input_data' topic.\n",
      "    2. Create a new message object (do not directly modify the original).\n",
      "    3. Increment the value of the data attribute by 1.\n",
      "    4. Send the modified message to the 'output_data' topic.\n",
      "    \"\"\"\n",
      "    raise NotImplementedError()\n",
      "==== YOUR RESPONSE ====\n",
      "\n",
      "\n",
      "===Role:user===\n",
      "\n",
      "Message:\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "from faststream import FastStream, Logger\n",
      "from faststream.kafka import KafkaBroker\n",
      "\n",
      "\n",
      "class InputMessage(BaseModel):\n",
      "    data: int = Field(\n",
      "        ..., examples=[5], description=\"The data attribute in the input message\"\n",
      "    )\n",
      "\n",
      "\n",
      "class OutputMessage(BaseModel):\n",
      "    data: int = Field(\n",
      "        ..., examples=[6], description=\"The modified data attribute in the output message\"\n",
      "    )\n",
      "\n",
      "\n",
      "broker = KafkaBroker(\"localhost\")\n",
      "app = FastStream(broker)\n",
      "\n",
      "\n",
      "to_output_data = broker.publisher(\"output_data\")\n",
      "\n",
      "\n",
      "@broker.subscriber(\"input_data\")\n",
      "async def on_input_data(msg: InputMessage, logger: Logger) -> None:\n",
      "    \"\"\"\n",
      "    Processes a message from the 'input_data' topic.\n",
      "    Create a new message object and increment the value of the data attribute by 1.\n",
      "    Send the modified message to the 'output_data' topic.\n",
      "\n",
      "    Instructions:\n",
      "    1. Consume a message from 'input_data' topic.\n",
      "    2. Create a new message object (do not directly modify the original).\n",
      "    3. Increment the value of the data attribute by 1.\n",
      "    4. Send the modified message to the 'output_data' topic.\n",
      "    \"\"\"\n",
      "    raise NotImplementedError()\n",
      "\n",
      "==== YOUR RESPONSE (WITH ISSUES) ====\n",
      "\n",
      "### application.py ###\n",
      "\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "from faststream import FastStream, Logger\n",
      "from faststream.kafka import KafkaBroker\n",
      "\n",
      "\n",
      "class InputMessage(BaseModel):\n",
      "    data: int = Field(\n",
      "        ..., examples=[5], description=\"The data attribute in the input message\"\n",
      "    )\n",
      "\n",
      "\n",
      "class OutputMessage(BaseModel):\n",
      "    data: int = Field(\n",
      "        ..., examples=[6], description=\"The modified data attribute in the output message\"\n",
      "    )\n",
      "\n",
      "\n",
      "broker = KafkaBroker(\"localhost\")\n",
      "app = FastStream(broker)\n",
      "\n",
      "\n",
      "to_output_data = broker.publisher(\"output_data\")\n",
      "\n",
      "\n",
      "@broker.subscriber(\"input_data\")\n",
      "async def on_input_data(msg: InputMessage, logger: Logger) -> None:\n",
      "    logger.info(f\"{msg=}\")\n",
      "    modified_msg = OutputMessage(data=msg.data + 1)\n",
      "    await to_output_data.publish(modified_msg)\n",
      "\n",
      "\n",
      "### test.py ###\n",
      "\n",
      "import pytest\n",
      "\n",
      "from faststream import Context\n",
      "from faststream.kafka import TestKafkaBroker\n",
      "\n",
      "from application import InputMessage, OutputMessage, app, broker, on_input_data\n",
      "\n",
      "\n",
      "@broker.subscriber(\"output_data\")\n",
      "async def on_output_data(msg: OutputMessage):\n",
      "    pass\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_modify_data():\n",
      "    async with TestKafkaBroker(broker):\n",
      "        async with app:\n",
      "            await broker.publish(InputMessage(data=5), \"input_data\")\n",
      "            on_input_data.mock.assert_called_with(dict(InputMessage(data=5)))\n",
      "            on_output_data.mock.assert_called_with(dict(OutputMessage(data=6)))\n",
      "\n",
      "Read the contents of ==== YOUR RESPONSE (WITH ISSUES) ==== section and fix the below mentioned issues:\n",
      "\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.11.5, pytest-7.4.2, pluggy-1.3.0\n",
      "rootdir: /tmp\n",
      "plugins: anyio-3.7.1, asyncio-0.21.1\n",
      "asyncio: mode=Mode.STRICT\n",
      "collected 1 item\n",
      "\n",
      "../tmp9by9fmhm/test.py \u001b[31mF\u001b[0m\u001b[31m                                                 [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_______________________________ test_modify_data _______________________________\u001b[0m\n",
      "\u001b[1m\u001b[31m../tmp9by9fmhm/test.py\u001b[0m:19: in test_modify_data\n",
      "    \u001b[94masync\u001b[39;49;00m \u001b[94mwith\u001b[39;49;00m app:\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE   TypeError: 'FastStream' object does not support the asynchronous context manager protocol\u001b[0m\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m ../tmp9by9fmhm/test.py::\u001b[1mtest_modify_data\u001b[0m - TypeError: 'FastStream' object does not support the asynchronous context ma...\n",
      "\u001b[31m============================== \u001b[31m\u001b[1m1 failed\u001b[0m\u001b[31m in 0.57s\u001b[0m\u001b[31m ===============================\u001b[0m\n",
      "\n",
      "==== YOUR RESPONSE ====\n",
      "\n",
      "23-09-21 23:57:48.833 [INFO] faststream_gen._code_generator.helper: ************************************************************************************************************************\n",
      "23-09-21 23:57:56.092 [INFO] faststream_gen._code_generator.helper: Validation failed, trying again...Errors:\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.11.5, pytest-7.4.2, pluggy-1.3.0\n",
      "rootdir: /tmp\n",
      "plugins: anyio-3.7.1, asyncio-0.21.1\n",
      "asyncio: mode=Mode.STRICT\n",
      "collected 1 item\n",
      "\n",
      "../tmp35h57gfe/test.py \u001b[31mF\u001b[0m\u001b[31m                                                 [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_______________________________ test_modify_data _______________________________\u001b[0m\n",
      "\u001b[1m\u001b[31m../tmp35h57gfe/test.py\u001b[0m:19: in test_modify_data\n",
      "    \u001b[94masync\u001b[39;49;00m \u001b[94mwith\u001b[39;49;00m app:\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE   TypeError: 'FastStream' object does not support the asynchronous context manager protocol\u001b[0m\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m ../tmp35h57gfe/test.py::\u001b[1mtest_modify_data\u001b[0m - TypeError: 'FastStream' object does not support the asynchronous context ma...\n",
      "\u001b[31m============================== \u001b[31m\u001b[1m1 failed\u001b[0m\u001b[31m in 0.57s\u001b[0m\u001b[31m ===============================\u001b[0m\n",
      "\n",
      "23-09-21 23:57:56.092 [INFO] faststream_gen._code_generator.helper: ************************************************************************************************************************\n",
      "23-09-21 23:57:56.092 [INFO] faststream_gen._code_generator.helper: \n",
      "\n",
      "Prompt to the model: \n",
      "\n",
      "===Role:system===\n",
      "\n",
      "Message:\n",
      "\n",
      "You are an expert Python developer, tasked to generate executable Python code as a part of your work with the FastStream framework. \n",
      "\n",
      "You are to abide by the following guidelines:\n",
      "\n",
      "1. You must never enclose the generated Python code with ``` python. It is mandatory that the output is a valid and executable Python code. Please ensure this rule is never broken.\n",
      "\n",
      "2. Some prompts might require you to generate code that contains async functions. For example:\n",
      "\n",
      "async def app_setup(context: ContextRepo):\n",
      "    raise NotImplementedError()\n",
      "\n",
      "In such cases, it is necessary to add the \"import asyncio\" statement at the top of the code. \n",
      "\n",
      "You will encounter sections marked as:\n",
      "\n",
      "==== APP DESCRIPTION: ====\n",
      "\n",
      "These sections contain the description of the FastStream app you need to implement. Treat everything below this line, until the end of the prompt, as the description to follow for the app implementation.\n",
      "\n",
      "\n",
      "===Role:user===\n",
      "\n",
      "Message:\n",
      "\n",
      "\n",
      "===Role:user===\n",
      "\n",
      "Message:\n",
      "\n",
      "You will be provided with a description about FastStream application in ==== APP DESCRIPTION ==== section and the skeleton code in ==== APP SKELETON ==== section. Your goal is to generate both the application and the test code based on the provided ==== APP SKELETON ==== section and the description ==== APP DESCRIPTION ==== in a single Python file. Your response will be split the application.py and the test.py files and saved to the disc.\n",
      "\n",
      "Input:\n",
      "\n",
      "You will be given skeleton code and the description for a FastStream application in ==== APP SKELETON ==== section and ==== APP DESCRIPTION ==== section respectively. The skeleton code and the description will have implementation details.\n",
      "\n",
      "Output:\n",
      "\n",
      "You need to understand the business logic mentioned in in ==== APP SKELETON ==== section and ==== APP DESCRIPTION ==== sections and generate the following:\n",
      "\n",
      "    - The application code by implementing the methods decorated with @broker.subscriber and @broker.publisher in the docstring\n",
      "    - The test code which tests the functionality of the generated application file\n",
      "\n",
      "You need to generate a single valid Python file containing both the application and the test code. Remember that the application and the test code will be split into two and saved to the disc as application.py and test.py. So while writing the test code, make sure to import all the necessary symbols from the application.py\n",
      "\n",
      "Application Code:\n",
      "\n",
      "Generate the entire application code from the provided skeleton. You must implement all of the business logic specified in the docstrings of the @broker.subscriber and @broker.publisher decorated functions in the skeleton code provided in the \"==== APP SKELETON ====\" section. When implementing business logic for methods decorated with @broker.subscriber and @broker.publisher, strictly follow the instructions in the docstring. Other parts of the code should not be changed.\n",
      "\n",
      "Test Code:\n",
      "\n",
      "Create test code using the TestBroker context managers to validate the functionality of the application code.\n",
      "\n",
      "The FastStream apps can be tested using the TestBroker context managers which, by default, puts the Broker into \"testing mode\". The Tester will redirect your subscriber and publisher decorated functions to the InMemory brokers so that you can quickly test your app without the need for a running broker and all its dependencies.\n",
      "\n",
      "While generating the application and the test code for FastStream application based on provided application code in the ==== APP SKELETON: ==== section, adhering to these guidelines:\n",
      "\n",
      "    - You need to generate the app code and the test code for the application code mentioned in ==== APP SKELETON: ==== in a single valid python file\n",
      "    - Follow the PEP 8 Style Guide for Python while writing the code\n",
      "    - Write optimised and readable Code\n",
      "    - Output only a valid executable python code. No other extra text should be included in your response.\n",
      "    - DO NOT enclose the response within back-ticks. Meaning NEVER ADD ```python to your response.\n",
      "    - Never try to explain and reason your answers. Only return a valid python code.\n",
      "    - Your response should be divided into two section, ### application.py ### which contains the application code and ### test.py ### which contains the test code.\n",
      "    - Remember, your response must have only two seperators ### application.py ### and ### test.py ###. DO NOT add any other separators (e.g. ### main.py ###).\n",
      "    - NEVER initialize publisher variable inside functions. You can use broker.publish function only in the test functions. In ALL OTHER functions, use publisher!\n",
      "    - ALWAYS initialize publisher variable (If you need to publish message) after defining app variable:\n",
      "\n",
      "        Example 1:\n",
      "        app = FastStream(broker)\n",
      "        publisher = broker.publisher(\"new_data\")\n",
      "\n",
      "        Example 2:\n",
      "        app = FastStream(broker)\n",
      "        new_data_publisher = broker.publisher(\"new_data\")\n",
      "\n",
      "Guidelines for writing tests: \n",
      "    - When a function is decorated @broker.publisher in the application code, remember that this function will always publish the message irrespective of the conditions mentioned in the \"==== APP SKELETON ====\" section. In such cases, never use mock.assert_not_called() while testing the function. For example:\n",
      "    - When you test a function in the application which is decorated with @broker.publisher(\"output_data\"), you should never use on_output_data.mock.assert_not_called() in your tests. Never ever break this rule. Because the decorator @broker.publisher will always publish the message irrespective of the conditions mentioned in the decorated function.\n",
      "\n",
      "Below are few examples for your understanding:\n",
      "\n",
      "\n",
      "==== EXAMPLE APP DESCRIPTION ====\n",
      "\n",
      "Develop a FastStream application using localhost kafka broker.\n",
      "The app should consume messages from the input_data topic.\n",
      "The input message is a JSON encoded object including two attributes:\n",
      "    - x: float\n",
      "    - y: float\n",
      "\n",
      "While consuming the message, increment x and y attributes by 1 and publish that message to the output_data topic.\n",
      "Use messages attribute x as a partition key when publishing to output_data topic.\n",
      "\n",
      "\n",
      "\n",
      "==== EXAMPLE APP SKELETON ====\n",
      "\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "from faststream import FastStream, Logger\n",
      "from faststream.kafka import KafkaBroker\n",
      "\n",
      "\n",
      "class Point(BaseModel):\n",
      "    x: float = Field(\n",
      "        ..., examples=[0.5], description=\"The X Coordinate in the coordinate system\"\n",
      "    )\n",
      "    y: float = Field(\n",
      "        ..., examples=[0.5], description=\"The Y Coordinate in the coordinate system\"\n",
      "    )\n",
      "\n",
      "\n",
      "broker = KafkaBroker(\"localhost:9092\")\n",
      "app = FastStream(broker)\n",
      "\n",
      "\n",
      "to_output_data = broker.publisher(\"output_data\")\n",
      "\n",
      "\n",
      "@broker.subscriber(\"input_data\")\n",
      "async def on_input_data(msg: Point, logger: Logger) -> None:\n",
      "    \"\"\"\n",
      "    Processes a message from the 'input_data' topic.\n",
      "    Increment msg x and y attributes with 1 and publish that message to the output_data topic.\n",
      "    Publish that message to the output_data topic\n",
      "    Use messages attribute x as a partition key when publishing to output_data topic.\n",
      "\n",
      "    Instructions:\n",
      "    1. Consume a message from 'input_data' topic.\n",
      "    2. Create a new message object (do not directly modify the original).\n",
      "    3. Increment msg x and y attributes with 1.\n",
      "    4. Publish that message to the output_data topic (Use messages attribute x as a partition key).\n",
      "    \"\"\"\n",
      "    raise NotImplementedError()\n",
      "\n",
      "\n",
      "==== YOUR RESPONSE ====\n",
      "\n",
      "### application.py ###\n",
      "\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "from faststream import FastStream, Logger\n",
      "from faststream.kafka import KafkaBroker\n",
      "\n",
      "\n",
      "class Point(BaseModel):\n",
      "    x: float = Field(\n",
      "        ..., examples=[0.5], description=\"The X Coordinate in the coordinate system\"\n",
      "    )\n",
      "    y: float = Field(\n",
      "        ..., examples=[0.5], description=\"The Y Coordinate in the coordinate system\"\n",
      "    )\n",
      "\n",
      "\n",
      "broker = KafkaBroker(\"localhost:9092\")\n",
      "app = FastStream(broker)\n",
      "\n",
      "\n",
      "to_output_data = broker.publisher(\"output_data\")\n",
      "\n",
      "\n",
      "@broker.subscriber(\"input_data\")\n",
      "async def on_input_data(msg: Point, logger: Logger) -> None:\n",
      "    logger.info(f\"{msg=}\")\n",
      "    incremented_point = Point(x=msg.x + 1, y=msg.y + 1)\n",
      "    key = str(msg.x).encode(\"utf-8\")\n",
      "    await to_output_data.publish(incremented_point, key=key)\n",
      "\n",
      "\n",
      "### test.py ###\n",
      "\n",
      "import pytest\n",
      "\n",
      "from faststream import Context\n",
      "from faststream.kafka import TestKafkaBroker\n",
      "\n",
      "from application import Point, broker, on_input_data\n",
      "\n",
      "\n",
      "@broker.subscriber(\"output_data\")\n",
      "async def on_output_data(msg: Point, key: bytes = Context(\"message.raw_message.key\")):\n",
      "    pass\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_point_was_incremented():\n",
      "    async with TestKafkaBroker(broker):\n",
      "        await broker.publish(Point(x=1.0, y=2.0), \"input_data\")\n",
      "        on_input_data.mock.assert_called_with(dict(Point(x=1.0, y=2.0)))\n",
      "        on_output_data.mock.assert_called_with(dict(Point(x=2.0, y=3.0)))\n",
      "\n",
      "\n",
      "==== EXAMPLE APP DESCRIPTION ====\n",
      "\n",
      "Develop a FastStream application using localhost kafka broker.\n",
      "The app should consume messages from the input_data topic.\n",
      "The input message is a JSON encoded object including two attributes:\n",
      "    - x: float\n",
      "    - y: float\n",
      "\n",
      "input_data topic should use partition key.\n",
      "While consuming the message, increment x and y attributes by 1 and publish that message to the output_data topic.\n",
      "The same partition key should be used in the input_data and output_data topic.\n",
      "\n",
      "\n",
      "\n",
      "==== EXAMPLE APP SKELETON ====\n",
      "\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "from faststream import Context, FastStream, Logger\n",
      "from faststream.kafka import KafkaBroker\n",
      "\n",
      "\n",
      "class Point(BaseModel):\n",
      "    x: float = Field(\n",
      "        ..., examples=[0.5], description=\"The X Coordinate in the coordinate system\"\n",
      "    )\n",
      "    y: float = Field(\n",
      "        ..., examples=[0.5], description=\"The Y Coordinate in the coordinate system\"\n",
      "    )\n",
      "\n",
      "\n",
      "broker = KafkaBroker(\"localhost:9092\")\n",
      "app = FastStream(broker)\n",
      "\n",
      "\n",
      "to_output_data = broker.publisher(\"output_data\")\n",
      "\n",
      "\n",
      "@broker.subscriber(\"input_data\")\n",
      "async def on_input_data(\n",
      "    msg: Point, logger: Logger, key: bytes = Context(\"message.raw_message.key\")\n",
      ") -> None:\n",
      "    \"\"\"\n",
      "    Processes a message from the 'input_data' topic.\n",
      "    Increment msg x and y attributes with 1 and publish that message to the output_data topic.\n",
      "    The same partition key should be used in the input_data and output_data topic.\n",
      "\n",
      "    Instructions:\n",
      "    1. Consume a message from 'input_data' topic.\n",
      "    2. Create a new message object (do not directly modify the original).\n",
      "    3. Increment msg x and y attributes with 1.\n",
      "    4. Publish that message to the output_data topic (The same partition key should be used in the input_data and output_data topic).\n",
      "    \"\"\"\n",
      "    raise NotImplementedError()\n",
      "\n",
      "\n",
      "==== YOUR RESPONSE ====\n",
      "\n",
      "### application.py ###\n",
      "\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "from faststream import Context, FastStream, Logger\n",
      "from faststream.kafka import KafkaBroker\n",
      "\n",
      "\n",
      "class Point(BaseModel):\n",
      "    x: float = Field(\n",
      "        ..., examples=[0.5], description=\"The X Coordinate in the coordinate system\"\n",
      "    )\n",
      "    y: float = Field(\n",
      "        ..., examples=[0.5], description=\"The Y Coordinate in the coordinate system\"\n",
      "    )\n",
      "\n",
      "\n",
      "broker = KafkaBroker(\"localhost:9092\")\n",
      "app = FastStream(broker)\n",
      "\n",
      "\n",
      "to_output_data = broker.publisher(\"output_data\")\n",
      "\n",
      "\n",
      "@broker.subscriber(\"input_data\")\n",
      "async def on_input_data(\n",
      "    msg: Point, logger: Logger, key: bytes = Context(\"message.raw_message.key\")\n",
      ") -> None:\n",
      "    logger.info(f\"{msg=}\")\n",
      "    incremented_point = Point(x=msg.x + 1, y=msg.y + 1)\n",
      "    await to_output_data.publish(incremented_point, key=key)\n",
      "\n",
      "\n",
      "### test.py ###\n",
      "\n",
      "import pytest\n",
      "\n",
      "from faststream import Context\n",
      "from faststream.kafka import TestKafkaBroker\n",
      "\n",
      "from application import Point, broker, on_input_data\n",
      "\n",
      "\n",
      "@broker.subscriber(\"output_data\")\n",
      "async def on_output_data(msg: Point, key: bytes = Context(\"message.raw_message.key\")):\n",
      "    pass\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_point_was_incremented():\n",
      "    async with TestKafkaBroker(broker):\n",
      "        await broker.publish(Point(x=1.0, y=2.0), \"input_data\", key=b\"key\")\n",
      "        on_input_data.mock.assert_called_with(dict(Point(x=1.0, y=2.0)))\n",
      "        on_output_data.mock.assert_called_with(dict(Point(x=2.0, y=3.0)))\n",
      "\n",
      "\n",
      "==== EXAMPLE APP DESCRIPTION ====\n",
      "\n",
      "Develop a FastStream application using localhost kafka broker.\n",
      "The app should consume messages from the input_data topic.\n",
      "The input message is a JSON encoded object including two attributes:\n",
      "    - x: float\n",
      "    - y: float\n",
      "\n",
      "input_data topic should use partition key.\n",
      "\n",
      "Keep all the previous messages in the memory.\n",
      "While consuming the message, add all x elements from the memory (x_sum) and all y from the memory (y_sum) and publish the message with x_sum and y_sum to the output_data topic.\n",
      "The same partition key should be used in the input_data and output_data topic.\n",
      "\n",
      "\n",
      "\n",
      "==== EXAMPLE APP SKELETON ====\n",
      "\n",
      "from typing import List\n",
      "\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "from faststream import Context, ContextRepo, FastStream, Logger\n",
      "from faststream.kafka import KafkaBroker\n",
      "\n",
      "\n",
      "class Point(BaseModel):\n",
      "    x: float = Field(\n",
      "        ..., examples=[0.5], description=\"The X Coordinate in the coordinate system\"\n",
      "    )\n",
      "    y: float = Field(\n",
      "        ..., examples=[0.5], description=\"The Y Coordinate in the coordinate system\"\n",
      "    )\n",
      "\n",
      "\n",
      "broker = KafkaBroker(\"localhost:9092\")\n",
      "app = FastStream(broker)\n",
      "\n",
      "\n",
      "to_output_data = broker.publisher(\"output_data\")\n",
      "\n",
      "\n",
      "@app.on_startup\n",
      "async def app_setup(context: ContextRepo):\n",
      "    \"\"\"\n",
      "    Set all necessary global variables inside ContextRepo object:\n",
      "        Set message_history for storing all input messages\n",
      "    \"\"\"\n",
      "    raise NotImplementedError()\n",
      "\n",
      "\n",
      "@broker.subscriber(\"input_data\")\n",
      "async def on_input_data(\n",
      "    msg: Point,\n",
      "    logger: Logger,\n",
      "    message_history: List[Point] = Context(),\n",
      "    key: bytes = Context(\"message.raw_message.key\"),\n",
      ") -> None:\n",
      "    \"\"\"\n",
      "    Processes a message from the 'input_data' topic.\n",
      "    Add all x elements from the memory (x_sum) and all y from the memory (y_sum) and publish the message with x_sum and y_sum to the output_data topic.\n",
      "    The same partition key should be used in the input_data and output_data topic.\n",
      "\n",
      "    Instructions:\n",
      "    1. Consume a message from 'input_data' topic.\n",
      "    2. Create a new message object (do not directly modify the original).\n",
      "    3. Add all x elements from the memory (x_sum) and all y from the memory (y_sum)\n",
      "    4. Publish the message with x_sum and y_sum to the output_data topic. (The same partition key should be used in the input_data and output_data topic).\n",
      "    \"\"\"\n",
      "    raise NotImplementedError()\n",
      "\n",
      "\n",
      "==== YOUR RESPONSE ====\n",
      "\n",
      "### application.py ###\n",
      "\n",
      "from typing import List\n",
      "\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "from faststream import Context, ContextRepo, FastStream, Logger\n",
      "from faststream.kafka import KafkaBroker\n",
      "\n",
      "\n",
      "class Point(BaseModel):\n",
      "    x: float = Field(\n",
      "        ..., examples=[0.5], description=\"The X Coordinate in the coordinate system\"\n",
      "    )\n",
      "    y: float = Field(\n",
      "        ..., examples=[0.5], description=\"The Y Coordinate in the coordinate system\"\n",
      "    )\n",
      "\n",
      "\n",
      "broker = KafkaBroker(\"localhost:9092\")\n",
      "app = FastStream(broker)\n",
      "\n",
      "\n",
      "to_output_data = broker.publisher(\"output_data\")\n",
      "\n",
      "\n",
      "@app.on_startup\n",
      "async def app_setup(context: ContextRepo):\n",
      "    message_history: List[Point] = []\n",
      "    context.set_global(\"message_history\", message_history)\n",
      "\n",
      "\n",
      "@broker.subscriber(\"input_data\")\n",
      "async def on_input_data(\n",
      "    msg: Point,\n",
      "    logger: Logger,\n",
      "    message_history: List[Point] = Context(),\n",
      "    key: bytes = Context(\"message.raw_message.key\"),\n",
      ") -> None:\n",
      "    logger.info(f\"{msg=}\")\n",
      "\n",
      "    message_history.append(msg)\n",
      "\n",
      "    x_sum = 0\n",
      "    y_sum = 0\n",
      "    for msg in message_history:\n",
      "        x_sum += msg.x\n",
      "        y_sum += msg.y\n",
      "\n",
      "    point_sum = Point(x=x_sum, y=y_sum)\n",
      "    await to_output_data.publish(point_sum, key=key)\n",
      "\n",
      "\n",
      "### test.py ###\n",
      "\n",
      "import pytest\n",
      "\n",
      "from faststream import Context, TestApp\n",
      "from faststream.kafka import TestKafkaBroker\n",
      "\n",
      "from application import Point, app, broker\n",
      "\n",
      "\n",
      "@broker.subscriber(\"output_data\")\n",
      "async def on_output_data(msg: Point, key: bytes = Context(\"message.raw_message.key\")):\n",
      "    pass\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_point_was_incremented():\n",
      "    async with TestKafkaBroker(broker):\n",
      "        async with TestApp(app):\n",
      "            await broker.publish(Point(x=1.0, y=2.0), \"input_data\", key=b\"point_key\")\n",
      "            await broker.publish(Point(x=1.0, y=2.0), \"input_data\", key=b\"point_key\")\n",
      "\n",
      "            on_output_data.mock.assert_called_with(dict(Point(x=2.0, y=4.0)))\n",
      "\n",
      "\n",
      "\n",
      "You need to avoid the below common issues while generating the test code. The potential fixes for the common issues are also given for your reference. You should use this information as an additional reference while generating the test:\n",
      "\n",
      "==== Error ====\n",
      "\n",
      "AssertionError: Expected 'mock' to not have been called. Called 2 times.\n",
      "\n",
      "==== ERROR CODE ====\n",
      "async with TestKafkaBroker(broker):\n",
      "await broker.publish(DataBasic(data=0.2), \"input_data\")\n",
      "\n",
      "        on_input_data.mock.assert_called_with(dict(DataBasic(data=0.2)))\n",
      "        on_output_data.mock.assert_not_called() # ERROR IN THIS LINE\n",
      "\n",
      "==== FIXED CODE ====\n",
      "\n",
      "    async with TestKafkaBroker(broker):\n",
      "        await broker.publish(DataBasic(data=0.2), \"input_data\")\n",
      "\n",
      "        on_input_data.mock.assert_called_with(dict(DataBasic(data=0.2)))\n",
      "        on_output_data.mock.assert_called_once_with(dict(DataBasic(data=1.2))) # ERROR FIXED IN THIS LINE\n",
      "\n",
      "==== Error ====\n",
      "\n",
      "pydantic_core.\\_pydantic_core.ValidationError: 1 validation error for ResponseModel\n",
      "\n",
      "==== ERROR CODE ====\n",
      "@broker.subscriber(\"output_data\")\n",
      "async def on_output_data(msg: DataBasic) -> DataBasic: # ERROR IN THIS LINE\n",
      "pass\n",
      "\n",
      "==== FIXED CODE ====\n",
      "\n",
      "@broker.subscriber(\"output_data\")\n",
      "async def on_output_data(msg: DataBasic): # ERROR FIXED IN THIS LINE\n",
      "pass\n",
      "\n",
      "\n",
      "==== APP DESCRIPTION ====\n",
      "\n",
      "Create a FastStream application using localhost broker for testing and use the default port number. It should consume messages from the \"input_data\" topic, where each message is a JSON encoded object containing a single attribute: 'data'. For each consumed message, create a new message object and increment the value of the data attribute by 1. Finally, send the modified message to the 'output_data' topic.\n",
      "\n",
      "\n",
      "==== APP SKELETON ====\n",
      "\n",
      "\n",
      "\n",
      "===Role:user===\n",
      "\n",
      "Message:\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "from faststream import FastStream, Logger\n",
      "from faststream.kafka import KafkaBroker\n",
      "\n",
      "\n",
      "class InputMessage(BaseModel):\n",
      "    data: int = Field(\n",
      "        ..., examples=[5], description=\"The data attribute in the input message\"\n",
      "    )\n",
      "\n",
      "\n",
      "class OutputMessage(BaseModel):\n",
      "    data: int = Field(\n",
      "        ..., examples=[6], description=\"The modified data attribute in the output message\"\n",
      "    )\n",
      "\n",
      "\n",
      "broker = KafkaBroker(\"localhost\")\n",
      "app = FastStream(broker)\n",
      "\n",
      "\n",
      "to_output_data = broker.publisher(\"output_data\")\n",
      "\n",
      "\n",
      "@broker.subscriber(\"input_data\")\n",
      "async def on_input_data(msg: InputMessage, logger: Logger) -> None:\n",
      "    \"\"\"\n",
      "    Processes a message from the 'input_data' topic.\n",
      "    Create a new message object and increment the value of the data attribute by 1.\n",
      "    Send the modified message to the 'output_data' topic.\n",
      "\n",
      "    Instructions:\n",
      "    1. Consume a message from 'input_data' topic.\n",
      "    2. Create a new message object (do not directly modify the original).\n",
      "    3. Increment the value of the data attribute by 1.\n",
      "    4. Send the modified message to the 'output_data' topic.\n",
      "    \"\"\"\n",
      "    raise NotImplementedError()\n",
      "==== YOUR RESPONSE ====\n",
      "\n",
      "\n",
      "===Role:user===\n",
      "\n",
      "Message:\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "from faststream import FastStream, Logger\n",
      "from faststream.kafka import KafkaBroker\n",
      "\n",
      "\n",
      "class InputMessage(BaseModel):\n",
      "    data: int = Field(\n",
      "        ..., examples=[5], description=\"The data attribute in the input message\"\n",
      "    )\n",
      "\n",
      "\n",
      "class OutputMessage(BaseModel):\n",
      "    data: int = Field(\n",
      "        ..., examples=[6], description=\"The modified data attribute in the output message\"\n",
      "    )\n",
      "\n",
      "\n",
      "broker = KafkaBroker(\"localhost\")\n",
      "app = FastStream(broker)\n",
      "\n",
      "\n",
      "to_output_data = broker.publisher(\"output_data\")\n",
      "\n",
      "\n",
      "@broker.subscriber(\"input_data\")\n",
      "async def on_input_data(msg: InputMessage, logger: Logger) -> None:\n",
      "    \"\"\"\n",
      "    Processes a message from the 'input_data' topic.\n",
      "    Create a new message object and increment the value of the data attribute by 1.\n",
      "    Send the modified message to the 'output_data' topic.\n",
      "\n",
      "    Instructions:\n",
      "    1. Consume a message from 'input_data' topic.\n",
      "    2. Create a new message object (do not directly modify the original).\n",
      "    3. Increment the value of the data attribute by 1.\n",
      "    4. Send the modified message to the 'output_data' topic.\n",
      "    \"\"\"\n",
      "    raise NotImplementedError()\n",
      "\n",
      "==== YOUR RESPONSE (WITH ISSUES) ====\n",
      "\n",
      "### application.py ###\n",
      "\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "from faststream import FastStream, Logger\n",
      "from faststream.kafka import KafkaBroker\n",
      "\n",
      "\n",
      "class InputMessage(BaseModel):\n",
      "    data: int = Field(\n",
      "        ..., examples=[5], description=\"The data attribute in the input message\"\n",
      "    )\n",
      "\n",
      "\n",
      "class OutputMessage(BaseModel):\n",
      "    data: int = Field(\n",
      "        ..., examples=[6], description=\"The modified data attribute in the output message\"\n",
      "    )\n",
      "\n",
      "\n",
      "broker = KafkaBroker(\"localhost\")\n",
      "app = FastStream(broker)\n",
      "\n",
      "\n",
      "to_output_data = broker.publisher(\"output_data\")\n",
      "\n",
      "\n",
      "@broker.subscriber(\"input_data\")\n",
      "async def on_input_data(msg: InputMessage, logger: Logger) -> None:\n",
      "    logger.info(f\"{msg=}\")\n",
      "    modified_msg = OutputMessage(data=msg.data + 1)\n",
      "    await to_output_data.publish(modified_msg)\n",
      "\n",
      "\n",
      "### test.py ###\n",
      "\n",
      "import pytest\n",
      "\n",
      "from faststream import Context\n",
      "from faststream.kafka import TestKafkaBroker\n",
      "\n",
      "from application import InputMessage, OutputMessage, app, broker, on_input_data\n",
      "\n",
      "\n",
      "@broker.subscriber(\"output_data\")\n",
      "async def on_output_data(msg: OutputMessage):\n",
      "    pass\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_modify_data():\n",
      "    async with TestKafkaBroker(broker):\n",
      "        async with app:\n",
      "            await broker.publish(InputMessage(data=5), \"input_data\")\n",
      "            on_input_data.mock.assert_called_with(dict(InputMessage(data=5)))\n",
      "            on_output_data.mock.assert_called_with(dict(OutputMessage(data=6)))\n",
      "\n",
      "Read the contents of ==== YOUR RESPONSE (WITH ISSUES) ==== section and fix the below mentioned issues:\n",
      "\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.11.5, pytest-7.4.2, pluggy-1.3.0\n",
      "rootdir: /tmp\n",
      "plugins: anyio-3.7.1, asyncio-0.21.1\n",
      "asyncio: mode=Mode.STRICT\n",
      "collected 1 item\n",
      "\n",
      "../tmp9by9fmhm/test.py \u001b[31mF\u001b[0m\u001b[31m                                                 [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_______________________________ test_modify_data _______________________________\u001b[0m\n",
      "\u001b[1m\u001b[31m../tmp9by9fmhm/test.py\u001b[0m:19: in test_modify_data\n",
      "    \u001b[94masync\u001b[39;49;00m \u001b[94mwith\u001b[39;49;00m app:\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE   TypeError: 'FastStream' object does not support the asynchronous context manager protocol\u001b[0m\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m ../tmp9by9fmhm/test.py::\u001b[1mtest_modify_data\u001b[0m - TypeError: 'FastStream' object does not support the asynchronous context ma...\n",
      "\u001b[31m============================== \u001b[31m\u001b[1m1 failed\u001b[0m\u001b[31m in 0.57s\u001b[0m\u001b[31m ===============================\u001b[0m\n",
      "\n",
      "==== YOUR RESPONSE ====\n",
      "\n",
      "\n",
      "===Role:user===\n",
      "\n",
      "Message:\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "from faststream import FastStream, Logger\n",
      "from faststream.kafka import KafkaBroker\n",
      "\n",
      "\n",
      "class InputMessage(BaseModel):\n",
      "    data: int = Field(\n",
      "        ..., examples=[5], description=\"The data attribute in the input message\"\n",
      "    )\n",
      "\n",
      "\n",
      "class OutputMessage(BaseModel):\n",
      "    data: int = Field(\n",
      "        ..., examples=[6], description=\"The modified data attribute in the output message\"\n",
      "    )\n",
      "\n",
      "\n",
      "broker = KafkaBroker(\"localhost\")\n",
      "app = FastStream(broker)\n",
      "\n",
      "\n",
      "to_output_data = broker.publisher(\"output_data\")\n",
      "\n",
      "\n",
      "@broker.subscriber(\"input_data\")\n",
      "async def on_input_data(msg: InputMessage, logger: Logger) -> None:\n",
      "    \"\"\"\n",
      "    Processes a message from the 'input_data' topic.\n",
      "    Create a new message object and increment the value of the data attribute by 1.\n",
      "    Send the modified message to the 'output_data' topic.\n",
      "\n",
      "    Instructions:\n",
      "    1. Consume a message from 'input_data' topic.\n",
      "    2. Create a new message object (do not directly modify the original).\n",
      "    3. Increment the value of the data attribute by 1.\n",
      "    4. Send the modified message to the 'output_data' topic.\n",
      "    \"\"\"\n",
      "    raise NotImplementedError()\n",
      "\n",
      "==== YOUR RESPONSE (WITH ISSUES) ====\n",
      "\n",
      "### application.py ###\n",
      "\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "from faststream import FastStream, Logger\n",
      "from faststream.kafka import KafkaBroker\n",
      "\n",
      "\n",
      "class InputMessage(BaseModel):\n",
      "    data: int = Field(\n",
      "        ..., examples=[5], description=\"The data attribute in the input message\"\n",
      "    )\n",
      "\n",
      "\n",
      "class OutputMessage(BaseModel):\n",
      "    data: int = Field(\n",
      "        ..., examples=[6], description=\"The modified data attribute in the output message\"\n",
      "    )\n",
      "\n",
      "\n",
      "broker = KafkaBroker(\"localhost\")\n",
      "app = FastStream(broker)\n",
      "\n",
      "\n",
      "to_output_data = broker.publisher(\"output_data\")\n",
      "\n",
      "\n",
      "@broker.subscriber(\"input_data\")\n",
      "async def on_input_data(msg: InputMessage, logger: Logger) -> None:\n",
      "    logger.info(f\"{msg=}\")\n",
      "    modified_msg = OutputMessage(data=msg.data + 1)\n",
      "    await to_output_data.publish(modified_msg)\n",
      "\n",
      "\n",
      "### test.py ###\n",
      "\n",
      "import pytest\n",
      "\n",
      "from faststream import Context\n",
      "from faststream.kafka import TestKafkaBroker\n",
      "\n",
      "from application import InputMessage, OutputMessage, app, broker, on_input_data\n",
      "\n",
      "\n",
      "@broker.subscriber(\"output_data\")\n",
      "async def on_output_data(msg: OutputMessage):\n",
      "    pass\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_modify_data():\n",
      "    async with TestKafkaBroker(broker):\n",
      "        async with app:\n",
      "            await broker.publish(InputMessage(data=5), \"input_data\")\n",
      "            on_input_data.mock.assert_called_with(dict(InputMessage(data=5)))\n",
      "            on_output_data.mock.assert_called_with(dict(OutputMessage(data=6)))\n",
      "\n",
      "Read the contents of ==== YOUR RESPONSE (WITH ISSUES) ==== section and fix the below mentioned issues:\n",
      "\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.11.5, pytest-7.4.2, pluggy-1.3.0\n",
      "rootdir: /tmp\n",
      "plugins: anyio-3.7.1, asyncio-0.21.1\n",
      "asyncio: mode=Mode.STRICT\n",
      "collected 1 item\n",
      "\n",
      "../tmp35h57gfe/test.py \u001b[31mF\u001b[0m\u001b[31m                                                 [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_______________________________ test_modify_data _______________________________\u001b[0m\n",
      "\u001b[1m\u001b[31m../tmp35h57gfe/test.py\u001b[0m:19: in test_modify_data\n",
      "    \u001b[94masync\u001b[39;49;00m \u001b[94mwith\u001b[39;49;00m app:\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE   TypeError: 'FastStream' object does not support the asynchronous context manager protocol\u001b[0m\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m ../tmp35h57gfe/test.py::\u001b[1mtest_modify_data\u001b[0m - TypeError: 'FastStream' object does not support the asynchronous context ma...\n",
      "\u001b[31m============================== \u001b[31m\u001b[1m1 failed\u001b[0m\u001b[31m in 0.57s\u001b[0m\u001b[31m ===============================\u001b[0m\n",
      "\n",
      "==== YOUR RESPONSE ====\n",
      "\n",
      "23-09-21 23:57:56.092 [INFO] faststream_gen._code_generator.helper: ************************************************************************************************************************\n",
      "23-09-21 23:58:02.684 [INFO] faststream_gen._code_generator.helper: Validation failed, trying again...Errors:\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.11.5, pytest-7.4.2, pluggy-1.3.0\n",
      "rootdir: /tmp\n",
      "plugins: anyio-3.7.1, asyncio-0.21.1\n",
      "asyncio: mode=Mode.STRICT\n",
      "collected 1 item\n",
      "\n",
      "../tmpcup4tath/test.py \u001b[31mF\u001b[0m\u001b[31m                                                 [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_______________________________ test_modify_data _______________________________\u001b[0m\n",
      "\u001b[1m\u001b[31m../tmpcup4tath/test.py\u001b[0m:19: in test_modify_data\n",
      "    \u001b[94masync\u001b[39;49;00m \u001b[94mwith\u001b[39;49;00m app:\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE   TypeError: 'FastStream' object does not support the asynchronous context manager protocol\u001b[0m\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m ../tmpcup4tath/test.py::\u001b[1mtest_modify_data\u001b[0m - TypeError: 'FastStream' object does not support the asynchronous context ma...\n",
      "\u001b[31m============================== \u001b[31m\u001b[1m1 failed\u001b[0m\u001b[31m in 0.56s\u001b[0m\u001b[31m ===============================\u001b[0m\n",
      "\n",
      "23-09-21 23:58:02.684 [INFO] faststream_gen._code_generator.helper: Attempt 0 failed. Restarting step.\n",
      "23-09-21 23:58:03.684 [INFO] faststream_gen._code_generator.helper: ************************************************************************************************************************\n",
      "23-09-21 23:58:03.684 [INFO] faststream_gen._code_generator.helper: \n",
      "\n",
      "Prompt to the model: \n",
      "\n",
      "===Role:system===\n",
      "\n",
      "Message:\n",
      "\n",
      "You are an expert Python developer, tasked to generate executable Python code as a part of your work with the FastStream framework. \n",
      "\n",
      "You are to abide by the following guidelines:\n",
      "\n",
      "1. You must never enclose the generated Python code with ``` python. It is mandatory that the output is a valid and executable Python code. Please ensure this rule is never broken.\n",
      "\n",
      "2. Some prompts might require you to generate code that contains async functions. For example:\n",
      "\n",
      "async def app_setup(context: ContextRepo):\n",
      "    raise NotImplementedError()\n",
      "\n",
      "In such cases, it is necessary to add the \"import asyncio\" statement at the top of the code. \n",
      "\n",
      "You will encounter sections marked as:\n",
      "\n",
      "==== APP DESCRIPTION: ====\n",
      "\n",
      "These sections contain the description of the FastStream app you need to implement. Treat everything below this line, until the end of the prompt, as the description to follow for the app implementation.\n",
      "\n",
      "\n",
      "===Role:user===\n",
      "\n",
      "Message:\n",
      "\n",
      "\n",
      "===Role:user===\n",
      "\n",
      "Message:\n",
      "\n",
      "You will be provided with a description about FastStream application in ==== APP DESCRIPTION ==== section and the skeleton code in ==== APP SKELETON ==== section. Your goal is to generate both the application and the test code based on the provided ==== APP SKELETON ==== section and the description ==== APP DESCRIPTION ==== in a single Python file. Your response will be split the application.py and the test.py files and saved to the disc.\n",
      "\n",
      "Input:\n",
      "\n",
      "You will be given skeleton code and the description for a FastStream application in ==== APP SKELETON ==== section and ==== APP DESCRIPTION ==== section respectively. The skeleton code and the description will have implementation details.\n",
      "\n",
      "Output:\n",
      "\n",
      "You need to understand the business logic mentioned in in ==== APP SKELETON ==== section and ==== APP DESCRIPTION ==== sections and generate the following:\n",
      "\n",
      "    - The application code by implementing the methods decorated with @broker.subscriber and @broker.publisher in the docstring\n",
      "    - The test code which tests the functionality of the generated application file\n",
      "\n",
      "You need to generate a single valid Python file containing both the application and the test code. Remember that the application and the test code will be split into two and saved to the disc as application.py and test.py. So while writing the test code, make sure to import all the necessary symbols from the application.py\n",
      "\n",
      "Application Code:\n",
      "\n",
      "Generate the entire application code from the provided skeleton. You must implement all of the business logic specified in the docstrings of the @broker.subscriber and @broker.publisher decorated functions in the skeleton code provided in the \"==== APP SKELETON ====\" section. When implementing business logic for methods decorated with @broker.subscriber and @broker.publisher, strictly follow the instructions in the docstring. Other parts of the code should not be changed.\n",
      "\n",
      "Test Code:\n",
      "\n",
      "Create test code using the TestBroker context managers to validate the functionality of the application code.\n",
      "\n",
      "The FastStream apps can be tested using the TestBroker context managers which, by default, puts the Broker into \"testing mode\". The Tester will redirect your subscriber and publisher decorated functions to the InMemory brokers so that you can quickly test your app without the need for a running broker and all its dependencies.\n",
      "\n",
      "While generating the application and the test code for FastStream application based on provided application code in the ==== APP SKELETON: ==== section, adhering to these guidelines:\n",
      "\n",
      "    - You need to generate the app code and the test code for the application code mentioned in ==== APP SKELETON: ==== in a single valid python file\n",
      "    - Follow the PEP 8 Style Guide for Python while writing the code\n",
      "    - Write optimised and readable Code\n",
      "    - Output only a valid executable python code. No other extra text should be included in your response.\n",
      "    - DO NOT enclose the response within back-ticks. Meaning NEVER ADD ```python to your response.\n",
      "    - Never try to explain and reason your answers. Only return a valid python code.\n",
      "    - Your response should be divided into two section, ### application.py ### which contains the application code and ### test.py ### which contains the test code.\n",
      "    - Remember, your response must have only two seperators ### application.py ### and ### test.py ###. DO NOT add any other separators (e.g. ### main.py ###).\n",
      "    - NEVER initialize publisher variable inside functions. You can use broker.publish function only in the test functions. In ALL OTHER functions, use publisher!\n",
      "    - ALWAYS initialize publisher variable (If you need to publish message) after defining app variable:\n",
      "\n",
      "        Example 1:\n",
      "        app = FastStream(broker)\n",
      "        publisher = broker.publisher(\"new_data\")\n",
      "\n",
      "        Example 2:\n",
      "        app = FastStream(broker)\n",
      "        new_data_publisher = broker.publisher(\"new_data\")\n",
      "\n",
      "Guidelines for writing tests: \n",
      "    - When a function is decorated @broker.publisher in the application code, remember that this function will always publish the message irrespective of the conditions mentioned in the \"==== APP SKELETON ====\" section. In such cases, never use mock.assert_not_called() while testing the function. For example:\n",
      "    - When you test a function in the application which is decorated with @broker.publisher(\"output_data\"), you should never use on_output_data.mock.assert_not_called() in your tests. Never ever break this rule. Because the decorator @broker.publisher will always publish the message irrespective of the conditions mentioned in the decorated function.\n",
      "\n",
      "Below are few examples for your understanding:\n",
      "\n",
      "\n",
      "==== EXAMPLE APP DESCRIPTION ====\n",
      "\n",
      "Develop a FastStream application using localhost kafka broker.\n",
      "The app should consume messages from the input_data topic.\n",
      "The input message is a JSON encoded object including two attributes:\n",
      "    - x: float\n",
      "    - y: float\n",
      "\n",
      "While consuming the message, increment x and y attributes by 1 and publish that message to the output_data topic.\n",
      "Use messages attribute x as a partition key when publishing to output_data topic.\n",
      "\n",
      "\n",
      "\n",
      "==== EXAMPLE APP SKELETON ====\n",
      "\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "from faststream import FastStream, Logger\n",
      "from faststream.kafka import KafkaBroker\n",
      "\n",
      "\n",
      "class Point(BaseModel):\n",
      "    x: float = Field(\n",
      "        ..., examples=[0.5], description=\"The X Coordinate in the coordinate system\"\n",
      "    )\n",
      "    y: float = Field(\n",
      "        ..., examples=[0.5], description=\"The Y Coordinate in the coordinate system\"\n",
      "    )\n",
      "\n",
      "\n",
      "broker = KafkaBroker(\"localhost:9092\")\n",
      "app = FastStream(broker)\n",
      "\n",
      "\n",
      "to_output_data = broker.publisher(\"output_data\")\n",
      "\n",
      "\n",
      "@broker.subscriber(\"input_data\")\n",
      "async def on_input_data(msg: Point, logger: Logger) -> None:\n",
      "    \"\"\"\n",
      "    Processes a message from the 'input_data' topic.\n",
      "    Increment msg x and y attributes with 1 and publish that message to the output_data topic.\n",
      "    Publish that message to the output_data topic\n",
      "    Use messages attribute x as a partition key when publishing to output_data topic.\n",
      "\n",
      "    Instructions:\n",
      "    1. Consume a message from 'input_data' topic.\n",
      "    2. Create a new message object (do not directly modify the original).\n",
      "    3. Increment msg x and y attributes with 1.\n",
      "    4. Publish that message to the output_data topic (Use messages attribute x as a partition key).\n",
      "    \"\"\"\n",
      "    raise NotImplementedError()\n",
      "\n",
      "\n",
      "==== YOUR RESPONSE ====\n",
      "\n",
      "### application.py ###\n",
      "\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "from faststream import FastStream, Logger\n",
      "from faststream.kafka import KafkaBroker\n",
      "\n",
      "\n",
      "class Point(BaseModel):\n",
      "    x: float = Field(\n",
      "        ..., examples=[0.5], description=\"The X Coordinate in the coordinate system\"\n",
      "    )\n",
      "    y: float = Field(\n",
      "        ..., examples=[0.5], description=\"The Y Coordinate in the coordinate system\"\n",
      "    )\n",
      "\n",
      "\n",
      "broker = KafkaBroker(\"localhost:9092\")\n",
      "app = FastStream(broker)\n",
      "\n",
      "\n",
      "to_output_data = broker.publisher(\"output_data\")\n",
      "\n",
      "\n",
      "@broker.subscriber(\"input_data\")\n",
      "async def on_input_data(msg: Point, logger: Logger) -> None:\n",
      "    logger.info(f\"{msg=}\")\n",
      "    incremented_point = Point(x=msg.x + 1, y=msg.y + 1)\n",
      "    key = str(msg.x).encode(\"utf-8\")\n",
      "    await to_output_data.publish(incremented_point, key=key)\n",
      "\n",
      "\n",
      "### test.py ###\n",
      "\n",
      "import pytest\n",
      "\n",
      "from faststream import Context\n",
      "from faststream.kafka import TestKafkaBroker\n",
      "\n",
      "from application import Point, broker, on_input_data\n",
      "\n",
      "\n",
      "@broker.subscriber(\"output_data\")\n",
      "async def on_output_data(msg: Point, key: bytes = Context(\"message.raw_message.key\")):\n",
      "    pass\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_point_was_incremented():\n",
      "    async with TestKafkaBroker(broker):\n",
      "        await broker.publish(Point(x=1.0, y=2.0), \"input_data\")\n",
      "        on_input_data.mock.assert_called_with(dict(Point(x=1.0, y=2.0)))\n",
      "        on_output_data.mock.assert_called_with(dict(Point(x=2.0, y=3.0)))\n",
      "\n",
      "\n",
      "==== EXAMPLE APP DESCRIPTION ====\n",
      "\n",
      "Develop a FastStream application using localhost kafka broker.\n",
      "The app should consume messages from the input_data topic.\n",
      "The input message is a JSON encoded object including two attributes:\n",
      "    - x: float\n",
      "    - y: float\n",
      "\n",
      "input_data topic should use partition key.\n",
      "While consuming the message, increment x and y attributes by 1 and publish that message to the output_data topic.\n",
      "The same partition key should be used in the input_data and output_data topic.\n",
      "\n",
      "\n",
      "\n",
      "==== EXAMPLE APP SKELETON ====\n",
      "\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "from faststream import Context, FastStream, Logger\n",
      "from faststream.kafka import KafkaBroker\n",
      "\n",
      "\n",
      "class Point(BaseModel):\n",
      "    x: float = Field(\n",
      "        ..., examples=[0.5], description=\"The X Coordinate in the coordinate system\"\n",
      "    )\n",
      "    y: float = Field(\n",
      "        ..., examples=[0.5], description=\"The Y Coordinate in the coordinate system\"\n",
      "    )\n",
      "\n",
      "\n",
      "broker = KafkaBroker(\"localhost:9092\")\n",
      "app = FastStream(broker)\n",
      "\n",
      "\n",
      "to_output_data = broker.publisher(\"output_data\")\n",
      "\n",
      "\n",
      "@broker.subscriber(\"input_data\")\n",
      "async def on_input_data(\n",
      "    msg: Point, logger: Logger, key: bytes = Context(\"message.raw_message.key\")\n",
      ") -> None:\n",
      "    \"\"\"\n",
      "    Processes a message from the 'input_data' topic.\n",
      "    Increment msg x and y attributes with 1 and publish that message to the output_data topic.\n",
      "    The same partition key should be used in the input_data and output_data topic.\n",
      "\n",
      "    Instructions:\n",
      "    1. Consume a message from 'input_data' topic.\n",
      "    2. Create a new message object (do not directly modify the original).\n",
      "    3. Increment msg x and y attributes with 1.\n",
      "    4. Publish that message to the output_data topic (The same partition key should be used in the input_data and output_data topic).\n",
      "    \"\"\"\n",
      "    raise NotImplementedError()\n",
      "\n",
      "\n",
      "==== YOUR RESPONSE ====\n",
      "\n",
      "### application.py ###\n",
      "\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "from faststream import Context, FastStream, Logger\n",
      "from faststream.kafka import KafkaBroker\n",
      "\n",
      "\n",
      "class Point(BaseModel):\n",
      "    x: float = Field(\n",
      "        ..., examples=[0.5], description=\"The X Coordinate in the coordinate system\"\n",
      "    )\n",
      "    y: float = Field(\n",
      "        ..., examples=[0.5], description=\"The Y Coordinate in the coordinate system\"\n",
      "    )\n",
      "\n",
      "\n",
      "broker = KafkaBroker(\"localhost:9092\")\n",
      "app = FastStream(broker)\n",
      "\n",
      "\n",
      "to_output_data = broker.publisher(\"output_data\")\n",
      "\n",
      "\n",
      "@broker.subscriber(\"input_data\")\n",
      "async def on_input_data(\n",
      "    msg: Point, logger: Logger, key: bytes = Context(\"message.raw_message.key\")\n",
      ") -> None:\n",
      "    logger.info(f\"{msg=}\")\n",
      "    incremented_point = Point(x=msg.x + 1, y=msg.y + 1)\n",
      "    await to_output_data.publish(incremented_point, key=key)\n",
      "\n",
      "\n",
      "### test.py ###\n",
      "\n",
      "import pytest\n",
      "\n",
      "from faststream import Context\n",
      "from faststream.kafka import TestKafkaBroker\n",
      "\n",
      "from application import Point, broker, on_input_data\n",
      "\n",
      "\n",
      "@broker.subscriber(\"output_data\")\n",
      "async def on_output_data(msg: Point, key: bytes = Context(\"message.raw_message.key\")):\n",
      "    pass\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_point_was_incremented():\n",
      "    async with TestKafkaBroker(broker):\n",
      "        await broker.publish(Point(x=1.0, y=2.0), \"input_data\", key=b\"key\")\n",
      "        on_input_data.mock.assert_called_with(dict(Point(x=1.0, y=2.0)))\n",
      "        on_output_data.mock.assert_called_with(dict(Point(x=2.0, y=3.0)))\n",
      "\n",
      "\n",
      "==== EXAMPLE APP DESCRIPTION ====\n",
      "\n",
      "Develop a FastStream application using localhost kafka broker.\n",
      "The app should consume messages from the input_data topic.\n",
      "The input message is a JSON encoded object including two attributes:\n",
      "    - x: float\n",
      "    - y: float\n",
      "\n",
      "input_data topic should use partition key.\n",
      "\n",
      "Keep all the previous messages in the memory.\n",
      "While consuming the message, add all x elements from the memory (x_sum) and all y from the memory (y_sum) and publish the message with x_sum and y_sum to the output_data topic.\n",
      "The same partition key should be used in the input_data and output_data topic.\n",
      "\n",
      "\n",
      "\n",
      "==== EXAMPLE APP SKELETON ====\n",
      "\n",
      "from typing import List\n",
      "\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "from faststream import Context, ContextRepo, FastStream, Logger\n",
      "from faststream.kafka import KafkaBroker\n",
      "\n",
      "\n",
      "class Point(BaseModel):\n",
      "    x: float = Field(\n",
      "        ..., examples=[0.5], description=\"The X Coordinate in the coordinate system\"\n",
      "    )\n",
      "    y: float = Field(\n",
      "        ..., examples=[0.5], description=\"The Y Coordinate in the coordinate system\"\n",
      "    )\n",
      "\n",
      "\n",
      "broker = KafkaBroker(\"localhost:9092\")\n",
      "app = FastStream(broker)\n",
      "\n",
      "\n",
      "to_output_data = broker.publisher(\"output_data\")\n",
      "\n",
      "\n",
      "@app.on_startup\n",
      "async def app_setup(context: ContextRepo):\n",
      "    \"\"\"\n",
      "    Set all necessary global variables inside ContextRepo object:\n",
      "        Set message_history for storing all input messages\n",
      "    \"\"\"\n",
      "    raise NotImplementedError()\n",
      "\n",
      "\n",
      "@broker.subscriber(\"input_data\")\n",
      "async def on_input_data(\n",
      "    msg: Point,\n",
      "    logger: Logger,\n",
      "    message_history: List[Point] = Context(),\n",
      "    key: bytes = Context(\"message.raw_message.key\"),\n",
      ") -> None:\n",
      "    \"\"\"\n",
      "    Processes a message from the 'input_data' topic.\n",
      "    Add all x elements from the memory (x_sum) and all y from the memory (y_sum) and publish the message with x_sum and y_sum to the output_data topic.\n",
      "    The same partition key should be used in the input_data and output_data topic.\n",
      "\n",
      "    Instructions:\n",
      "    1. Consume a message from 'input_data' topic.\n",
      "    2. Create a new message object (do not directly modify the original).\n",
      "    3. Add all x elements from the memory (x_sum) and all y from the memory (y_sum)\n",
      "    4. Publish the message with x_sum and y_sum to the output_data topic. (The same partition key should be used in the input_data and output_data topic).\n",
      "    \"\"\"\n",
      "    raise NotImplementedError()\n",
      "\n",
      "\n",
      "==== YOUR RESPONSE ====\n",
      "\n",
      "### application.py ###\n",
      "\n",
      "from typing import List\n",
      "\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "from faststream import Context, ContextRepo, FastStream, Logger\n",
      "from faststream.kafka import KafkaBroker\n",
      "\n",
      "\n",
      "class Point(BaseModel):\n",
      "    x: float = Field(\n",
      "        ..., examples=[0.5], description=\"The X Coordinate in the coordinate system\"\n",
      "    )\n",
      "    y: float = Field(\n",
      "        ..., examples=[0.5], description=\"The Y Coordinate in the coordinate system\"\n",
      "    )\n",
      "\n",
      "\n",
      "broker = KafkaBroker(\"localhost:9092\")\n",
      "app = FastStream(broker)\n",
      "\n",
      "\n",
      "to_output_data = broker.publisher(\"output_data\")\n",
      "\n",
      "\n",
      "@app.on_startup\n",
      "async def app_setup(context: ContextRepo):\n",
      "    message_history: List[Point] = []\n",
      "    context.set_global(\"message_history\", message_history)\n",
      "\n",
      "\n",
      "@broker.subscriber(\"input_data\")\n",
      "async def on_input_data(\n",
      "    msg: Point,\n",
      "    logger: Logger,\n",
      "    message_history: List[Point] = Context(),\n",
      "    key: bytes = Context(\"message.raw_message.key\"),\n",
      ") -> None:\n",
      "    logger.info(f\"{msg=}\")\n",
      "\n",
      "    message_history.append(msg)\n",
      "\n",
      "    x_sum = 0\n",
      "    y_sum = 0\n",
      "    for msg in message_history:\n",
      "        x_sum += msg.x\n",
      "        y_sum += msg.y\n",
      "\n",
      "    point_sum = Point(x=x_sum, y=y_sum)\n",
      "    await to_output_data.publish(point_sum, key=key)\n",
      "\n",
      "\n",
      "### test.py ###\n",
      "\n",
      "import pytest\n",
      "\n",
      "from faststream import Context, TestApp\n",
      "from faststream.kafka import TestKafkaBroker\n",
      "\n",
      "from application import Point, app, broker\n",
      "\n",
      "\n",
      "@broker.subscriber(\"output_data\")\n",
      "async def on_output_data(msg: Point, key: bytes = Context(\"message.raw_message.key\")):\n",
      "    pass\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_point_was_incremented():\n",
      "    async with TestKafkaBroker(broker):\n",
      "        async with TestApp(app):\n",
      "            await broker.publish(Point(x=1.0, y=2.0), \"input_data\", key=b\"point_key\")\n",
      "            await broker.publish(Point(x=1.0, y=2.0), \"input_data\", key=b\"point_key\")\n",
      "\n",
      "            on_output_data.mock.assert_called_with(dict(Point(x=2.0, y=4.0)))\n",
      "\n",
      "\n",
      "\n",
      "You need to avoid the below common issues while generating the test code. The potential fixes for the common issues are also given for your reference. You should use this information as an additional reference while generating the test:\n",
      "\n",
      "==== Error ====\n",
      "\n",
      "AssertionError: Expected 'mock' to not have been called. Called 2 times.\n",
      "\n",
      "==== ERROR CODE ====\n",
      "async with TestKafkaBroker(broker):\n",
      "await broker.publish(DataBasic(data=0.2), \"input_data\")\n",
      "\n",
      "        on_input_data.mock.assert_called_with(dict(DataBasic(data=0.2)))\n",
      "        on_output_data.mock.assert_not_called() # ERROR IN THIS LINE\n",
      "\n",
      "==== FIXED CODE ====\n",
      "\n",
      "    async with TestKafkaBroker(broker):\n",
      "        await broker.publish(DataBasic(data=0.2), \"input_data\")\n",
      "\n",
      "        on_input_data.mock.assert_called_with(dict(DataBasic(data=0.2)))\n",
      "        on_output_data.mock.assert_called_once_with(dict(DataBasic(data=1.2))) # ERROR FIXED IN THIS LINE\n",
      "\n",
      "==== Error ====\n",
      "\n",
      "pydantic_core.\\_pydantic_core.ValidationError: 1 validation error for ResponseModel\n",
      "\n",
      "==== ERROR CODE ====\n",
      "@broker.subscriber(\"output_data\")\n",
      "async def on_output_data(msg: DataBasic) -> DataBasic: # ERROR IN THIS LINE\n",
      "pass\n",
      "\n",
      "==== FIXED CODE ====\n",
      "\n",
      "@broker.subscriber(\"output_data\")\n",
      "async def on_output_data(msg: DataBasic): # ERROR FIXED IN THIS LINE\n",
      "pass\n",
      "\n",
      "\n",
      "==== APP DESCRIPTION ====\n",
      "\n",
      "Create a FastStream application using localhost broker for testing and use the default port number. It should consume messages from the \"input_data\" topic, where each message is a JSON encoded object containing a single attribute: 'data'. For each consumed message, create a new message object and increment the value of the data attribute by 1. Finally, send the modified message to the 'output_data' topic.\n",
      "\n",
      "\n",
      "==== APP SKELETON ====\n",
      "\n",
      "\n",
      "\n",
      "===Role:user===\n",
      "\n",
      "Message:\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "from faststream import FastStream, Logger\n",
      "from faststream.kafka import KafkaBroker\n",
      "\n",
      "\n",
      "class InputMessage(BaseModel):\n",
      "    data: int = Field(\n",
      "        ..., examples=[5], description=\"The data attribute in the input message\"\n",
      "    )\n",
      "\n",
      "\n",
      "class OutputMessage(BaseModel):\n",
      "    data: int = Field(\n",
      "        ..., examples=[6], description=\"The modified data attribute in the output message\"\n",
      "    )\n",
      "\n",
      "\n",
      "broker = KafkaBroker(\"localhost\")\n",
      "app = FastStream(broker)\n",
      "\n",
      "\n",
      "to_output_data = broker.publisher(\"output_data\")\n",
      "\n",
      "\n",
      "@broker.subscriber(\"input_data\")\n",
      "async def on_input_data(msg: InputMessage, logger: Logger) -> None:\n",
      "    \"\"\"\n",
      "    Processes a message from the 'input_data' topic.\n",
      "    Create a new message object and increment the value of the data attribute by 1.\n",
      "    Send the modified message to the 'output_data' topic.\n",
      "\n",
      "    Instructions:\n",
      "    1. Consume a message from 'input_data' topic.\n",
      "    2. Create a new message object (do not directly modify the original).\n",
      "    3. Increment the value of the data attribute by 1.\n",
      "    4. Send the modified message to the 'output_data' topic.\n",
      "    \"\"\"\n",
      "    raise NotImplementedError()\n",
      "==== YOUR RESPONSE ====\n",
      "\n",
      "23-09-21 23:58:03.684 [INFO] faststream_gen._code_generator.helper: ************************************************************************************************************************\n",
      "23-09-21 23:58:10.400 [INFO] faststream_gen._code_generator.helper: ************************************************************************************************************************\n",
      "23-09-21 23:58:10.400 [INFO] faststream_gen._code_generator.helper: \n",
      "\n",
      "Prompt to the model: \n",
      "\n",
      "===Role:system===\n",
      "\n",
      "Message:\n",
      "\n",
      "You are an expert Python developer, tasked to generate executable Python code as a part of your work with the FastStream framework. \n",
      "\n",
      "You are to abide by the following guidelines:\n",
      "\n",
      "1. You must never enclose the generated Python code with ``` python. It is mandatory that the output is a valid and executable Python code. Please ensure this rule is never broken.\n",
      "\n",
      "2. Some prompts might require you to generate code that contains async functions. For example:\n",
      "\n",
      "async def app_setup(context: ContextRepo):\n",
      "    raise NotImplementedError()\n",
      "\n",
      "In such cases, it is necessary to add the \"import asyncio\" statement at the top of the code. \n",
      "\n",
      "You will encounter sections marked as:\n",
      "\n",
      "==== APP DESCRIPTION: ====\n",
      "\n",
      "These sections contain the description of the FastStream app you need to implement. Treat everything below this line, until the end of the prompt, as the description to follow for the app implementation.\n",
      "\n",
      "\n",
      "===Role:user===\n",
      "\n",
      "Message:\n",
      "\n",
      "\n",
      "===Role:user===\n",
      "\n",
      "Message:\n",
      "\n",
      "You will be provided with an application code in ==== APP CODE ====, ==== REQUIREMENT ==== and ==== DEV REQUIREMENT ==== section. Your goal is to update both the r==== REQUIREMENT ==== and ==== DEV REQUIREMENT ==== section based on the provided ==== APP CODE ====.\n",
      "\n",
      "Input:\n",
      "\n",
      "You will be given application code a FastStream application in ==== APP CODE ==== section and requirements in ==== REQUIREMENT ==== and ==== DEV REQUIREMENT ==== section.\n",
      "\n",
      "Output:\n",
      "\n",
      "You need to understand the ==== APP CODE ==== and update the following:\n",
      "\n",
      "    - The ==== REQUIREMENT ==== section based on the application code\n",
      "    - The ==== DEV REQUIREMENT ==== section based on the application code\n",
      "\n",
      "Instructions you must follow while generating the files:\n",
      "\n",
      "    - You need to understand the ==== APP CODE ====, if it contains kafka related code, e.g: import statement like \"from faststream.kafka import KafkaBroker\", then the application is related to kafka, then your ==== REQUIREMENT ==== contents should be faststream[kafka, docs], and ==== DEV REQUIREMENT ==== contents should be faststream[kafka, testing]\n",
      "\n",
      "    - You need to understand the ==== APP CODE ====, if it contains RabbitMQ related code, e.g: import statement like \"from faststream.rabbit import RabbitBroker\", then the application is related to RabbitMQ, then your ==== REQUIREMENT ==== contents should be faststream[rabbit, docs], and ==== DEV REQUIREMENT ==== contents should be faststream[rabbit, testing]\n",
      "\n",
      "    - You need to generate a single txt file containing both the contents of ==== REQUIREMENT ==== and the ==== DEV REQUIREMENT ==== with proper delimiters\n",
      "    \n",
      "    - You should only update the faststream package, if the  ==== REQUIREMENT ==== and the ==== DEV REQUIREMENT ==== contains any other requirements, you should retain it as it is.\n",
      "    \n",
      "    - you should always respond with the below example format and do not add additional text to it.\n",
      "    \n",
      "    - Do not add unnecessary new lines in your response. Do not add additional new line at the end of your response.\n",
      "\n",
      "Below are few examples for your understanding:\n",
      "\n",
      "==== EXAMPLE APP CODE ====\n",
      "\n",
      "from pydantic import BaseModel, Field, NonNegativeFloat\n",
      "\n",
      "from faststream import FastStream, Logger\n",
      "from faststream.kafka import KafkaBroker\n",
      "import pandas\n",
      "\n",
      "\n",
      "class DataBasic(BaseModel):\n",
      "    data: NonNegativeFloat = Field(\n",
      "        ..., examples=[0.5], description=\"Float data example\"\n",
      "    )\n",
      "\n",
      "\n",
      "broker = KafkaBroker(\"localhost:9092\")\n",
      "app = FastStream(broker)\n",
      "\n",
      "\n",
      "@broker.publisher(\"output_data\")\n",
      "@broker.subscriber(\"input_data\")\n",
      "async def on_input_data(msg: DataBasic, logger: Logger) -> DataBasic:\n",
      "    logger.info(msg)\n",
      "    return DataBasic(data=msg.data + 1.0)\n",
      "\n",
      "==== REQUIREMENT ====\n",
      "faststream[docs]==0.0.1.dev20230912\n",
      "pandas===0.0.1\n",
      "\n",
      "==== DEV REQUIREMENT ====\n",
      "faststream[testing]==0.0.1.dev20230912\n",
      "\n",
      "==== YOUR RESPONSE ====\n",
      "### requirements.txt ###\n",
      "faststream[kafka, docs]==0.0.1.dev20230912\n",
      "pandas===0.0.1\n",
      "### dev_requirements.txt ###\n",
      "faststream[kafka, testing]==0.0.1.dev20230912\n",
      "\n",
      "==== EXAMPLE APP CODE ====\n",
      "\n",
      "import asyncio\n",
      "from faststream.rabbit import RabbitBroker\n",
      "\n",
      "async def pub():\n",
      "    async with RabbitBroker() as broker:\n",
      "        await broker.publish(\n",
      "            \"Hi!\",\n",
      "            queue=\"test\",\n",
      "            exchange=\"test\"\n",
      "        )\n",
      "\n",
      "asyncio.run(pub())\n",
      "\n",
      "==== REQUIREMENT ====\n",
      "faststream[docs]==0.0.2\n",
      "\n",
      "==== DEV REQUIREMENT ====\n",
      "faststream[testing]==0.0.2\n",
      "\n",
      "==== YOUR RESPONSE ====\n",
      "### requirements.txt ###\n",
      "faststream[rabbit, docs]==0.0.2\n",
      "### dev_requirements.txt ###\n",
      "faststream[rabbit, testing]==0.0.2\n",
      "\n",
      "\n",
      "==== APP CODE ====\n",
      "\n",
      "\n",
      "\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "from faststream import FastStream, Logger\n",
      "from faststream.kafka import KafkaBroker\n",
      "\n",
      "\n",
      "class InputMessage(BaseModel):\n",
      "    data: int = Field(\n",
      "        ..., examples=[5], description=\"The data attribute in the input message\"\n",
      "    )\n",
      "\n",
      "\n",
      "class OutputMessage(BaseModel):\n",
      "    data: int = Field(\n",
      "        ..., examples=[6], description=\"The modified data attribute in the output message\"\n",
      "    )\n",
      "\n",
      "\n",
      "broker = KafkaBroker(\"localhost\")\n",
      "app = FastStream(broker)\n",
      "\n",
      "\n",
      "to_output_data = broker.publisher(\"output_data\")\n",
      "\n",
      "\n",
      "@broker.subscriber(\"input_data\")\n",
      "async def on_input_data(msg: InputMessage, logger: Logger) -> None:\n",
      "    logger.info(f\"{msg=}\")\n",
      "    modified_msg = OutputMessage(data=msg.data + 1)\n",
      "    await to_output_data.publish(modified_msg)\n",
      "\n",
      "\n",
      "\n",
      "==== REQUIREMENT ====\n",
      "faststream[docs]==0.1.0\n",
      "PyYAML==6.0.1\n",
      "\n",
      "==== DEV REQUIREMENT ====\n",
      "faststream[testing]==0.1.0\n",
      "\n",
      "\n",
      "===Role:user===\n",
      "\n",
      "Message:\n",
      "\n",
      "\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "from faststream import FastStream, Logger\n",
      "from faststream.kafka import KafkaBroker\n",
      "\n",
      "\n",
      "class InputMessage(BaseModel):\n",
      "    data: int = Field(\n",
      "        ..., examples=[5], description=\"The data attribute in the input message\"\n",
      "    )\n",
      "\n",
      "\n",
      "class OutputMessage(BaseModel):\n",
      "    data: int = Field(\n",
      "        ..., examples=[6], description=\"The modified data attribute in the output message\"\n",
      "    )\n",
      "\n",
      "\n",
      "broker = KafkaBroker(\"localhost\")\n",
      "app = FastStream(broker)\n",
      "\n",
      "\n",
      "to_output_data = broker.publisher(\"output_data\")\n",
      "\n",
      "\n",
      "@broker.subscriber(\"input_data\")\n",
      "async def on_input_data(msg: InputMessage, logger: Logger) -> None:\n",
      "    logger.info(f\"{msg=}\")\n",
      "    modified_msg = OutputMessage(data=msg.data + 1)\n",
      "    await to_output_data.publish(modified_msg)\n",
      "\n",
      "\n",
      "\n",
      "==== REQUIREMENT ====\n",
      "faststream[docs]==0.1.0\n",
      "PyYAML==6.0.1\n",
      "\n",
      "==== DEV REQUIREMENT ====\n",
      "faststream[testing]==0.1.0\n",
      "\n",
      "==== YOUR RESPONSE ====\n",
      "\n",
      "23-09-21 23:58:10.400 [INFO] faststream_gen._code_generator.helper: ************************************************************************************************************************\n",
      "23-09-21 23:58:21.022 [INFO] faststream_gen.cli: Prompt Tokens: 25927\n",
      "23-09-21 23:58:21.022 [INFO] faststream_gen.cli: Completion Tokens: 1518\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "\n",
    "fixture = \"\"\"\n",
    "Create a FastStream application using localhost broker for testing and use the default port number. \n",
    "It should consume messages from the \"input_data\" topic, where each message is a JSON encoded object containing a single attribute: 'data'. \n",
    "For each consumed message, create a new message object and increment the value of the data attribute by 1. Finally, send the modified message to the 'output_data' topic.\n",
    "\"\"\"\n",
    "\n",
    "with TemporaryDirectory() as d:\n",
    "    app_description = Path(d) / \"hello_world.txt\"\n",
    "    with app_description.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(fixture)\n",
    "\n",
    "    result = runner.invoke(app, [d])\n",
    "    files = [f for f in Path(d).glob(\"*.txt\")]\n",
    "    print(files)\n",
    "    assert len(files) == 2\n",
    "    assert app_description.exists()\n",
    "    log_file = Path(d) / \"hello_world-log.txt\"\n",
    "    assert log_file.exists()\n",
    "    \n",
    "    with log_file.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        contents = f.read()\n",
    "    \n",
    "    print(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3b26a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
